{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - 决策树原理\n",
    "\n",
    "决策树（Decision Tree）是一种基本的分类与回归方法，本文主要讨论分类决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对数据进行分类的过程。它可以认为是if-then规则的集合。每个内部节点表示在属性上的一个测试，每个分支代表一个测试输出，每个叶节点代表一种类别。 \n",
    "\n",
    "\n",
    "决策树的优点： \n",
    "* 1）可以自学习。在学习过程中不需要使用者了解过多的背景知识，只需要对训练数据进行较好的标注，就能进行学习。 \n",
    "* 2）决策树模型可读性好，具有描述性，有助于人工分析； \n",
    "* 3）效率高，决策树只需要一次构建，就可以反复使用，每一次预测的最大计算次数不超过决策树的深度。\n",
    "\n",
    "简单介绍完毕，让我们来通过一个例子让决策树“原形毕露”。\n",
    "\n",
    "一天，老师问了个问题，只根据头发和声音怎么判断一位同学的性别。 \n",
    "为了解决这个问题，同学们马上简单的统计了7位同学的相关特征，数据如下：\n",
    "\n",
    "| 　　　　　　　头发　　　　　　　　 | 　　　　　　　　声音 　　　　　　　　| 　　　　　　　　性别　　　　　　　 |\n",
    "| ------ | ------ | ------ |\n",
    "| 长 | 粗 | 男 |\n",
    "| 短 | 粗 | 男 |\n",
    "| 短 | 粗 | 男 |\n",
    "| 长 | 细 | 女 |\n",
    "| 短 | 细 | 女 |\n",
    "| 短 | 粗 | 女 |\n",
    "| 长 | 粗 | 女 |\n",
    "| 长 | 粗 | 女 |\n",
    "\n",
    "机智的同学A想了想，先根据头发判断，若判断不出，再根据声音判断，于是画了一幅图，如下： \n",
    "\n",
    "![20170325232900261.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/10-1.png?raw=true)\n",
    "\n",
    "\n",
    "于是，一个简单、直观的决策树就这么出来了。头发长、声音粗就是男生；头发长、声音细就是女生；头发短、声音粗是男生；头发短、声音细是女生。 \n",
    "原来机器学习中决策树就这玩意，这也太简单了吧。。。 \n",
    "这时又蹦出个同学B，想先根据声音判断，然后再根据头发来判断，如是大手一挥也画了个决策树： \n",
    "\n",
    "\n",
    "![20170325231929573.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/10-2.png?raw=true)\n",
    "\n",
    "\n",
    "同学B的决策树：首先判断声音，声音细，就是女生；声音粗、头发长是男生；声音粗、头发长是女生。\n",
    "\n",
    "那么问题来了：同学A和同学B谁的决策树好些？计算机做决策树的时候，面对多个特征，该如何选哪个特征为最佳的划分特征？\n",
    "\n",
    "如何评估分裂点的好坏？如果一个分裂点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分裂点。一般而言，随着划分不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”（purity）越来越高 \n",
    "具体实践中，到底选择哪个特征作为当前分裂特征，常用的有下面三种算法： \n",
    "\n",
    "* ID3：使用信息增益g(D,A)进行特征选择 \n",
    "* C4.5：信息增益率 =g(D,A)/H(A) \n",
    "* CART：基尼系数 \n",
    "\n",
    "一个特征的信息增益(或信息增益率，或基尼系数)越大，表明特征对样本的熵的减少能力更强，这个特征使得数据由不确定性到确定性的能力越强。\n",
    "\n",
    "# 2 - 使用ID3算法构建决策树\n",
    "\n",
    "\n",
    "ID3算法是通过信息增益来判断选取哪个特征进行决策的，要了解信息增益这个概念，我们需要先知道什么是“信息熵”\n",
    "\n",
    "## 2.1 - 信息熵\n",
    "\n",
    "    \n",
    "“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,2,\\dots,|y|)$，则$D$的信息熵定义为：\n",
    "$$Ent(D)=-\\sum_{k=1}^{|y|}p_klog_2p_k$$\n",
    "\n",
    "$Ent(D)$的值越小，则D的纯度越高\n",
    "\n",
    "那么这个式子$Ent(D)=-\\sum_{k=1}^{|y|}p_klog_2p_k$又是怎么来的呢？\n",
    "\n",
    "实际上，熵的概念首先在热力学中引入，用于表述热力学第二定律。波尔兹曼研究得到，热力学熵与微观状态数目的对数之间存在联系，并给出了公式：\n",
    "$$S = klnW$$\n",
    "\n",
    "信息熵的定义与上述这个热力学的熵，虽然不是一个东西，但是有一定的联系。熵在信息论中代表随机变量不确定度的度量。一个离散型随机变量$X$的熵$H(X)$定义为：\n",
    "\n",
    "$$H(X)=-\\sum_{x\\in {X}}p(x)logp(x)$$\n",
    "这个定义的特点是，有明确定义的科学名词且与内容无关，而且不随信息的具体表达式的变化而变化。是独立于形式，反映了信息表达式中统计方面的性质。是统计学上的抽象概念。\n",
    "\n",
    "所以这个定义如题主提到的可能有点抽象和晦涩，不易理解。那么下面让我们从直觉出发，以生活中的一些例子来阐述信息熵是什么，以及有什么用处。\n",
    "\n",
    "直觉上，信息量等于传输该信息所用的代价，这个也是通信中考虑最多的问题。比如说：赌马比赛里，有4匹马${A,B,C,D}$,获胜概率分别为${\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\frac{1}{8}}$\n",
    "\n",
    "接下来，让我们将哪一匹马获胜视为一个随机变量$X\\in {A,B,C,D}$假定我们需要用尽可能少的二元问题来确定随机变量$X$的取值。\n",
    "\n",
    "例如：问题1：A获胜了吗？问题2：B获胜了吗？问题3：C获胜了吗？最后我们可以通过最多3个二元问题，来确定$X$的取值，即哪一匹马赢了比赛。\n",
    "\n",
    "如果$X=A$，那么需要问1次（问题1：是不是A？）概率为$\\frac{1}{2}$\n",
    "\n",
    "如果$X=B$，那么需要问2次（问题1：是不是A？问题2：是不是B？）概率为$\\frac{1}{4}$\n",
    "\n",
    "如果$X=C$，那么需要问3次（问题1,问题2，问题3）概率为$\\frac{1}{8}$\n",
    "\n",
    "如果$X=D$，那么需要问3次（问题1,问题2，问题3）概率为$\\frac{1}{8}$\n",
    "\n",
    "那么很容易计算，在这种问法下，为确定$X$取值的二元问题数量为：\n",
    "$$E(N)=\\frac{1}{2}\\cdot 1+\\frac{1}{4}\\cdot 2+\\frac{1}{8}\\cdot 3+\\frac{1}{8}\\cdot 3=\\frac{7}{4}$$\n",
    "那么我们回到信息熵的定义，会发现通过之前的信息熵公式，神奇地得到了：\n",
    "$$H(X)=\\frac{1}{2}log_2(2)+\\frac{1}{4}log_2(4)+\\frac{1}{8}log_2(8)+\\frac{1}{8}log_2(8)=\\frac{7}{4}$$\n",
    "\n",
    "从而验证了上式（当然这只是便于理解而不是推导过程）。\n",
    "\n",
    "## 2.2 - 信息增益\n",
    "\n",
    "我们计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^V|/|D|$,即样本数越多的分支结点的影响越大，于是可计算“信息增益”（information gain）\n",
    "$$Gain(D,a)=Ent(D)-\\sum_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)$$\n",
    "\n",
    "一般而言，信息增益越大，则意味着使用属性来进行划分所得的“纯度提升”越大，因此，我们可用信息增益进行决策树的划分属性选择。\n",
    "\n",
    "以上述为例：\n",
    "\n",
    "首先计算未分类前的熵，总共有8位同学，男生3位，女生5位。\n",
    "\n",
    "$$Ent(总)=-\\frac{3}{8}\\cdot log_2(\\frac{3}{8})-\\frac{5}{8}\\cdot log_2(\\frac{5}{8})=0.9544$$\n",
    "\n",
    "接着分别计算同学A和同学B分类后信息熵。\n",
    "\n",
    "同学A首先按头发分类，分类结果为：长头发中有1男3女，短头发中有2男2女\n",
    "$$D=(所有样例)$$\n",
    "$$D^1=(长发)$$\n",
    "$$D^2=(短发)$$\n",
    "\n",
    "$$Ent(D^1)=-\\frac{1}{4}\\cdot log_2(\\frac{1}{4})-\\frac{3}{4}\\cdot log_2(\\frac{3}{4})=0.8113$$\n",
    "$$Ent(D^2)=-\\frac{2}{4}\\cdot log_2(\\frac{2}{4})-\\frac{2}{4}\\cdot log_2(\\frac{2}{4})=1$$\n",
    "$$\\sum_{v=1}^2\\frac{|D^v|}{|D|}Ent(D^v)= \\frac{4}{8}\\cdot 0.08113+\\frac{4}{8}\\cdot 1 = 0.9057$$\n",
    "$$Gain(D,头发)=Ent(总)-\\sum_{v=1}^2\\frac{|D^v|}{|D|}Ent(D^v)=0.9544-0.9057=0.0487$$\n",
    "\n",
    "同理，按同学B的方法，首先按声音特征来分，分类后的结果为:声音粗中有3男3女，声音细中有0男2女\n",
    "$$D^1=(声音粗)$$\n",
    "$$D^2=(声音细)$$\n",
    "\n",
    "$$Ent(D^1)=-\\frac{3}{6}\\cdot log_2(\\frac{3}{6})-\\frac{3}{6}\\cdot log_2(\\frac{3}{6})=1$$\n",
    "$$Ent(D^2)=-\\frac{2}{2}\\cdot log_2(\\frac{2}{2})=0$$\n",
    "$$\\sum_{v=1}^2\\frac{|D^v|}{|D|}Ent(D^v)= \\frac{6}{8}\\cdot 1+\\frac{2}{8}\\cdot 0=0.75$$\n",
    "$$Gain(D,声音)=Ent(总)-\\sum_{v=1}^2\\frac{|D^v|}{|D|}Ent(D^v)=0.9544-0.75=0.2087$$\n",
    "\n",
    "按同学B的方法，先按声音特征分类，信息增益更大，区分样本的能力更强，更具有代表性。 \n",
    "\n",
    "\n",
    "### 以上就是决策树ID3算法的核心思想。 \n",
    "\n",
    "## 2.3 - 接下来用python代码来实现ID3算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建数据集\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "def createDataSet1():    # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发','声音']  #两个特征\n",
    "    return dataSet,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算数据信息熵\n",
    "def calcShannonEnt(dataSet):  # 计算数据的熵(entropy)\n",
    "    numEntries=len(dataSet)  # 数据条数\n",
    "    labelCounts={} #构造一个空的字典类型\n",
    "    for featVec in dataSet:  # 统计有多少个类以及每个类的数量，也就是分别统计男女的数量\n",
    "        currentLabel=featVec[-1] # 每行数据的最后一个字（类别）\n",
    "        if currentLabel not in labelCounts.keys(): #如果currentLabel没有在labelCounts中，则在labelCounts创建一个这个类别，并记为0\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1 #否则，这个类别的数量+1\n",
    "    shannonEnt=0\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries # 计算单个类的熵值\n",
    "        shannonEnt-=prob*log(prob,2) # 累加每个类的熵值\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下，可以看到数据集的信息熵与上文我们手算的结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9544340029249649\n"
     ]
    }
   ],
   "source": [
    "#测试calcShannonEnt函数\n",
    "dataSet,labels=createDataSet1()\n",
    "shannonEnt=calcShannonEnt(dataSet)\n",
    "print(shannonEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义按照某个特征进行划分的函数splitDataSet\n",
    "#输入三个变量（待划分的数据集，特征，分类值）\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    retDataSet=[] #创建一个空List\n",
    "    for featVec in dataSet: #遍历数据集\n",
    "        if featVec[axis]==value: \n",
    "            reducedFeatVec =featVec[:axis] # 取划分特征值前面一部分，但不包括特征值\n",
    "            reducedFeatVec.extend(featVec[axis+1:]) # 取划分特征值后面一部分，但不包括特征值              \n",
    "            \"\"\"\n",
    "            以上两行获得的数据集将不包括特征值的记录，\n",
    "            以达到每一次划分都将大数据集划分为小数据集的目的\n",
    "\n",
    "            \"\"\"\n",
    "             # 以每条记录为单位追加到空列表中\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下，可以看到按照所选的特征划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['粗', '男'], ['细', '女'], ['粗', '女'], ['粗', '女']]\n"
     ]
    }
   ],
   "source": [
    "#测试splitDataSet函数\n",
    "\n",
    "retDataSet=splitDataSet(dataSet,0,\"长\")\n",
    "print(retDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):  # 选择最优的分类特征\n",
    "    numFeatures = len(dataSet[0])-1\n",
    "    baseEntropy = calcShannonEnt(dataSet)  # 原始的熵\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)\n",
    "            prob =len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy +=prob*calcShannonEnt(subDataSet)  # 按特征分类后的熵\n",
    "        infoGain = baseEntropy - newEntropy  # 原始熵与按特征分类后的熵的差值\n",
    "        if (infoGain>bestInfoGain):   # 若按某特征划分后，熵值减少的最大，则次特征为最优分类特征\n",
    "            bestInfoGain=infoGain\n",
    "            bestFeature = i\n",
    "            \n",
    "    return bestFeature #返回值对应标签取值，若为0则为头发长短，若为1则为声音粗细"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下，返回值对应标签取值，若为0则为头发长短，若为1则为声音粗细，可以看到也与我们手动计算信息增益所选出的特征一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "声音\n"
     ]
    }
   ],
   "source": [
    "#测试chooseBestFeatureToSplit函数\n",
    "bestFeature=chooseBestFeatureToSplit(dataSet)\n",
    "print(labels[bestFeature]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#功能：多数表决决定叶子结点\n",
    "#使用分类名称的列表，创建键值为classList中唯一值的数据字典，字典对象存储了classList每个类标签出现的频率\n",
    "#返回：出现次数最多的分类名称\n",
    "def majorityCnt(classList):    \n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) \n",
    "    \"\"\"\n",
    "    classCount.iteritems()将classCount字典分解为元组列表，\n",
    "    operator.itemgetter(1)按照第二个元素的次序对元组进行排序，\n",
    "    reverse=True是逆序，即按照从大到小的顺序排列\n",
    "    \"\"\"\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用初始的数据集进行测试，该函数会返回最多的类标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女\n"
     ]
    }
   ],
   "source": [
    "#测试majorityCnt\n",
    "classList=[example[-1] for example in dataSet]\n",
    "sortedClassCount=majorityCnt(classList)\n",
    "print(sortedClassCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造决策树，采用递归的方法构建\n",
    "def createTree(dataSet,labels):\n",
    "    classList=[example[-1] for example in dataSet]  # 类别：男或女\n",
    "    if classList.count(classList[0])==len(classList): #当所有的类都相等时停止分裂\n",
    "        return classList[0]\n",
    "    if len(dataSet[0])==1:      #当使用完所有的特征进行分类后仍然仍然不能将数据集划分成仅包含唯一类别的分组，则将子叶定为数量最多类\n",
    "        return majorityCnt(classList) # 采用多数多数原则选出分组\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet) #选择最优特征\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    # 这里直接使用字典变量来存储树信息，这对于绘制树形图很重要。\n",
    "    myTree={bestFeatLabel:{}} #分类结果以字典形式保存\n",
    "    del(labels[bestFeat])     ## 删除已经在选取的特征\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    uniqueVals=set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:] # 复制所有标签，这样树就不会破坏现有的标签\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet\\\n",
    "                            (dataSet,bestFeat,value),subLabels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过递归的方法构造决策树，最后得出一个由ID3算法选取分类特征的已分好类的树形结构（字典型数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'声音': {'粗': {'头发': {'短': '男', '长': '女'}}, '细': '女'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试createTree\n",
    "createTree(dataSet,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - 使用C4.5算法构建决策树：\n",
    "\n",
    "实际上，ID3算法对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法不直接使用信息增益，而是使用“增益率”（gain ratio）来选择最优划分属性。那么什么是增益率呢？\n",
    "\n",
    "## 3.1 - 增益率\n",
    "\n",
    "增益率定义为：\n",
    "$$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$\n",
    "\n",
    "其中：\n",
    "$$IV(a)=-\\sum_{v=1}^V\\frac{|D^V|}{|D|}log_2(\\frac{|D^V|}{|D|})$$\n",
    "\n",
    "成为属性a的“固有值”，属性a的可能取值数目越多（即V越大），则IV（a）的值通常会越大，从而改善了由于取值数目较多属性的影响。\n",
    "\n",
    "同学A首先按头发分类，分类结果为：长头发中有1男3女，短头发中有2男2女\n",
    "$$D=(所有样例)$$\n",
    "$$D^1=(长发)$$\n",
    "$$D^2=(短发)$$\n",
    "\n",
    "$$IV(a)=-\\sum_{v=1}^V\\frac{|D^v|}{D}log_2\\frac{|D^v|}{D}=-\\frac{4}{8}log_2\\frac{4}{8}-\\frac{4}{8}log_2\\frac{4}{8}=1$$\n",
    "$$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}=0.0487/1=0.0487$$\n",
    "\n",
    "同理，按同学B的方法，首先按声音特征来分，分类后的结果为:声音粗中有3男3女，声音细中有0男2女\n",
    "$$D^1=(声音粗)$$\n",
    "$$D^2=(声音细)$$\n",
    "\n",
    "$$IV(a)=-\\sum_{v=1}^V\\frac{|D^v|}{D}log_2\\frac{|D^v|}{D}=-\\frac{2}{8}log_2\\frac{2}{8}-\\frac{6}{8}log_2\\frac{6}{8}=0.8112$$\n",
    "$$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}=0.2087/0.8112=0.2572$$\n",
    "\n",
    "按同学B的方法，先按声音特征分类，增益率更大，区分样本的能力更强，更具有代表性。 \n",
    "\n",
    "## 3.2 - 使用Python实现C4.5决策树：\n",
    "\n",
    "C4.5决策树的实现代码和ID3的代码基本一致，只是在选取最优特征的代码中，将信息增益改为了增益率，其他都没有变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'声音': {'粗': {'头发': {'短': '男', '长': '女'}}, '细': '女'}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import operator\n",
    "#功能：导入数据表\n",
    "#功能：计算熵\n",
    "def createDataSet1():    # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发','声音']  #两个特征\n",
    "    return dataSet,labels\n",
    "def calcShannonEnt (dataSet):\n",
    "    num = len(dataSet)#实例的个数\n",
    "    labelCounts = {}#类标签\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]#最后一列的数值\n",
    "        if currentLabel not in labelCounts.keys():#如果在labelCounts中没出现\n",
    "            labelCounts[currentLabel] = 0#就把currentLabel键加入labelCounts中，值为0\n",
    "        labelCounts[currentLabel] += 1#labelCounts对应的值就加一\n",
    "    #计算香农熵H(D)\n",
    "    ShannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / num#计算p(Xi)概率\n",
    "        ShannonEnt -= prob * math.log(prob, 2) \n",
    "    return ShannonEnt\n",
    "\n",
    "#功能：按照给定特征划分数据集\n",
    "#输入：数据集、划分数据集的特征、需要返回的特征的值\n",
    "#返回：划分后的数据集\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    newdataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            newdataSet.append(reducedFeatVec)\n",
    "    return newdataSet\n",
    "\n",
    "#功能：选取最好的数据集划分方式\n",
    "#返回：最佳特征下标(增益率最大)\n",
    "def chooseBestFeatureTosplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1#特征个数\n",
    "    baseEntropy = calcShannonEnt(dataSet)#原始香农熵H(D)\n",
    "    bestInfoGainrate = 0.0; bestFeature = -1#信息增益和最好的特征\n",
    "    #遍历特征\n",
    "    for i in range(numFeatures):\n",
    "        featureSet = set([example[i] for example in dataSet])#第i个特征取值集合\n",
    "        newEntropy = 0.0\n",
    "        splitinfo = 0.0\n",
    "        for value in featureSet:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)#经验条件熵H(D|A)\n",
    "            splitinfo -= prob * math.log(prob,2)#经验熵HA(D)\n",
    "        #当概率为1或者0时(因为经验熵要做被除数)：\n",
    "        if not splitinfo:\n",
    "            splitinfo = -0.99 * math.log(0.99,2) - 0.01 * math.log(0.01,2)\n",
    "        infoGain = baseEntropy - newEntropy#信息增益=H(D)-H(D|A)\n",
    "        infoGainrate = float(infoGain) / float(splitinfo)#增益率=信息增益/经验熵\n",
    "        if infoGainrate > bestInfoGainrate:\n",
    "            bestInfoGainrate = infoGainrate\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "#功能：多数表决决定叶子结点\n",
    "#使用分类名称的列表，创建键值为classList中唯一值的数据字典，字典对象存储了classList每个类标签出现的频率\n",
    "#返回：出现次数最多的分类名称\n",
    "def majorityCnt(classList):    #按分类后类别数量排序，比如：最后分类为2男1女，则判定为男；\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "#功能：创建树\n",
    "#输入数据集和类标签\n",
    "#返回字典树\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]#数据集的所有类标签\n",
    "    if classList.count(classList[0]) == len(classList):#停止条件是所有的类标签完全相同\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:#当使用完了所有特征，还不能将数据集划分成仅包含唯一类别的分组\n",
    "        return majorityCnt(classList)#挑选出出现次数最多的类别作为返回值\n",
    "    #开始创建树\n",
    "    bestFeat = chooseBestFeatureTosplit(dataSet)#当前数据集选取的最好特征\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    #得到列表包含的所有属性值\n",
    "    #赋值当前特征标签列表，防止改变原始列表的内容\n",
    "    subLabels = labels[:]\n",
    "    #删除属性列表中当前分类数据集特征\n",
    "    del(subLabels[bestFeat])\n",
    "    #获取数据集中最优特征所在列\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    #采用set集合性质，获取特征的所有的唯一取值\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
    "    return myTree\n",
    "if __name__=='__main__':\n",
    "    dataSet, labels=createDataSet1()  # 创造示列数据\n",
    "    print(createTree(dataSet, labels))  # 输出决策树模型结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - CART决策树\n",
    "\n",
    "CART决策树使用“基尼指数”来选择划分属性，所以我们先来介绍一下基尼指数\n",
    "\n",
    "## 4 - 1 基尼指数\n",
    "\n",
    "基尼指数由基尼值构成，而基尼值定义为：\n",
    "$$Gini(D)=1-\\sum_{k=1}^{|y|}p_k^2$$\n",
    "\n",
    "直观来说，$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因此，$Gini(D)$越小，则数据集$D$的纯度越高。\n",
    "\n",
    "则属性a的基尼指数定义为：\n",
    "\n",
    "$$Gini_{-}index(D,a)=\\sum_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)$$\n",
    "\n",
    "于是，我们在候选属性集合A中，选择哪个使用划分后基尼指数最小的属性作为划分属性。\n",
    "\n",
    "同学A首先按头发分类，分类结果为：长头发中有1男3女，短头发中有2男2女\n",
    "$$D=(所有样例)$$\n",
    "$$D^1=(长发)$$\n",
    "$$D^2=(短发)$$\n",
    "\n",
    "$$Gini(D^1)=1-(\\frac{1}{4})^2-(\\frac{3}{4})^2=0.375$$\n",
    "\n",
    "$$Gini(D^2)=1-(\\frac{2}{4})^2-(\\frac{2}{4})^2=0.5$$\n",
    "\n",
    "$$Gini_{-}gain(头发)=\\frac{4}{8}Gini(D^1)+\\frac{4}{8}Gini(D^2)=0.435$$\n",
    "\n",
    "同理，按同学B的方法，首先按声音特征来分，分类后的结果为:声音粗中有3男3女，声音细中有0男2女\n",
    "$$D^1=(声音粗)$$\n",
    "$$D^2=(声音细)$$\n",
    "\n",
    "$$Gini(D^1)=1-(\\frac{3}{6})^2-(\\frac{3}{6})^2=0.5$$\n",
    "\n",
    "$$Gini(D^2)=1-(\\frac{0}{2})^2-(\\frac{2}{2})^2=0$$\n",
    "\n",
    "$$Gini_{-}gain(声音)=\\frac{6}{8}Gini(D^1)+\\frac{2}{8}Gini(D^2)=0.375$$\n",
    "\n",
    "按同学B的方法，先按声音特征分类，基尼指数更小，区分样本的能力更强，更具有代表性。 \n",
    "\n",
    "## 4.2 - Python实现CART决策树\n",
    "\n",
    "CART决策树的代码也与ID3和C4.5算法类似，主要区别在于选取最优节点的函数chooseBestFeatureTosplit算法不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'声音': {'粗': {'头发': {'长': '女', '短': '男'}}, '细': '女'}}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import operator\n",
    "#功能：导入数据表\n",
    "#功能：计算熵\n",
    "def createDataSet1():    # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发','声音']  #两个特征\n",
    "    return dataSet,labels\n",
    "def calcShannonEnt (dataSet):\n",
    "    num = len(dataSet)#实例的个数\n",
    "    labelCounts = {}#类标签\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]#最后一列的数值\n",
    "        if currentLabel not in labelCounts.keys():#如果在labelCounts中没出现\n",
    "            labelCounts[currentLabel] = 0#就把currentLabel键加入labelCounts中，值为0\n",
    "        labelCounts[currentLabel] += 1#labelCounts对应的值就加一\n",
    "    #计算香农熵H(D)\n",
    "    ShannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / num#计算p(Xi)概率\n",
    "        ShannonEnt -= prob * math.log(prob, 2) \n",
    "    return ShannonEnt\n",
    "\n",
    "#功能：按照给定特征划分数据集\n",
    "#输入：数据集、划分数据集的特征、需要返回的特征的值\n",
    "#返回：划分后的数据集\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    newdataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            newdataSet.append(reducedFeatVec)\n",
    "    return newdataSet\n",
    "\n",
    "#功能：选取最好的数据集划分方式\n",
    "#返回：最佳特征下标(基尼指数最小)\n",
    "def chooseBestFeatureTosplit(dataSet):\n",
    "    \"\"\"\n",
    "    输入：数据集\n",
    "    输出：最好的划分维度\n",
    "    描述：选择最好的数据集划分维度\n",
    "    \"\"\"\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    bestGini = 999999.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        gini = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            subProb = len(splitDataSet(subDataSet, -1, '男')) / float(len(subDataSet))\n",
    "            gini += prob * (1.0 - pow(subProb, 2) - pow(1 - subProb, 2))\n",
    "        if (gini < bestGini):\n",
    "            bestGini = gini\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "#功能：多数表决决定叶子结点\n",
    "#使用分类名称的列表，创建键值为classList中唯一值的数据字典，字典对象存储了classList每个类标签出现的频率\n",
    "#返回：出现次数最多的分类名称\n",
    "def majorityCnt(classList):    #按分类后类别数量排序，比如：最后分类为2男1女，则判定为男；\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "#功能：创建树\n",
    "#输入数据集和类标签\n",
    "#返回字典树\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]#数据集的所有类标签\n",
    "    if classList.count(classList[0]) == len(classList):#停止条件是所有的类标签完全相同\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:#当使用完了所有特征，还不能将数据集划分成仅包含唯一类别的分组\n",
    "        return majorityCnt(classList)#挑选出出现次数最多的类别作为返回值\n",
    "    #开始创建树\n",
    "    bestFeat = chooseBestFeatureTosplit(dataSet)#当前数据集选取的最好特征\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    #得到列表包含的所有属性值\n",
    "    #赋值当前特征标签列表，防止改变原始列表的内容\n",
    "    subLabels = labels[:]\n",
    "    #删除属性列表中当前分类数据集特征\n",
    "    del(subLabels[bestFeat])\n",
    "    #获取数据集中最优特征所在列\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    #采用set集合性质，获取特征的所有的唯一取值\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
    "    return myTree\n",
    "if __name__=='__main__':\n",
    "    dataSet, labels=createDataSet1()  # 创造示列数据\n",
    "    print(createTree(dataSet, labels))  # 输出决策树模型结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
