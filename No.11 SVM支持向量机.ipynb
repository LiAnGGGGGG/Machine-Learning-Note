{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 - SVM的基本概念\n",
    "\n",
    "　　支持向量机（Support Vector Machine, SVM）的基本模型是在特征空间上找到最佳的分离超平面使得训练集上正负样本间隔最大。SVM是用来解决二分类问题的有监督学习算法，在引入了核方法之后SVM也可以用来解决非线性问题。我们分别从线性和非线性两个方面来学习支持向量机\n",
    "\n",
    "* 线性可分支持向量机：当训练数据线性可分时，可通过间隔最大化学得一个线性可分支持向量机。\n",
    "\n",
    "\n",
    "* 非线性支持向量机：当训练数据线性不可分时，可通过核方法以及间隔最大化学得一个非线性支持向量机。\n",
    "\n",
    "# 2 - 线性SVM\n",
    "\n",
    "先看下线性可分的二分类问题。\n",
    "\n",
    "![ml_8_9.jpg](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E7%BA%BF%E6%80%A71.jpg?raw=true)\n",
    "\n",
    "　　上图中的(a)是已有的数据，红色和蓝色分别代表两个不同的类别。数据显然是线性可分的，但是将两类数据点分开的直线显然不止一条。上图的(b)和(c)分别给出了B、C两种不同的分类方案，其中黑色实线为分界线，术语称为“决策面”。每个决策面对应了一个线性分类器。虽然从分类结果上看，分类器A和分类器B的效果是相同的。但是他们的性能是有差距的，看下图：\n",
    "\n",
    "![ml_8_10.jpg](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E7%BA%BF%E6%80%A72.jpg?raw=true)\n",
    "\n",
    "　　在\"决策面\"不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的\"决策面\"放置的位置优于分类器C的\"决策面\"放置的位置，SVM算法也是这么认为的，它的依据就是分类器B的分类间隔比分类器C的分类间隔大。\n",
    "  \n",
    "　　这里涉及到第一个SVM独有的概念\"分类间隔\"。在保证决策面方向不变且不会出现错分样本的情况下移动决策面，会在原来的决策面两侧找到两个极限位置（越过该位置就会产生错分现象），如虚线所示。虚线的位置由决策面的方向和距离原决策面最近的几个样本的位置决定。\n",
    "   \n",
    "　　而这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。两条虚线之间的垂直距离就是这个最优决策面对应的分类间隔。显然每一个可能把数据集正确分开的方向都有一个最优决策面（有些方向无论如何移动决策面的位置也不可能将两类样本完全分开），而不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为\"支持向量\"。\n",
    "  \n",
    "  \n",
    "# 3 - 数学建模\n",
    "\n",
    "　　 求解这个\"决策面\"的过程，就是最优化。一个最优化问题通常有两个基本的因素：1）目标函数，也就是你希望什么东西的什么指标达到最好；2）优化对象，你期望通过改变哪些因素来使你的目标函数达到最优。在线性SVM算法中，目标函数显然就是那个\"分类间隔\"，而优化对象则是决策面。所以要对SVM问题进行数学建模，首先要对上述两个对象（\"分类间隔\"和\"决策面\"）进行数学描述。按照一般的思维习惯，我们先描述决策面。\n",
    "   \n",
    "数学建模的时候，先在二维空间建模，然后再推广到多维。\n",
    "  \n",
    "（1）“决策面”方程\n",
    "  \n",
    "我们都知道二维空间下一条直线的方式如下所示：\n",
    "\n",
    "$y=ax+b$\n",
    "  \n",
    "  \n",
    "现在我们做个小小的改变，让原来的x轴变成x1，y轴变成x2：\n",
    "\n",
    "$x_2 = ax_1+b$\n",
    "\n",
    "移项得：\n",
    "\n",
    "$ax_1-x_2+b=0$\n",
    "\n",
    "将公式向量化得：\n",
    " $$ \\begin{bmatrix}\n",
    " a&-1 \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    " x1\\\\\n",
    " x2\n",
    "\\end{bmatrix}+b=0$$\n",
    "进一步向量化，用w列向量和x列向量和标量γ进一步向量化：\n",
    "  $$w^Tx+\\gamma=0$$\n",
    "其中，向量$w$和$x$分别为：\n",
    "  $$w=[w_1,w_2]^T,x=[x_1,x_2]^T$$\n",
    "\n",
    "这里$w1=a$，$w2=-1$。我们都知道，最初的那个直线方程$a$和$b$的几何意义，$a$表示直线的斜率，$b$表示截距，$a$决定了直线与$x$轴正方向的夹角，$b$决定了直线与$y$轴交点位置。那么向量化后的直线的$w$和$\\gamma$的几何意义是什么呢？  \n",
    "\n",
    "现在假设：\n",
    "$$a=\\sqrt{3},b=0$$\n",
    "\n",
    "可得：\n",
    "$$w=[\\sqrt{3},-1]^T$$\n",
    "\n",
    "在坐标轴上画出直线和向量W：\n",
    "![ml_8_19.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E5%BB%BA%E6%A8%A11.png?raw=true)\n",
    "\n",
    "蓝色的线代表向量w，红色的线代表直线y。我们可以看到向量w和直线的关系为垂直关系。这说明了向量w也控制这直线的方向，只不过是与这个直线的方向是垂直的。标量γ的作用也没有变，依然决定了直线的截距。此时，我们称w为直线的法向量。\n",
    "\n",
    "二维空间的直线方程已经推导完成，将其推广到n维空间，就变成了超平面方程。(一个超平面，在二维空间的例子就是一个直线)但是它的公式没变，依然是：\n",
    "$$w^Tx+\\gamma=0$$\n",
    "\n",
    "不同之处在于：\n",
    "$$w=[w_1,w_2,\\dots,w_n]^T$$\n",
    "\n",
    "$$x=[x_1,x_2,\\dots,x_n]^T$$\n",
    "我们已经顺利推导出了\"决策面\"方程，它就是我们的超平面方程，之后，我们统称其为超平面方程。\n",
    "\n",
    "（2）\"分类间隔\"方程\n",
    "\n",
    "现在，我们依然对于一个二维平面的简单例子进行推导。\n",
    "\n",
    "![ml_8_22.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E5%BB%BA%E6%A8%A12.png?raw=true)\n",
    "\n",
    "我们已经知道间隔的大小实际上就是支持向量对应的样本点到决策面的距离的二倍。那么图中的距离d我们怎么求？我们高中都学过，点到直线的距离距离公式如下：\n",
    "\n",
    "$$d = \\left |\\frac{Ax_0+By_0+C}{\\sqrt{A^2+B^2}}\\right|$$\n",
    "\n",
    "公式中的直线方程为$Ax_0+By_0+C=0$，点P的坐标为$(x_0,y_0)$\n",
    "\n",
    "现在，将直线方程扩展到多维，求得我们现在的超平面方程，对公式进行如下变形：\n",
    "\n",
    "$$d=\\left|\\frac{w^Tx+\\gamma}{\\left \\| w \\right \\|}\\right|$$\n",
    "\n",
    "这个d就是\"分类间隔\"。其中||w||表示w的二范数，求所有元素的平方和，然后再开方。比如对于二维平面：\n",
    "\n",
    "$$w=[w_1,w_2]^T$$\n",
    "\n",
    "那么：\n",
    "$$||w||=\\sqrt{w_1^2+w_2^2}$$\n",
    "\n",
    "我们目的是为了找出一个分类效果好的超平面作为分类器。分类器的好坏的评定依据是分类间隔W=2d的大小，即分类间隔w越大，我们认为这个超平面的分类效果越好。此时，求解超平面的问题就变成了求解分类间隔W最大化的为题。W的最大化也就是d最大化的。\n",
    "\n",
    "（3）约束条件\n",
    "\n",
    "看起来，我们已经顺利获得了目标函数的数学形式。但是为了求解w的最大值。我们不得不面对如下问题：\n",
    "\n",
    "* 我们如何判断超平面是否将样本点正确分类？\n",
    "\n",
    "\n",
    "* 我们知道要求距离d的最大值，我们首先需要找到支持向量上的点，怎么在众多的点中选出支持向量上的点呢？\n",
    "\n",
    "\n",
    "上述我们需要面对的问题就是约束条件，也就是说我们优化的变量d的取值范围受到了限制和约束。事实上约束条件一直是最优化问题里最让人头疼的东西。但既然我们已经知道了这些约束条件确实存在，就不得不用数学语言对他们进行描述。但SVM算法通过一些巧妙的小技巧，将这些约束条件融合到一个不等式里面。\n",
    "\n",
    "这个二维平面上有两种点，我们分别对它们进行标记：\n",
    "\n",
    "* 红颜色的圆点标记为1，我们人为规定其为正样本；\n",
    "\n",
    "\n",
    "* 蓝颜色的五角星标记为-1，我们人为规定其为负样本。\n",
    "\n",
    "\n",
    "对每个样本点xi加上一个类别标签$y_i$：\n",
    "\n",
    "$$y_i=\\begin{cases}\n",
    "+1 & \\text{ red }  \\\\ \n",
    " -1& \\text{ blue } \n",
    "\\end{cases}$$\n",
    "\n",
    "如果我们的超平面方程能够完全正确地对上图的样本点进行分类，就会满足下面的方程：\n",
    "\n",
    "$$\\begin{cases}\n",
    "w^Tx_i+\\gamma>0 & y_i=1  \\\\ \n",
    "w^Tx_i+\\gamma<0& y_i=-1 \n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式进一步写成：\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\frac{w^Tx_i+\\gamma}{||w||}\\geq d & \\forall y_i=1\\\\ \n",
    " \\frac{w^Tx_i+\\gamma}{||w||}\\geq -d & \\forall y_i=-1 \n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "上述公式的解释就是，对于所有分类标签为1和-1样本点，它们到直线的距离都大于等于d(支持向量上的样本点到超平面的距离)。公式两边都除以d，就可以得到：\n",
    "\n",
    "$$\\begin{cases}\n",
    "w_d^Tx_i+\\gamma_d>1 & y_i=1  \\\\ \n",
    "w_d^Tx_i+\\gamma_d<-1& y_i=-1 \n",
    "\\end{cases}$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$w_d=\\frac{w}{||w||d}$$\n",
    "\n",
    "$$\\gamma_d=\\frac{\\gamma}{||w||d}$$\n",
    "\n",
    "\n",
    "因为||w||和d都是标量。所以上述公式的两个矢量，依然描述一条直线的法向量和截距。\n",
    "\n",
    "$$w_d^T+\\gamma_d=0$$\n",
    "\n",
    "$$w^Tx+\\gamma=0$$\n",
    "\n",
    "上述两个公式，都是描述一条直线，数学模型代表的意义是一样的。现在，让我们对wd和γd重新起个名字，就叫它们w和γ。因此，我们就可以说：\"对于存在分类间隔的两类样本点，我们一定可以找到一些超平面，使其对于所有的样本点均满足下面的条件：\"\n",
    "\n",
    "$$\\begin{cases}\n",
    "w^Tx_i+\\gamma>1 & y_i=1  \\\\ \n",
    "w^Tx_i+\\gamma<-1& y_i=-1 \n",
    "\\end{cases}$$\n",
    "\n",
    "上述方程即给出了SVM最优化问题的约束条件。这时候，可能有人会问了，为什么标记为1和-1呢？因为这样标记方便我们将上述方程变成如下形式：\n",
    "\n",
    "$$y_i(w^Tx_i+\\gamma)\\geq 1$$\n",
    "\n",
    "正是因为标签为1和-1，才方便我们将约束条件变成一个约束方程，从而方便我们的计算。\n",
    "\n",
    "（4）线性SVM优化问题基本描述\n",
    "\n",
    "现在整合一下思路，我们已经得到我们的目标函数：\n",
    "\n",
    "$$d=\\frac{|w^Tx+\\gamma|}{||w||}$$\n",
    "\n",
    "我们的优化目标是是d最大化。我们已经说过，我们是用支持向量上的样本点求解d的最大化的问题的。那么支持向量上的样本点有什么特点呢？\n",
    "\n",
    "$$|w^Tx_i+\\gamma|=1$$\n",
    "\n",
    "现在我们就可以将我们的目标函数进一步化简：\n",
    "\n",
    "$$d=\\frac{1}{||w||}$$\n",
    "\n",
    "因为，我们只关心支持向量上的点。随后我们求解d的最大化问题变成了||w||的最小化问题。进而||w||的最小化问题等效于\n",
    "\n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "\n",
    "为什么要做这样的等效呢？这是为了在进行最优化的过程中对目标函数求导时比较方便，但这绝对不影响最优化问题最后的求解。我们将最终的目标函数和约束条件放在一起进行描述：\n",
    "\n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "\n",
    "$$s.t.　y_i(w^Tx_i+b)\\geq 1,i=1,2,\\dots,n$$\n",
    "\n",
    "这里n是样本点的总个数，缩写s.t.表示\"Subject to\"，是\"服从某某条件\"的意思。上述公式描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是支持向量机的基本数学模型。\n",
    "\n",
    "（5）求解准备\n",
    "\n",
    "我们已经得到支持向量机的基本数学模型，接下来的问题就是如何根据数学模型，求得我们想要的最优解。在学习求解方法之前，我们得知道一点，想用我下面讲述的求解方法有一个前提，就是我们的目标函数必须是凸函数。理解凸函数，我们还要先明确另一个概念，凸集。在凸几何中，凸集(convex set)是在)凸组合下闭合的放射空间的子集。看一幅图可能更容易理解：\n",
    "\n",
    "![ml_8_41.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E6%B1%82%E8%A7%A31.png?raw=true)\n",
    "\n",
    "左右量图都是一个集合。如果集合中任意2个元素连线上的点也在集合中，那么这个集合就是凸集。显然，上图中的左图是一个凸集，上图中的右图是一个非凸集。\n",
    "\n",
    "凸函数的定义也是如此，其几何意义表示为函数任意两点连线上的值大于对应自变量处的函数值。若这里凸集C即某个区间L，那么，设函数f为定义在区间L上的函数，若对L上的任意两点x1，x2和任意的实数λ，λ属于(0,1)，总有：\n",
    "\n",
    "$$f(\\lambda x_1+(1-\\lambda)x_2)\\leq \\lambda f(x_1)+(1-\\lambda)f(x_2)$$\n",
    "\n",
    "则函数f称为L上的凸函数，当且仅当其上镜图（在函数图像上方的点集）为一个凸集。再看一幅图，也许更容易理解：\n",
    "\n",
    "\n",
    "![ml_8_43_modify.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E6%B1%82%E8%A7%A32.png?raw=true)\n",
    "\n",
    "像上图这样的函数，它整体就是一个非凸函数，我们无法获得全局最优解的，只能获得局部最优解。比如红框内的部分，如果单独拿出来，它就是一个凸函数。对于我们的目标函数：\n",
    "\n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "\n",
    "通常我们需要求解的最优化问题有如下几类：\n",
    "\n",
    "* 无约束优化问题，可以写为：\n",
    "\n",
    "$$min　f(x)$$\n",
    "\n",
    "* 有等式约束的优化问题，可以写为：\n",
    "\n",
    "$$min　f(x)$$\n",
    "\n",
    "$$s.t.　h_i(x)=0,i=1,2,\\dots,n$$\n",
    "\n",
    "* 有不等式约束的优化问题，可以写为：\n",
    "\n",
    "$$min　f(x)$$\n",
    "\n",
    "$$s.t.　g_i(x)\\leq 0,i=1,2,\\dots,n$$\n",
    "\n",
    "$$h_j(x)=0,j=1,2,\\dots,m$$\n",
    "\n",
    "对于第(a)类的优化问题，尝试使用的方法就是费马大定理(Fermat)，即使用求取函数f(x)的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。这也就是我们高中经常使用的求函数的极值的方法。\n",
    "\n",
    "对于第(b)类的优化问题，常常使用的方法就是拉格朗日乘子法（Lagrange Multiplier) ，即把等式约束h_i(x)用一个系数与f(x)写为一个式子，称为拉格朗日函数，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。\n",
    "\n",
    "对于第(c)类的优化问题，常常使用的方法就是KKT条件。同样地，我们把所有的等式、不等式约束与f(x)写为一个式子，也叫拉格朗日函数，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的必要条件，这个条件称为KKT条件。\n",
    "\n",
    "必要条件和充要条件如果不理解，可以看下面这句话：\n",
    "\n",
    "* A的必要条件就是A可以推出的结论\n",
    "\n",
    "* A的充分条件就是可以推出A的前提\n",
    "\n",
    "了解到这些，现在让我们再看一下我们的最优化问题：\n",
    "\n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "\n",
    "$$s.t.　y_i(w^Tx_i+b)\\geq 1,i=1,2,\\dots,n$$\n",
    "\n",
    "现在，我们的这个对优化问题属于哪一类？很显然，它属于第(c)类问题。在学习求解最优化问题之前，我们还要学习两个东西：拉格朗日函数和KKT条件。\n",
    "\n",
    "（6）拉格朗日函数\n",
    "\n",
    "首先，我们先要从宏观的视野上了解一下拉格朗日对偶问题出现的原因和背景。\n",
    "\n",
    "我们知道我们要求解的是最小化问题，所以一个直观的想法是如果我能够构造一个函数，使得该函数在可行解区域内与原目标函数完全一致，而在可行解区域外的数值非常大，甚至是无穷大，那么这个没有约束条件的新目标函数的优化问题就与原来有约束条件的原始目标函数的优化问题是等价的问题。这就是使用拉格朗日方程的目的，它将约束条件放到目标函数中，从而将有约束优化问题转换为无约束优化问题。\n",
    "\n",
    "随后，人们又发现，使用拉格朗日获得的函数，使用求导的方法求解依然困难。进而，需要对问题再进行一次转换，即使用一个数学技巧：拉格朗日对偶。\n",
    "\n",
    "所以，显而易见的是，我们在拉格朗日优化我们的问题这个道路上，需要进行下面二个步骤：\n",
    "\n",
    "* 将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数\n",
    "\n",
    "* 使用拉格朗日对偶性，将不易求解的优化问题转化为易求解的优化\n",
    "\n",
    "下面，进行第一步：将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数\n",
    "\n",
    "公式变形如下：\n",
    "\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)$$\n",
    "\n",
    "其中αi是拉格朗日乘子，αi大于等于0，是我们构造新目标函数时引入的系数变量(我们自己设置)。现在我们令：\n",
    "\n",
    "$$\\theta(w)=\\underset{\\alpha_i\\geq 0}{max}L(w,b,\\alpha)$$\n",
    "\n",
    "当样本点不满足约束条件时，即在可行解区域外：\n",
    "\n",
    "$$y_i(w^Tx_i+b)<1$$\n",
    "\n",
    "此时，我们将αi设置为正无穷，此时θ(w)显然也是正无穷。\n",
    "\n",
    "当样本点满足约束条件时，即在可行解区域内：\n",
    "\n",
    "$$y_i(w^Tx_i+b)\\geq 1$$\n",
    "\n",
    "此时，显然θ(w)为原目标函数本身。我们将上述两种情况结合一下，就得到了新的目标函数：\n",
    "\n",
    "$$\\theta(w)=\\begin{cases}\n",
    "\\frac{1}{2}||w||^2 & x\\in feasible  \\\\ \n",
    "\\infty & x\\in not feasible\n",
    "\\end{cases}$$\n",
    "\n",
    "此时，再看我们的初衷，就是为了建立一个在可行解区域内与原目标函数相同，在可行解区域外函数值趋近于无穷大的新函数，现在我们做到了。\n",
    "\n",
    "现在，我们的问题变成了求新目标函数的最小值，即：\n",
    "\n",
    "$$\\underset{w,b}{min}\\theta(w)=\\underset{w,b}{min}\\underset{\\alpha_i \\geq 0}{max}L(w,b,\\alpha)=p^*$$\n",
    "\n",
    "这里用p*表示这个问题的最优值，且和最初的问题是等价的。\n",
    "\n",
    "接下来，我们进行第二步：将不易求解的优化问题转化为易求解的优化\n",
    "\n",
    "我们看一下我们的新目标函数，先求最大值，再求最小值。这样的话，我们首先就要面对带有需要求解的参数w和b的方程，而αi又是不等式约束，这个求解过程不好做。所以，我们需要使用拉格朗日函数对偶性，将最小和最大的位置交换一下，这样就变成了：\n",
    "\n",
    "$$\\underset{\\alpha_i\\geq 0}{max}\\underset{w,b}{min}L(w,b,\\alpha)=d^*$$\n",
    "\n",
    "交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用d*来表示。而且d*<=p*。我们关心的是d=p的时候，这才是我们要的解。需要什么条件才能让d=p呢？\n",
    "\n",
    "* 首先必须满足这个优化问题是凸优化问题。\n",
    "\n",
    "\n",
    "* 其次，需要满足KKT条件。\n",
    "\n",
    "\n",
    "凸优化问题的定义是：求取最小值的目标函数为凸函数的一类优化问题。目标函数是凸函数我们已经知道，这个优化问题又是求最小值。所以我们的最优化问题就是凸优化问题。\n",
    "\n",
    "接下里，就是探讨是否满足KKT条件了。\n",
    "\n",
    "（7）KKT条件\n",
    "\n",
    "我们已经使用拉格朗日函数对我们的目标函数进行了处理，生成了一个新的目标函数。通过一些条件，可以求出最优值的必要条件，这个条件就是接下来要说的KKT条件。一个最优化模型能够表示成下列标准形式：\n",
    "\n",
    "$$minf(x)$$\n",
    "\n",
    "$$s.t.　h_j(x)=0,j=1,2,\\dots,p$$\n",
    "\n",
    "$$g_k(x)\\leq 0,k=1,2,\\dots,q$$\n",
    "\n",
    "$$x\\in X\\subset \\mathbb{R}$$\n",
    "\n",
    "KKT条件的全称是Karush-Kuhn-Tucker条件，KKT条件是说最优值条件必须满足以下条件：\n",
    "\n",
    "* 条件一：经过拉格朗日函数处理之后的新目标函数L(w,b,α)对x求导为零：\n",
    "\n",
    "* 条件二：h(x) = 0；\n",
    "\n",
    "* 条件三：α*g(x) = 0；\n",
    "\n",
    "现在，凸优化问题和KKT都满足了，问题转换成了对偶问题。而求解这个对偶学习问题，可以分为三个步骤：首先要让L(w,b,α)关于w和b最小化，然后求对α的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。现在，我们继续推导。\n",
    "\n",
    "（8）对偶问题求解\n",
    "\n",
    "第一步：\n",
    "\n",
    "根据上述推导已知：\n",
    "\n",
    "$$\\underset{\\alpha_i\\geq 0}{max}\\underset{w,b}{min}L(w,b,\\alpha)=d^*$$\n",
    "\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)$$\n",
    "\n",
    "首先固定α，要让L(w,b,α)关于w和b最小化，我们分别对w和b偏导数，令其等于0，即：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w}=0\\Rightarrow w=\\sum_{i=1}^n\\alpha_iy_ix_i$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w}=0\\Rightarrow \\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "将上述结果带回L(w,b,α)得到：\n",
    "\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum_{i=1}^n\\alpha_i(y_i(w^Tx_i+b)-1)$$\n",
    "\n",
    "$$\\frac{1}{2}w^Tw-w^T\\sum_{i=1}^n\\alpha_iy_ix_i-b\\sum_{i=1}^n\\alpha_iy_i+\\sum_{i=1}^n\\alpha_i$$\n",
    "\n",
    "$$=\\frac{1}{2}w^T\\sum_{i=1}^n\\alpha_iy_ix_i-w^T\\sum_{i=1}^n\\alpha_iy_ix_i-b\\cdot 0+\\sum_{i=1}^n\\alpha_i$$\n",
    "\n",
    "$$=\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}(\\sum_{i=1}^n\\alpha_iy_ix_i)^T\\sum_{i=1}^n\\alpha_iy_ix_i$$\n",
    "\n",
    "$$=\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_j y_iy_jx_i^Tx_j$$\n",
    "\n",
    "从上面的最后一个式子，我们可以看出，此时的L(w,b,α)函数只含有一个变量，即αi。\n",
    "\n",
    "第二步：\n",
    "\n",
    "现在内侧的最小值求解完成，我们求解外侧的最大值，从上面的式子得到\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_j^Tx_j$$\n",
    "\n",
    "$$s.t.　\\alpha_i\\geq 0,i=1,2,\\dots,n$$\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "现在我们的优化问题变成了如上的形式。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。我们通过这个优化算法能得到α，再根据α，我们就可以求解出w和b，进而求得我们最初的目的：找到超平面，即\"决策平面\"。\n",
    "\n",
    "对于上述问题，我们就可以使用SMO算法进行求解了，但是，SMO算法又是什么呢？ \n",
    "\n",
    "# 4 - SMO算法推导\n",
    "\n",
    "现在，我们已经得到了可以用SMO算法求解的目标函数，但是对于怎么编程实现SMO算法还是感觉无从下手。那么现在就聊聊如何使用SMO算法进行求解。\n",
    "\n",
    "（1）Platt的SMO算法\n",
    "\n",
    "1996年，John Platt发布了一个称为SMO的强大算法，用于训练SVM。SM表示序列最小化(Sequential Minimal Optimizaion)。Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果完全一致的。在结果完全相同的同时，SMO算法的求解时间短很多。\n",
    "\n",
    "SMO算法的目标是求出一系列$\\alpha$和$b$，一旦求出了这些$\\alpha$，就很容易计算出权重向量$w$并得到分隔超平面。\n",
    "\n",
    "SMO算法的工作原理是：每次循环中选择两个$\\alpha$进行优化处理。一旦找到了一对合适的$\\alpha$，那么就增大其中一个同时减小另一个。这里所谓的\"合适\"就是指两个$\\alpha$必须符合以下两个条件，条件之一就是两个$\\alpha$必须要在间隔边界之外，而且第二个条件则是这两个$\\alpha$还没有进行过区间化处理或者不在边界上。\n",
    "\n",
    "（2）SMO算法的解法\n",
    "先来定义特征到结果的输出函数为：\n",
    "\n",
    "$$u=w^Tx+b$$\n",
    "\n",
    "接着，我们回忆一下原始优化问题，如下：\n",
    "\n",
    "$$min\\frac{1}{2}||w||^2$$\n",
    "\n",
    "$$s.t.　y_i(w^Tx_i+b)\\geq 1,i=1,2,\\dots,n$$\n",
    "\n",
    "求导得：\n",
    "\n",
    "$$w=\\sum_{i=1}^n\\alpha_iy_ix_i$$\n",
    "\n",
    "\n",
    "将上述公式代入输出函数中：\n",
    "\n",
    "$$u=\\sum_{i=1}^n\\alpha_iy_ix_i^Tx+b$$\n",
    "\n",
    "与此同时，拉格朗日对偶后得到最终的目标化函数：\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j$$\n",
    "\n",
    "$$s.t.　\\alpha_i\\geq 0,i=1,2,\\dots,n$$\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "将目标函数变形，在前面增加一个符号，将最大值问题转换成最小值问题：\n",
    "\n",
    "$$min\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j-\\sum_{j=1}^n\\alpha_i$$\n",
    "\n",
    "$$s.t. \\alpha_i \\geq 0,i=1,2,\\dots,n$$\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "实际上，对于上述目标函数，是存在一个假设的，即数据100%线性可分。但是，目前为止，我们知道几乎所有数据都不那么\"干净\"。这时我们就可以通过引入所谓的松弛变量ξ(slack variable)和惩罚参数C，来允许有些数据点可以处于超平面的错误的一侧。此时我们的约束条件有所改变：\n",
    "\n",
    "$$s.t.　C\\geq\\alpha_i\\geq 0,i=1,2,\\dots,n$$\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "同时，考虑到松弛变量和松弛变量ξ和惩罚参数C，目标函数变为：\n",
    "\n",
    "$$\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\varepsilon_i$$\n",
    "\n",
    "原始问题的拉格朗日函数变为：\n",
    "\n",
    "$$L(w,b,\\varepsilon,\\alpha,u)=\\frac{1}{2}||w||^2+C/sum_{i=1}^N\\varepsilon_i$$\n",
    "\n",
    "对偶问题拉格朗日函数的极大极小问题，得到以下等价优化问题：\n",
    "\n",
    "$$\\underset{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i$$\n",
    "\n",
    "$$s.t.　\\sum_{i=1}^N\\alpha_iy_i=0$$\n",
    "\n",
    "$$0\\leq \\alpha_i\\leq C,i=1,2,\\dots,N$$\n",
    "\n",
    "则，原始问题的解对偶问题的解相同需要满足KKT对偶互补条件，即：\n",
    "\n",
    "$$\\alpha_i(y_i(w\\cdot x_i+b)-1+\\varepsilon_i)=0$$\n",
    "\n",
    "$$u_i\\varepsilon_i$$\n",
    "\n",
    "对样本点$x_i$，记SVM的输出结果为：\n",
    "$$u_i=w\\cdot x_i+b$$\n",
    "\n",
    "Platt在序列最小优化（SMO）方法1中提到，对正定二次优化问题（a positive definite QP problem）的优化点的充分必要条件为KKT条件（Karush-Kuhn-Tucker conditions）。\n",
    "对于所有的i，若满足以下条件，QP问题可解。KKT条件如下：\n",
    "$$\\alpha_i=0\\Leftrightarrow y_iu_i\\geq 1$$\n",
    "$$0<\\alpha_i<C\\Leftrightarrow y_iu_i=1$$\n",
    "$$\\alpha_i=C\\Leftrightarrow y_iu_i\\leq 1$$\n",
    "\n",
    "其中$y_iu_i$就是每个样本点的函数间隔\n",
    "\n",
    "因此：\n",
    "\n",
    "### 在间隔边界上\n",
    "\n",
    "当$0<\\alpha_i^*<C$时，$y_iu_i=1$,则分类正确，且$u_i=\\pm 1$,即在分类间隔边界上\n",
    "\n",
    "### 在间隔边界与分离超平面之间\n",
    "\n",
    "当$\\alpha_i^*=C$，$0<\\varepsilon_i<1$时，得\n",
    "$$0<y_iu_i<1$$\n",
    "则说明$y_i,u_i$同号，分类正确，且函数间隔小于1，即在间隔边界内\n",
    "\n",
    "### 在分离超平面上\n",
    "\n",
    "当$\\alpha_i^*=C,\\varepsilon_i=1$时，得\n",
    "$$y_iu_i=0\\Rightarrow u_i=0$$\n",
    "即$x_i$在分离超平面上\n",
    "\n",
    "### 在分离超平面误分一侧\n",
    "\n",
    "当$\\alpha_i^*=C,\\varepsilon_i>1$得\n",
    "$$y_iu_i<0$$\n",
    "则分类错误，$x_i$在分离超平面误分一侧\n",
    "\n",
    "而最优解需要满足KKT条件，即上述3个条件都得满足，如果存在不能满足KKT条件的αi，那么需要更新这些αi，这是第一个约束条件。此外，更新的同时还要受到第二个约束条件的限制，即：\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "因为这个条件，我们同时更新两个α值，因为只有成对更新，才能保证更新之后的值仍然满足和为0的约束，假设我们选择的两个乘子为α1和α2：\n",
    "\n",
    "$$\\alpha_1^{new}y_1+\\alpha_2^{new}=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\zeta $$\n",
    "\n",
    "当$y_1$不等于$y_2$时，即一个为正1，一个为负1的时候，可以得到：\n",
    "$$\\alpha_1^{old}-\\alpha_2^{old}=\\zeta$$\n",
    "\n",
    "所以有：\n",
    "\n",
    "$$L=max(0,-\\zeta),H=min(C,C-\\zeta)$$\n",
    "\n",
    "此时，取值范围如下图所示：\n",
    "![ml_8_77.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E5%88%86%E7%A6%BB1.png?raw=true)\n",
    "\n",
    "当$y_1$等于$y_2$时，即两个都为正1或者都为负1，可得到：\n",
    "$$\\alpha_1^{old]+\\alpha_2^{old}=\\zeta$$\n",
    "\n",
    "所以有：\n",
    "$$L = max(0,\\zeta-c),H=min(C,\\zeta)$$\n",
    "\n",
    "此时，取值范围如下图所示：\n",
    "![ml_8_80.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E5%88%86%E7%A6%BB2.png?raw=true)\n",
    "\n",
    "如此，根据$y_1$和$y_2$异号或同号，可以得出α2 new的上下界分别为：\n",
    "\n",
    "$$\\begin{cases}\n",
    "L=max(0,\\alpha_2^{old}-\\alpha_1^{old}),H=min(C,C+\\alpha_2^{old}-\\alpha_1^{old}) & \\text{ if } y_1 \\neq y_2 \\\\ \n",
    "L=max(0,\\alpha_2^{old}-\\alpha_1^{old}-C),H=min(C,\\alpha_2^{old}+\\alpha_1^{old}) & \\text{ if } y_1 = y_2 & \\text{ if } x= \n",
    "\\end{cases}$$\n",
    "\n",
    "这个界限就是编程的时候需要用到的。已经确定了边界，接下来，就是推导迭代式，用于更新 α值。\n",
    "\n",
    "我们已经知道，更新α的边界，接下来就是讨论如何更新α值。我们依然假设选择的两个乘子为α1和α2。\n",
    "\n",
    "为了方便描述，我们定义如下符号：\n",
    "$$f(x_i)=\\sum_{j=1}^n\\alpha_jy_jx_i^Tx_j+b$$\n",
    "\n",
    "$$v_i=\\sum_{j=3}^n\\alpha_jy_jx_i^Tx_j=f(x_i)-\\sum_{j=1}^2\\alpha_jy_jx_i^Tx_j-b$$\n",
    "\n",
    "最终目标函数变为：\n",
    "$$w(\\alpha_2)=\\alpha_1+\\alpha_2-\\frac{1}{2}\\alpha_1^2x_1^Tx_1-\\frac{1}{2}\\alpha_2^2x_2^Tx_2-y_1y_2\\alpha_1\\alpha_2x_1^Tx_2-y_1\\alpha_1v_1-y_2\\alpha_2v_2+constant$$\n",
    "\n",
    "我们不关心constant的部分，因为对于α1和α2来说，它们都是常数项，在求导的时候，直接变为0。对于这个目标函数，如果对其求导，还有个未知数α1，所以要推导出α1和α2的关系，然后用α2代替α1，这样目标函数就剩一个未知数了，我们就可以求导了，推导出迭代公式。所以现在继续推导α1和α2的关系。注意第一个约束条件：\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "我们在求α1和α2的时候，可以将α3,α4,...,αn和y3,y4,...,yn看作常数项。因此有：\n",
    "$$\\alpha_1y_1+\\alpha_2y_2=B$$\n",
    "\n",
    "我们不必关心常数B的大小，现在将上述等式两边同时乘以y1，得到(y1y1=1)：\n",
    "$$\\alpha_1=\\gamma-s\\alpha_2$$\n",
    "\n",
    "其中γ为常数By1，我们不关心这个值，s=y1y2。接下来，我们将得到的α1带入W(α2)公式得：\n",
    "$$w(\\alpha_2)=\\gamma-s\\alpha_2+\\alpha_2-\\frac{1}{2}(\\gamma-sa_2)^2x_1^Tx_1-\\frac{1}{2}\\alpha_2x_2^Tx_2-s(\\gamma-s\\alpha_2)\\alpha_2x_1^Tx_2-y_1(\\gamma-s\\alpha_2)v_1-y_2\\alpha_2v_2+constant$$\n",
    "\n",
    "这样目标函数中就只剩下α2了，我们对其求偏导（注意：s=y1y2，所以s的平方为1，y1的平方和y2的平方均为1）：\n",
    "\n",
    "$$\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2}=-s+1+\\gamma sx_1^Tx_1-\\alpha_2x_1^Tx_1-\\alpha_2x_2^Tx_2-\\gamma sx_1^Tx_2+2\\alpha_2x_1^Tx_2+y_2v_1-y_2v_2=0$$\n",
    "\n",
    "继续简化，将$s=y_1y_2$代入方程\n",
    "$$\\alpha_2^{new}=\\frac{y_2(y_2-y_1+y_1\\gamma(x_1^Tx_1-x_1^Tx_2)+v_1-v_2)}{x_1^Tx_1+x_2^Tx_2-2x_1^Tx_2}$$\n",
    "\n",
    "我们令：\n",
    "$$E_i=f(x_i)-y_i$$\n",
    "$$\\eta =x_1^Tx_1+x_2^Tx_2-2x_1^Tx_2$$\n",
    "\n",
    "$E_i$为误差项，$\\eta$为学习速率。\n",
    "\n",
    "再根据我们已知的公式：\n",
    "$$\\gamma=\\alpha_1^{old}+s\\alpha_2^{old}$$\n",
    "$$v_j=\\sum_{i=3}^n\\alpha_iy_ix_j^Tx_i=f(x_j)-\\sum_{i=1}^2\\alpha_iy_ix_j^T-b$$\n",
    "\n",
    "将$\\alpha_2^{new}$继续优化简得：\n",
    "$$\\alpha_2^{new}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}$$\n",
    "\n",
    "这样，我们就得到了最终需要的迭代公式，然后需要考虑约束条件：\n",
    "$$0<\\alpha_i<C$$\n",
    "\n",
    "根据之前推导的$\\alpha$取值范围，我们得到最终的解析式为：\n",
    "$$\\alpha_2^{new,clipped}=\\begin{cases}\n",
    "H  &  \\alpha_2^{new}>H \\\\ \n",
    "\\alpha_2^{new} &  L\\leq \\alpha_2^{new}\\leq H \\\\ \n",
    "L &  \\alpha_2^{new}<L\n",
    "\\end{cases}$$\n",
    "\n",
    "又因为：\n",
    "\n",
    "$$\\alpha_1^{old}=\\gamma-s\\alpha_2^{old}$$\n",
    "$$\\alpha_1^{new}=\\gamma-s\\alpha_2^{new,clipped}$$\n",
    "\n",
    "消去$\\gamma$得：\n",
    "$$\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new,clipped})$$\n",
    "这样，我们就知道了怎样计算α1和α2了，也就是如何对选择的α进行更新。\n",
    "\n",
    "当我们更新了α1和α2之后，需要重新计算阈值b，因为b关系到了我们f(x)的计算，也就关系到了误差Ei的计算。\n",
    "\n",
    "我们要根据α的取值范围，去更正b的值，使间隔最大化。当α1 new在0和C之间的时候，根据KKT条件可知，这个点是支持向量上的点。因此，满足下列公式：\n",
    "\n",
    "$$y1(w^Tx_1+b)=1$$\n",
    "公式两边同时乘以y1得$（y_1y_1=1）$\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_ix_ix_1+b=y_1$$\n",
    "\n",
    "因为我们是根据$\\alpha_1$和$\\alpha_2$的值去更新b，所以单独提出$i=1$和$i=2$的时候，整理可得：\n",
    "$$b_1^{new}=y_1-\\sum_{i=3}^n\\alpha_iy_ix_i^Tx_1-\\alpha_1^{new}y_1x_1^Tx_1-\\alpha_2^{new}y_2x_2^Tx_1$$\n",
    "\n",
    "其中两项为：\n",
    "$$y_1-\\sum_{i=3}^n\\alpha_iy_ix_i^Tx_1=-E_i+\\alpha_1^{old}y_1x_1^Tx_1+\\alpha_2^{old}y_2x_2^Tx_1+b^{old}$$\n",
    "\n",
    "将上述两个公式，整理得：\n",
    "$$b_1^{new}=b^{old}-E_2-y_1(\\alpha_1^{new}-\\alpha_1^{old})x_1^Tx_2-y_2(\\alpha_2^{new}-\\alpha_2^{old})x_2^Tx_2$$\n",
    "\n",
    "当b1和b2都有效的时候，它们是相等的，即：\n",
    "$$b^{new}=b_1^{new}=b_2^{new}$$\n",
    "当两个乘子都在边界上，则$b$阈值和KKT条件一直，当不满足条件的时候，SMO算法选择它们的中点作为新的阈值：\n",
    "$$b=\\begin{cases}\n",
    "    b_1  & 0<\\alpha_1^{new}<C \\\\ \n",
    "b_2 & 0<\\alpha_2^{new}<C \\\\ \n",
    "(b_1+b_2)/2 & otherwise \n",
    "\\end{cases}$$\n",
    "\n",
    "最后，更新所有的a和b，这样模型就出来了，从而即可求出我们的分类函数。\n",
    "\n",
    "## 5 - SMO算法总结\n",
    "\n",
    "* 步骤１：计算误差：\n",
    "\n",
    "$$E_i=f(x_i)-y_i=\\sum_{j=1}^n\\alpha_jy_jx_i^Tx_j+b-y_i$$\n",
    "\n",
    "* 步骤2：计算上下界L和H：\n",
    "\n",
    "$$\\begin{cases}\n",
    "L=max(0,\\alpha_j^{old}-\\alpha_i^{old})& H = min(C,C+\\alpha_j^{old}-\\alpha_i^{old}) \\text{ if } y_i \\neq y_j \\\\\n",
    "L=max(0,\\alpha_j^{old}-\\alpha_i^{old}-C) & H = min(C,C+\\alpha_j^{old}+\\alpha_i^{old})\\text{ if } y_i \\neq y_j \n",
    "\\end{cases}$$\n",
    "\n",
    "* 步骤3：计算$\\eta$\n",
    "\n",
    "$$\\eta=x_i^Tx_i+x_j^Tx_i-2x_i^Tx_j$$\n",
    "\n",
    "* 步骤4：更新$\\alpha_j$\n",
    "\n",
    "$$\\alpha_J^{new}=\\alpha_j^{old}+\\frac{y_j(E_i-E_j)}{\\eta}$$\n",
    "\n",
    "* 步骤5：根据取值范围修剪$\\alpha_j$\n",
    "\n",
    "$$\\alpha_2^{new,clipped}=\\begin{cases}\n",
    "H  &  \\alpha_2^{new}>H \\\\ \n",
    "\\alpha_2^{new} &  L\\leq \\alpha_2^{new}\\leq H \\\\ \n",
    "L &  \\alpha_2^{new}<L\n",
    "\\end{cases}$$\n",
    "\n",
    "* 步骤6：更新$\\alpha_i$\n",
    "$$\\alpha_i^{new}=\\alpha_i^{old}+y_iy_j(\\alpha_j^{old}-\\alpha_j^{new,clipped})$$\n",
    "\n",
    "* 步骤7：更新b1和b2\n",
    "$$b_1^{new}=b^{old}-E_i-y_i(\\alpha_{new}-\\alpha_i^{old})x_i^Tx_i-y_i(\\alpha_j^{new}-\\alpha_j^{old})x_j^Tx_i$$\n",
    "\n",
    "$$b_2^{new}=b^{old}-E_j-y_i(\\alpha_i^{new}-\\alpha_i^{old})x_i^Tx_j-y_j(\\alpha_j^{new}-\\alpha_j^{old})x_j^Tx_j$$\n",
    "\n",
    "* 步骤8：根据b1和b2更新b\n",
    "\n",
    "$$b=\\begin{cases}\n",
    "    b_1  & 0<\\alpha_1^{new}<C \\\\ \n",
    "b_2 & 0<\\alpha_2^{new}<C \\\\ \n",
    "(b_1+b_2)/2 & otherwise \n",
    "\\end{cases}$$\n",
    "\n",
    "# 6 - python实现线性SVM算法\n",
    "\n",
    "已经梳理完了SMO算法实现步骤，接下来按照这个思路编写代码，进行实战练习。\n",
    "\n",
    "## 6.1 - 可视化数据集\n",
    "\n",
    "我们先使用简单的数据集进行测试。\n",
    "编写程序可视化数据集，看下它是长什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "函数说明:读取数据\n",
    "\n",
    "Parameters:\n",
    "    fileName - 文件名\n",
    "Returns:\n",
    "    dataMat - 数据矩阵\n",
    "    labelMat - 数据标签\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-21\n",
    "\"\"\"\n",
    "def loadDataSet(fileName):\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    "\n",
    "\"\"\"\n",
    "函数说明:数据可视化\n",
    "\n",
    "Parameters:\n",
    "    dataMat - 数据矩阵\n",
    "    labelMat - 数据标签\n",
    "Returns:\n",
    "    无\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-21\n",
    "\"\"\"\n",
    "def showDataSet(dataMat, labelMat):\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) #负样本散点图\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataMat, labelMat = loadDataSet('testSet.txt')\n",
    "    showDataSet(dataMat, labelMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是我们使用的二维数据集，显然线性可分。现在我们使用简化版的SMO算法进行求解。\n",
    "\n",
    "##  6.2 - 简化版SMO算法\n",
    "按照上述已经推导的步骤编写代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次迭代 样本:0, alpha优化次数:1\n",
      "第0次迭代 样本:3, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:10, alpha优化次数:3\n",
      "第0次迭代 样本:18, alpha优化次数:4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:5, alpha优化次数:1\n",
      "L==H\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "第0次迭代 样本:17, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "L==H\n",
      "第0次迭代 样本:54, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "第0次迭代 样本:69, alpha优化次数:4\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:18, alpha优化次数:1\n",
      "第0次迭代 样本:22, alpha优化次数:2\n",
      "第0次迭代 样本:23, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:35, alpha优化次数:4\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "L==H\n",
      "第0次迭代 样本:54, alpha优化次数:5\n",
      "第0次迭代 样本:55, alpha优化次数:6\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:10, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:5, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:17, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:54, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:29, alpha优化次数:1\n",
      "第0次迭代 样本:35, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "第1次迭代 样本:0, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:10, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:23, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第2次迭代 样本:94, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:0, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:8, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:24, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:54, alpha优化次数:4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:23, alpha优化次数:1\n",
      "第0次迭代 样本:24, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:54, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第2次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:30, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第4次迭代 样本:23, alpha优化次数:1\n",
      "第4次迭代 样本:29, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:0, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:97, alpha优化次数:2\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:10, alpha优化次数:1\n",
      "第0次迭代 样本:17, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:0, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:8, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:54, alpha优化次数:2\n",
      "第0次迭代 样本:55, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:24, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:24, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "第0次迭代 样本:97, alpha优化次数:2\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:24, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:30, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:0, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:24, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:97, alpha优化次数:2\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:24, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第3次迭代 样本:97, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:10, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第2次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:10, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:54, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:97, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:23, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:10, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:23, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第6次迭代 样本:24, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "第1次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第2次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第8次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第8次迭代 样本:55, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第7次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "第1次迭代 样本:17, alpha优化次数:1\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "第7次迭代 样本:23, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第4次迭代 样本:29, alpha优化次数:1\n",
      "第4次迭代 样本:54, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:23, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "第4次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "第7次迭代 样本:23, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第7次迭代 样本:55, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "第5次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第7次迭代 样本:55, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第18次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "第4次迭代 样本:17, alpha优化次数:1\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 19\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 20\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 21\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 22\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "第22次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "第22次迭代 样本:55, alpha优化次数:2\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "第10次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "第4次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 19\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 20\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 21\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 22\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 23\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 24\n",
      "alpha_j变化太小\n",
      "第24次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "第9次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "第14次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "第7次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 19\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 20\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 21\n",
      "alpha_j变化太小\n",
      "第21次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "第5次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "第3次迭代 样本:52, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第3次迭代 样本:55, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 19\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第19次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第2次迭代 样本:55, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "第9次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "第1次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "第2次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "第1次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "第0次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "第1次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "L==H\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第6次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "第0次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第5次迭代 样本:55, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "第13次迭代 样本:17, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 19\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 20\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 21\n",
      "第21次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "第16次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第6次迭代 样本:55, alpha优化次数:1\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "第6次迭代 样本:54, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "迭代次数: 0\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 7\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 8\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 9\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 10\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 11\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 12\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 13\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 14\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 15\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 16\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 17\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 18\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 19\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 20\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 21\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 22\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 23\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 24\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 25\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 26\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 27\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 28\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 29\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 30\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 31\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 32\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 33\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 34\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 35\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 36\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 37\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 38\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 39\n",
      "alpha_j变化太小\n",
      "alpha_j变化太小\n",
      "迭代次数: 40\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import types\n",
    "\n",
    "\"\"\"\n",
    "函数说明:读取数据\n",
    "\n",
    "Parameters:\n",
    "    fileName - 文件名\n",
    "Returns:\n",
    "    dataMat - 数据矩阵\n",
    "    labelMat - 数据标签\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-21\n",
    "\"\"\"\n",
    "def loadDataSet(fileName):\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:随机选择alpha\n",
    "\n",
    "Parameters:\n",
    "    i - alpha\n",
    "    m - alpha参数个数\n",
    "Returns:\n",
    "    j -\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-21\n",
    "\"\"\"\n",
    "def selectJrand(i, m):\n",
    "    j = i                                 #选择一个不等于i的j\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    "\n",
    "\"\"\"\n",
    "函数说明:修剪alpha\n",
    "\n",
    "Parameters:\n",
    "    aj - alpha值\n",
    "    H - alpha上限\n",
    "    L - alpha下限\n",
    "Returns:\n",
    "    aj - alpah值\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-21\n",
    "\"\"\"\n",
    "def clipAlpha(aj,H,L):\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    "\n",
    "\"\"\"\n",
    "函数说明:简化版SMO算法\n",
    "\n",
    "Parameters:\n",
    "    dataMatIn - 数据矩阵\n",
    "    classLabels - 数据标签\n",
    "    C - 松弛变量\n",
    "    toler - 容错率\n",
    "    maxIter - 最大迭代次数\n",
    "Returns:\n",
    "    无\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-23\n",
    "\"\"\"\n",
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    #转换为numpy的mat存储\n",
    "    dataMatrix = np.mat(dataMatIn); labelMat = np.mat(classLabels).transpose()\n",
    "    #初始化b参数，统计dataMatrix的维度\n",
    "    b = 0; m,n = np.shape(dataMatrix)\n",
    "    #初始化alpha参数，设为0\n",
    "    alphas = np.mat(np.zeros((m,1)))\n",
    "    #初始化迭代次数\n",
    "    iter_num = 0\n",
    "    #最多迭代matIter次\n",
    "    while (iter_num < maxIter):\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m):\n",
    "            #步骤1：计算误差Ei\n",
    "            fXi = float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\n",
    "            Ei = fXi - float(labelMat[i])\n",
    "            #优化alpha，更设定一定的容错率。\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "                #随机选择另一个与alpha_i成对优化的alpha_j\n",
    "                j = selectJrand(i,m)\n",
    "                #步骤1：计算误差Ej\n",
    "                fXj = float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                #保存更新前的aplpha值，使用深拷贝\n",
    "                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\n",
    "                #步骤2：计算上下界L和H\n",
    "                if (labelMat[i] != labelMat[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                if L==H: print(\"L==H\"); continue\n",
    "                #步骤3：计算eta\n",
    "                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                if eta >= 0: print(\"eta>=0\"); continue\n",
    "                #步骤4：更新alpha_j\n",
    "                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n",
    "                #步骤5：修剪alpha_j\n",
    "                alphas[j] = clipAlpha(alphas[j],H,L)\n",
    "                if (abs(alphas[j] - alphaJold) < 0.00001): print(\"alpha_j变化太小\"); continue\n",
    "                #步骤6：更新alpha_i\n",
    "                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\n",
    "                #步骤7：更新b_1和b_2\n",
    "                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n",
    "                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                #步骤8：根据b_1和b_2更新b\n",
    "                if (0 < alphas[i]) and (C > alphas[i]): b = b1\n",
    "                elif (0 < alphas[j]) and (C > alphas[j]): b = b2\n",
    "                else: b = (b1 + b2)/2.0\n",
    "                #统计优化次数\n",
    "                alphaPairsChanged += 1\n",
    "                #打印统计信息\n",
    "                print(\"第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter_num,i,alphaPairsChanged))\n",
    "        #更新迭代次数\n",
    "        if (alphaPairsChanged == 0): iter_num += 1\n",
    "        else: iter_num = 0\n",
    "        print(\"迭代次数: %d\" % iter_num)\n",
    "    return b,alphas\n",
    "\n",
    "\"\"\"\n",
    "函数说明:分类结果可视化\n",
    "\n",
    "Parameters:\n",
    "    dataMat - 数据矩阵\n",
    "    w - 直线法向量\n",
    "    b - 直线解决\n",
    "Returns:\n",
    "    无\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-23\n",
    "\"\"\"\n",
    "def showClassifer(dataMat, w, b):\n",
    "    #绘制样本点\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1], s=30, alpha=0.7)   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1], s=30, alpha=0.7) #负样本散点图\n",
    "    #绘制直线\n",
    "    x1 = max(dataMat)[0]\n",
    "    x2 = min(dataMat)[0]\n",
    "    a1, a2 = w\n",
    "    b = float(b)\n",
    "    a1 = float(a1[0])\n",
    "    a2 = float(a2[0])\n",
    "    y1, y2 = (-b- a1*x1)/a2, (-b - a1*x2)/a2\n",
    "    plt.plot([x1, x2], [y1, y2])\n",
    "    #找出支持向量点\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha > 0:\n",
    "            x, y = dataMat[i]\n",
    "            plt.scatter([x], [y], s=150, c='none', alpha=0.7, linewidth=1.5, edgecolor='red')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:计算w\n",
    "\n",
    "Parameters:\n",
    "    dataMat - 数据矩阵\n",
    "    labelMat - 数据标签\n",
    "    alphas - alphas值\n",
    "Returns:\n",
    "    无\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-09-23\n",
    "\"\"\"\n",
    "def get_w(dataMat, labelMat, alphas):\n",
    "    alphas, dataMat, labelMat = np.array(alphas), np.array(dataMat), np.array(labelMat)\n",
    "    w = np.dot((np.tile(labelMat.reshape(1, -1).T, (1, 2)) * dataMat).T, alphas)\n",
    "    return w.tolist()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataMat, labelMat = loadDataSet('testSet.txt')\n",
    "    b,alphas = smoSimple(dataMat, labelMat, 0.6, 0.001, 40)\n",
    "    w = get_w(dataMat, labelMat, alphas)\n",
    "    showClassifer(dataMat, w, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7 - 非线性SVM\n",
    "\n",
    "## 7.1 - 核技巧\n",
    "\n",
    "我们已经了解到，SVM如何处理线性可分的情况，而对于非线性的情况，SVM的处理方式就是选择一个核函数。简而言之：在线性不可分的情况下，SVM通过某种事先选择的非线性映射（核函数）将输入变量映到一个高维特征空间，将其变成在高维空间线性可分，在这个高维空间中构造最优分类超平面。\n",
    "\n",
    "在线性可分的情况下，可知最终的超平面方程为：\n",
    "\n",
    "$$f(x)=\\sum_{i=1}^n\\alpha_iy_ix_i^Tx+b$$\n",
    "将上述公式用内积来表示：\n",
    "$$f(x)=\\sum_{i=1}^n\\alpha_iy_i\\left \\langle x_i,x \\right \\rangle+b$$\n",
    "\n",
    "对于线性不可分，我们使用一个非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器，分类函数变形如下：\n",
    "$$f(x)=\\sum_{i=1}^n\\alpha_iy_i\\left \\langle \\Phi (x_i),\\Phi (x) \\right \\rangle+b$$\n",
    "\n",
    "其中ϕ从输入空间(X)到某个特征空间(F)的映射，这意味着建立非线性学习器分为两步：\n",
    "\n",
    "* 首先使用一个非线性映射将数据变换到一个特征空间F；\n",
    "\n",
    "* 然后在特征空间使用线性学习器分类。\n",
    "\n",
    "如果有一种方法可以在特征空间中直接计算内积 <ϕ(xi),ϕ(x)>，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个分线性的学习器，这样直接计算的方法称为核函数方法。\n",
    "\n",
    "这里直接给出一个定义：核是一个函数k，对所有x,z∈X，满足k(x,z)=<ϕ(xi),ϕ(x)>，这里ϕ(·)是从原始输入空间X到内积空间F的映射。\n",
    "\n",
    "简而言之：如果不是用核技术，就会先计算线性映ϕ(x1)和ϕ(x2)，然后计算这它们的内积，使用了核技术之后，先把ϕ(x1)和ϕ(x2)的一般表达式<ϕ(x1),ϕ(x2)>=k(<ϕ(x1),ϕ(x2) >)计算出来，这里的<·，·>表示内积，k(·，·)就是对应的核函数，这个表达式往往非常简单，所以计算非常方便。\n",
    "\n",
    "这种将内积替换成核函数的方式被称为核技巧(kernel trick)。\n",
    "\n",
    "## 7.2 - 非线性数据处理\n",
    "\n",
    "已经知道了核技巧是什么，但是为什么要这样做呢？我们先举一个简单的例子，进行说明。假设二维平面x-y上存在若干点，其中点集A服从 {x,y|x^2+y^2=1}，点集B服从{x,y|x^2+y^2=9}，那么这些点在二维平面上的分布是这样的：\n",
    "\n",
    "![ml_9_6.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E6%A0%B81.png?raw=true)\n",
    "\n",
    "蓝色的是点集A，红色的是点集B，他们在xy平面上并不能线性可分，即用一条直线分割（ 虽然肉眼是可以识别的） 。采用映射(x,y)->(x,y,x^2+y^2)后，在三维空间的点的分布为：\n",
    "\n",
    "![ml_9_7.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E6%A0%B82.png?raw=true)\n",
    "\n",
    "可见红色和蓝色的点被映射到了不同的平面，在更高维空间中是线性可分的（用一个平面去分割）。\n",
    "\n",
    "上述例子中的样本点的分布遵循圆的分布。继续推广到椭圆的一般样本形式：\n",
    "\n",
    "![ml_9_8.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/11-%E6%A0%B83.png?raw=true)\n",
    "\n",
    "上图的两类数据分布为两个椭圆的形状，这样的数据本身就是不可分的。不难发现，这两个半径不同的椭圆是加上了少量的噪音生成得到的。所以，一个理想的分界应该也是一个椭圆，而不是一个直线。如果用X1和X2来表示这个二维平面的两个坐标的话，我们知道这个分界椭圆可以写为：\n",
    "\n",
    "$$\\alpha_1X_1+\\alpha_2X_2^2+\\alpha_3X_2+\\alpha_4X_2^2+\\alpha_5X_1X_2+\\alpha_6=0$$\n",
    "\n",
    "这个方程就是高中学过的椭圆一般方程。注意上面的形式，如果我们构造另外一个五维的空间，其中五个坐标的值分别为：\n",
    "\n",
    "$$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1X_2$$\n",
    "\n",
    "那么，显然我们可以将这个分界的椭圆方程写成如下形式：\n",
    "$$\\sum_{i=1}^5\\alpha_iZ_i+\\alpha_6=0$$\n",
    "\n",
    "这个关于新的坐标Z1,Z2,Z3,Z4,Z5的方程，就是一个超平面方程，它的维度是5。也就是说，如果我们做一个映射 ϕ : 二维 → 五维，将 X1,X2按照上面的规则映射为 Z1,Z2,··· ,Z5，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。\n",
    "\n",
    "我们举个简单的计算例子，现在假设已知的映射函数为：\n",
    "\n",
    "$$\\Phi ((x_1,x_2))=(\\sqrt 2 x_1,x_1^2,\\sqrt 2x_2,x_2^2,\\sqrt 2x_1x_2,1)$$\n",
    "\n",
    "这个是一个从2维映射到5维的例子。如果没有使用核函数，根据上一小节的介绍，我们需要先结算映射后的结果，然后再进行内积运算。那么对于两个向量a1=(x1,x2)和a2=(y1,y2)有：\n",
    "\n",
    "$$\\left \\langle \\Phi((x_1,x_2),\\Phi(y_1,y_2)) \\right \\rangle=2x_1y_1+x_1^2y_1^2+2x_2y_2+x_2^2y_2^2+2x_1x_2y_1y_2+1$$\n",
    "\n",
    "另外，如果我们不进行映射计算，直接运算下面的公式：\n",
    "$$(\\left \\langle x_1,x_2\\right\\rangle+1)^2=2x_1y_1+x_1^2y_1^2+2x_2y_2+x_2^2y_2^2+2x_1x_2y_1y_2+1$$\n",
    "\n",
    "你会发现，这两个公式的计算结果是相同的。区别在于什么呢？\n",
    "\n",
    "* 一个是根据映射函数，映射到高维空间中，然后再根据内积的公式进行计算，计算量大；\n",
    "* 另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果，计算量小。\n",
    "\n",
    "其实，在这个例子中，核函数是：\n",
    "$$k(x_1,x_2)=(\\left\\langle x_1,x_2\\right\\langle+1)^2$$\n",
    "\n",
    "我们通过k(x1,x2)的低维运算得到了先映射再内积的高维运算的结果，这就是核函数的神奇之处，它有效减少了我们的计算量。在这个例子中，我们对一个2维空间做映射，选择的新的空间是原始空间的所以一阶和二阶的组合，得到了5维的新空间；如果原始空间是3维的，那么我们会得到19维的新空间，这个数目是呈爆炸性增长的。如果我们使用ϕ(·)做映射计算，难度非常大，而且如果遇到无穷维的情况，就根本无从计算了。所以使用核函数进行计算是非常有必要的。\n",
    "\n",
    "## 7.3 - 核技巧的实现\n",
    "\n",
    "通过核技巧的转变，我们的分类函数变为：\n",
    "\n",
    "$$f(x)=\\sum_{i=1}^n\\alpha_iy_ik(x_i,x)+b$$\n",
    "\n",
    "我们的对偶问题变成了：\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\alpha_i\\alpha_jy_iy_jk(x_i,x_j)$$\n",
    "\n",
    "$$s.t.　\\alpha_i\\geq 0 ,i=1,2,\\dots,n$$\n",
    "\n",
    "$$\\sum_{i=1}^n\\alpha_iy_i=0$$\n",
    "\n",
    "这样，我们就避开了高纬度空间中的计算。当然，我们刚刚的例子是非常简单的，我们可以手动构造出来对应映射的核函数出来，如果对于任意一个映射，要构造出对应的核函数就很困难了。因此，通常，人们会从一些常用的核函数中进行选择，根据问题和数据的不同，选择不同的参数，得到不同的核函数。接下来，要介绍的就是一个非常流行的核函数，那就是径向基核函数。\n",
    "\n",
    "径向基核函数是SVM中常用的一个核函数。径向基核函数采用向量作为自变量的函数，能够基于向量举例运算输出一个标量。径向基核函数的高斯版本的公式如下：\n",
    "\n",
    "$$k(x_1,x-2)=exp\\left \\{ -\\frac{||x_1-x_2||^2}{2\\sigma^2} \\right \\}$$\n",
    "\n",
    "其中，σ是用户自定义的用于确定到达率(reach)或者说函数值跌落到0的速度参数。上述高斯核函数将数据从原始空间映射到无穷维空间。关于无穷维空间，我们不必太担心。\n",
    "\n",
    "高斯核函数只是一个常用的核函数，使用者并不需要确切地理解数据到底是如何表现的，而且使用高斯核函数还会得到一个理想的结果。如果σ选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；\n",
    "\n",
    "反过来，如果σ选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数σ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。\n",
    "\n",
    "# 8 - 用Python实现非线性SVM\n",
    "\n",
    "## 8.1 - 可视化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3X2wXHWd5/H3lxAgW9bkJpIZYx4k\nzGRQUEy0F52lalSedcok62AMlmucxUrp6liLa8pQTEFkxjJK1WSWWnc1gwjqjICM4mXVymACO1Wz\nZobLAgnBiolhkDyMZIQwtUsMIXz3jz4XTvc9p7tvn4f+nXM+r6pbt/s8dP/uuff29/yevj9zd0RE\nRCadMuoCiIhIWBQYRESkgwKDiIh0UGAQEZEOCgwiItJBgUFERDooMIiISAcFBhER6aDAICIiHU4d\ndQGGceaZZ/pZZ5016mKIiFTKQw899C/uPq/fcZUMDGeddRYTExOjLoaISKWY2ZODHKemJBER6aDA\nICIiHRQYRESkgwKDiIh0UGAQEZEOuQQGM7vVzJ42s8dS9puZ3Wxm+8xsp5m9JbZvrZntjb7W5lEe\nEREZXl41htuAK3rsfzewNPpaB/wPADObC9wAvA24ALjBzObkVCYRERlCLoHB3f8OeKbHISuBb3jb\nDmDMzOYDlwP3ufsz7v4scB+9A4xI+XbeBZvfCBvH2t933jXqEokUqqwJbguAp2LPD0Tb0raLhGHn\nXXDvp+DEsfbz555qPwc4f/XoyiVSoLI6ny1hm/fYPvUFzNaZ2YSZTRw5ciTXwomk2nbjK0Fh0olj\n7e0iNVVWYDgALIo9Xwgc6rF9Cnff4u4td2/Nm9c31YdIPp47ML3tIjVQVmAYBz4cjU56O/Ccux8G\ntgKXmdmcqNP5smibSBhmL5zedpEayKWPwcy+DbwTONPMDtAeaTQTwN2/AvwQeA+wD3ge+KNo3zNm\n9qfAg9FL3ejuvTqxRcp18fWdfQwAM2e1t4vUVC6Bwd2v6rPfgU+k7LsVuDWPcojkbrKDeduN7eaj\n2QvbQUEdz1JjlUy7LVKq81f3DwQ771LwkNpQYBDJSkNapWaUK0kkKw1plZpRYBDJSkNapWYUGESy\n0pBWqRkFBpGsLr6+PYQ1TkNapcIUGESyOn81vPdmmL0IsPb3996sjmepLI1KEsnDIENaRSpCNQYR\nEemgwCASAq35IAFRU5IUR7OBB6MJchIY1RikGJMfds89BfgrH3aD3gk36Q5aE+QkMAoMUowsH3ZZ\ng0rVaIKcBEaBQYqR5cOuaXfQmiAngVFgkGJk+bBr2h20JshJYBQYpBhZPuyadgetCXISGI1KkmJk\nWeCmiaumaYKcBESBQYoz7IedVk0TGSkFBgmT7qCHo7kjkoNc+hjM7Aoz22Nm+8xsQ8L+zWb2SPT1\nMzM7Gtt3MrZvPI/yiDRS04b5SmEy1xjMbAbwZeBS4ADwoJmNu/vjk8e4+zWx4/8YWB57iWPuvixr\nOUQar9cwX9UaZBryqDFcAOxz9/3u/gJwB7Cyx/FXAd/O4X1FJK5pw3ylMHkEhgXAU7HnB6JtU5jZ\n64AlwPbY5jPMbMLMdpjZqhzKI9JMTRvmK4XJIzBYwjZPOXYNcLe7n4xtW+zuLeCDwF+Y2W8nvonZ\nuiiATBw5ciRbiUXqaNC5I03KQyVDySMwHAAWxZ4vBA6lHLuGrmYkdz8Ufd8PPEBn/0P8uC3u3nL3\n1rx587KWWaR+Bpkopw5qGUAew1UfBJaa2RLgIO0P/w92H2Rm5wBzgJ/Ets0Bnnf342Z2JnAh8KUc\nyiTSTP2G+aqDWgaQOTC4+4tm9klgKzADuNXdd5vZjcCEu08OQb0KuMPd481MbwC+amYv0a69bIqP\nZhKRnKmDWgaQywQ3d/8h8MOubdd3Pd+YcN7/Bt6URxlEZACzF0bNSAnbRSJKoifSJMrkKgNQYBBp\nEmVylQEoV5JUi3IBZac8VNKHAoNUx+RQy8lRNZNDLUEfdGXpDsxLL4O9f6tAXTNqSpLqaNqSn6FJ\nmgMx8bVqzonQJL+eFBikPFn/GTXUcrSSAnO3KgRqTfLrS4Gh6cq6c8rjn1G5gEZr0AAceqBWzbMv\nBYYmK/POKY9/Rg21HK1BA3DogVo1z74UGJqszDunPP4ZNdRytJICc7cqBGrVPPvSqKQmK/POKa8Z\ntxpqOTpJa3FXcVTSxdd3jm6DagS0EikwNFmZ6RH0z1gPdQjMSQGuCgGtRAoMTVbmh7X+GfMTyiS/\nUMoxjDoEuAIpMDRZ2R/W+mfMLpRJfqGUQwphnVmwq6HVavnExMSoiyFSvs1vTGn+WwTXPNa8csi0\nmNlD0YqZPWlUkkiVhDLUMpRySCEUGJpA0//rI5ShlqGUQwqhwFB3o5j+r0BUnFAm+YVSDimEAkPd\nlT39v+55aEYd9HpN8iuzbJpsWGvqfK67jWNA0u/YYOPR/N+vzp2S3SNxoH2XHMIHYshlk2CU2vls\nZleY2R4z22dmGxL2f8TMjpjZI9HXR2P71prZ3uhrbR7lkZiy24L7dUqO+o47i5CTr4VcNqmczIHB\nzGYAXwbeDZwLXGVm5yYceqe7L4u+bonOnQvcALwNuAC4wczmZC2TxJTdFtwrEFW9mSnkkTghl00q\nJ48awwXAPnff7+4vAHcAKwc893LgPnd/xt2fBe4DrsihTDKp7LbgXoGo6ne1Zde+plO70ighyVEe\ngWEBEG9UPhBt6/aHZrbTzO42s0XTPFeyOH91u31/49H29yLbnHsFoqrf1ZZZ+5pu7UqjhCRHeaTE\nsIRt3b2d9wLfdvfjZvYx4HbgogHPbb+J2TpgHcDixYuHL61k1y9HTlrqizKT9hWhzBQivWpXSe+n\nXFSSozwCwwFgUez5QuBQ/AB3/1Xs6V8CX4yd+86ucx9IehN33wJsgfaopCwFlgyy5MipQ4bVsvI9\nDVO7Ui4qyUkeTUkPAkvNbImZnQasAcbjB5jZ/NjTFcBPo8dbgcvMbE7U6XxZtE1ClaWfQGPfB6c+\ng3qo6Ci8zDUGd3/RzD5J+wN9BnCru+82sxuBCXcfBz5lZiuAF4FngI9E5z5jZn9KO7gA3Ojuz2Qt\nUyOVlQI5az+B7moHU4faVdNVOAOtJrhV2cvB4Cna3TWx32VRk5vqPIEtNFVe7yAko7qOAf6vDDrB\nTYGhqpJmunYr4g9QM2ylSkb595qadQDaTanlB3ul3a67pLb+bkUMA1U/gVTJKOfO9OwPCnuCp1Zw\nq6pBPvSL6qhUP4FUxSjnziT1E3XrNQR5hFRjqKp+H/rqqJQ6mu4on1GO7uquXacJcIKnAkNVJc10\nnfzjU/NOUO55+CAXbtrOkg0/4MJN27nn4YOjLlI1DZNra9QzwuNZB2YvSj4mwCHICgxVldTW/74t\nsPG54tNeyMDuefgg1353FwePHsOBg0ePce13d4UbHEIedz9Mf0FIfWKjDlLToD6GKktq69cQx6Dc\ntHUPx06c7Nh27MRJbtq6h1XLA0sLFvq4+2H7C0LpE6tQ2hIFhjoJ/R+7Bu55+CA3bd3DoaPHeO3Y\nLNZffk7PD/hDR5M7HtO2j9R08zOVreq5tiCcINWHmpLqpOpprQM3TLPQa8e6+4F6bx+p0LPfVqgp\npuoUGOok9H/siuvVLJRm/eXnMGvmjI5ts2bOYP3l5xRSxkxCz88UUn9BzakpqU7qUNUO2DDNQpPN\nTNNpfhqZKuRnqkhTTNUpMNRJFf6xK+y1Y7M4mBAE+jULrVq+IMxA0K1CnaOlauCADgWGOtE/dqHW\nX34O1353V0dzUrDNQsPSHXmnhg7oUGCoG/1jF6ZSzUKSj9BHahVEgUFkGirTLCT5aOiADo1KEhFJ\nE/pIrYIoMEjlKPeQlKahcyfUlCSVMjnJbLIDeHKSGaAmHslfQwd0KDBIpVQq95DUQwMHdOTSlGRm\nV5jZHjPbZ2YbEvZ/2sweN7OdZrbNzF4X23fSzB6JvsbzKI/UV6VyD4lUVOYag5nNAL4MXAocAB40\ns3F3fzx22MNAy92fN7OPA18CPhDtO+buy7KWQ5ph2ElmTTDdBH8iafKoMVwA7HP3/e7+AnAHsDJ+\ngLvf7+7PR093APXu0pfCVCr3UIkqt+6DBC2PwLAAiCfoORBtS3M18KPY8zPMbMLMdpjZqhzKIzW2\navkCvvC+N7FgbBYGLBibxRfe96bG3xkPk+BPJE0enc9Ji5l64oFmHwJawDtimxe7+yEzOxvYbma7\n3P3nCeeuA9YBLF68OHuppbI0yWwq9b1InvKoMRwA4ouZLgQOdR9kZpcA1wEr3P345HZ3PxR93w88\nACxPehN33+LuLXdvzZs3L4dii9RHpdZ9yEPIS5DWQB6B4UFgqZktMbPTgDVAx+giM1sOfJV2UHg6\ntn2OmZ0ePT4TuBCId1qLyAAa1fcymdjuuacAfyWxnYJDbjI3Jbn7i2b2SWArMAO41d13m9mNwIS7\njwM3Aa8CvmNmAL9w9xXAG4CvmtlLtIPUpq7RTPXUwDS+UuyooUYl+GtoYrsymXtid0DQWq2WT0xM\njLoYw+lO4wvtKfZaiarWumdsQ/uOXh3nQ9g4RnI3psHGo2WXplLM7CF3b/U7TrmSylaRdZmVjyjf\na6BRQzlqaGK7MikwlK0CaXw1Jj7/a6BRQzlqaGK7MikwlK0Cdzu6u02/Bv/lrkeHqkE0btRQkc5f\n3W56nb0IsPZ3NcXmSkn0ylaBdZnT7mIPHj3GhZu2179zk/RrcDLqk4tndYX+nb6NWBa0TA1MbFcm\nBYayVSCNb1o+IoOXtxed7nrUeX/SrkHcsRMn+dy9u/n1iZf6pgFv1KghqTyNSpIpkkbQGMnjQBaM\nzeLvN1xU+PuXPYInqQzTUcR16WXUgVSqQaOSZGhJ+YjSbh+K6DwNoY+j+xrMsKTML+nK7FTWYAHJ\nm5qS+mnoZLTufEQXbtpeWrrrUEbwxK9BWi3m9FNP4eixE1POLbNTWYsXSd5UY+hFU+9fVmbKhRBH\n8KRldd244ryRp6IIJZBKfajG0Eu/yWhJNYma1jDK7DwNdQRPr6yuIXaUayisDEudz72kTr2nPcS0\ne8jpmz8Ij/610l3kQJ2pgwuhs16qYdDOZwWGXja/MWpG6mIzwBNGq6Rtn70Irnks//KJRBRIZRCD\nBgY1JfWSNhmtu3lpUlJQgKDSXUinunygavEiyZM6n3tJm3o/e1Hy8TYjeXtA6S7kFRrmKZJMNYZ+\n0qbeJ9Uk0voYAkp3Ia/QME+RZAoMw+iV1mLx22s5KqmOyhzmWZcmK2kGBYZhpdUklNyrMoYZ5jnM\nB3z3qKGi80yJZKU+Bmms6U7au+fhg6y/+9GOPon1dz/at0+iyBQfWlBJiqAagzTWdCftfe7e3Zw4\n2Tm8+8RJ53P37p5yTrxmUVSeKdVEpCi5BAYzuwL4r8AM4BZ339S1/3TgG8BbgV8BH3D3f4r2XQtc\nDZwEPuXuW/Mok9RD0W3z0xnm+ezzU3MiJW0fNDNr1pnJ6jyXomRuSjKzGcCXgXcD5wJXmdm5XYdd\nDTzr7r8DbAa+GJ17LrAGOA+4Avjv0euJVHY4adIHdrc8UnwoR5IUJY8+hguAfe6+391fAO4AVnYd\nsxK4PXp8N3CxmVm0/Q53P+7uTwD7otcTCSL9drwNPy3x9tismR3Pe30wxxPwZb2rDzHZoNRDHk1J\nC4B43ogDwNvSjnH3F83sOeDV0fYdXeeqDixAsXfEgzRRDdIkNPMUY+OK8zq2pY12Gps1k0duuCxz\n2Se96/Xz+NaOX3SWZ4aNPNmgVF8eNYakG6nu/ra0YwY5t/0CZuvMbMLMJo4cOTLNIkoVFXVHPGgT\nVVqT0Ayzl+/8b3r/mxPXd555ytQ/7f/3wou5NYPd8/BB7nxwah6vkyerl/tMwpNHYDgAxHNELAQO\npR1jZqcCs4FnBjwXAHff4u4td2/Nmzcvh2JLLyEMgyxqDYhBm6jSaiYvufPEpj9g/eXncNPWPVOu\n0arlC3jVGVMr4ydOem7NYDdt3TNlhBTAS9E+kSzyCAwPAkvNbImZnUa7M3m865hxYG30+Epgu7fT\nuo4Da8zsdDNbAiwF/jGHMkkGoXT6pi2Ok7VtftAmql41ln7X6GjKCKa8OoZ7vY46nyWrzH0MUZ/B\nJ4GttIer3uruu83sRmDC3ceBrwHfNLN9tGsKa6Jzd5vZXcDjwIvAJ9zTUpTmoKaL6OQtpGGQRWQN\nHXTGc68Fg/pdo6IXz0l7/Tzfoxel+Ki3XGY+u/sP3f133f233f3z0bbro6CAu//a3d/v7r/j7he4\n+/7YuZ+PzjvH3X+UR3kSaZnOgdV9GOSgTVS9aiz9rlFRzWCTTXxpQWHmKfl1Pqc1J4ZSoyzczrva\na7JsHGt/b9BnRXNmPvdaplO1hg51XypyOjOe02oss2fN5Oixqc1Fs6Ohq0UshdpvlNTYrJlsXHFe\n4ntM9w6/16zqkGqUhZm8kZz8zJi8kYRGfF40JzCkLZajRXSmCHXN5TxlbaKylEkN8e15N4OljZJa\nMDaLv99wUep5w6TO6PXhH2SNMu9m4obfSDYniV7aYjlaRGeKojp9q2DQ0Vhpnctp2/Mw7AfyMBMF\ne71XcBPrimgmbviNZHNqDGnLdGoRnURNXCpyOnfWZaXsnu57Jr3HMAGl13sFV6Ms4u5+9sLk9d4b\nciPZnBpD2jKdDagWymCmc2c9TMrurB22/d4z7T1md6XsmNQriPV6r+BqlEXc3V98ffvGMW7QG8ka\ndFo3p8YAWkRHeprOnfV0O5fz6LDt955p73HGzFOYNXPGtO7w+71XUDXKIu7ue63S2EtNOq2bFRgk\nV3Ubyz7d5qHpfDjm1WHb6z3TXuvo8yfY/IFl0/5dBfXh30tRzcTD3EjWpNNagUGGksciMaEFliLb\nzssYAtzrPSrzIT+MYe/ui1CTTuvm9DFIrrKmxA5xklSRbedFTXgr+z2Cdf5quOYx2Hi0/X1Ud+c1\nGf2oGoMMJWvTSNGTpJJqI5Pv26uGUtSddRET3kbxHtJHEc1aI0jlo8AgQ8naNFL0WgvdzVzrv/Mo\nGC9nJB3F+sjTDTrDNLXVusmoCvJu1hpRZ7YCgwwla3t8kW3uSbWREy9NTVEdchqHPPpwiiqXaiR9\n5Dn6cUSd2epjkKFkbY8vsj18OrWOUBMD9uvDGcV6GSH2C9XeiDqzVWOQoWVptiiyPbxXSuqkY0PU\nq6ltVLWJRiTPC82IZmArMMjIFNUentTMNfMU6+hjgLBH7PRqakv7gN44vrvQD+ggk+fV3YhS+agp\nSWonqZnrpve/mZuufHM4aRz66NXUljqR7diJQpt1Sk2eV4O0ErkYUSofa6+wWS2tVssnJiZGXQyR\nQqV19PZaqKdfCu6s5UkacJB7gO0eiQPtu2TlNsvMzB5y91a/49SUJBKotKa29Zefw3++85HEc4ps\n1iltnkRN0kpUmQKDSMWsWr6Az927m2cT1n4oujO9lHkSNUkrUWWZ+hjMbK6Z3Wdme6PvcxKOWWZm\nPzGz3Wa208w+ENt3m5k9YWaPRF/LspRHpClueO959U1/UZO0ElWWtfN5A7DN3ZcC26Ln3Z4HPuzu\n5wFXAH9hZmOx/evdfVn0lVw/FpEOwa2JkKcsayFILrI2Ja0E3hk9vh14APhs/AB3/1ns8SEzexqY\nBxzN+N4ijVbb9BchZUttqEyjkszsqLuPxZ4/6+5TmpNi+y+gHUDOc/eXzOw24PeA40Q1Dnc/nnLu\nOmAdwOLFi9/65JNPDl1uEZEmym1Ukpn9GHhNwq7rplmg+cA3gbXu/lK0+Vrgn4HTgC20axs3Jp3v\n7luiY2i1WtUbYytSMuU1kmH1DQzufknaPjP7pZnNd/fD0Qf/0ynH/QbwA+BP3H1H7LUPRw+Pm9nX\ngc9Mq/RVMYK0udJsoSbhk2rI2vk8DqyNHq8Fvt99gJmdBnwP+Ia7f6dr3/zouwGrgMcylic8k5N1\nnnsK8FfS5jZ1JqeUIutCStJsWQPDJuBSM9sLXBo9x8xaZnZLdMxq4PeBjyQMS/0rM9sF7ALOBP4s\nY3nC02uyjkhBlNdIssg0KsndfwVcnLB9Avho9PhbwLdSzi9m7n5INFlHRqCMNaalvpREr2iarCMj\n0Oj1nyUzpcQo2ojS5spohDISSOs/SxYKDHnoNeqoCpN1NGoqF6GNBKrtBDgpnAJDVoMs1p3nGrB5\nG9Fi43WkFc6kLtTHkFXVRx1VvfwB0UggqQsFhqyqPuqo6uUPSKkrnNWZVm8bOQWGrKo+6qjq5Q+I\nRgLlQBNCg6DAkFXVUwRXvfwBqXUq7LKoaTMI6nzOqgqjjnqpevkDo5FAGalpMwgKDHkIedTRIKpe\nfqmP2QujZqSE7VIaNSWJSDjUtBkEBQYRCcf5q+G9N8PsRYC1v7/3ZtVoS6amJBEJQ/cM/PdtUUAY\nEQUGERk9zcAPipqSRGT0NEw1KAoMIjJ6GqYaFAUGERk9zcAPigKDiIyehqkGRYFBREZPw1SDkmlU\nkpnNBe4EzgL+CVjt7s8mHHcS2BU9/YW7r4i2LwHuAOYC/wf4D+7+QpYyiUhFaQZ+MLLWGDYA29x9\nKbAtep7kmLsvi75WxLZ/Edgcnf8scHXG8lSL0guLSICyBoaVwO3R49uBVYOeaGYGXATcPcz5laf0\nwiISqKyB4bfc/TBA9P03U447w8wmzGyHmU1++L8aOOruL0bPDwCpaSnNbF30GhNHjhzJWOwAaNy2\niASqbx+Dmf0YeE3Cruum8T6L3f2QmZ0NbDezXcC/JhznaS/g7luALQCtViv1uCB0T+1PSmOtcdsi\nEqi+gcHdL0nbZ2a/NLP57n7YzOYDT6e8xqHo+34zewBYDvwNMGZmp0a1hoXAoSF+hrAMOrVf6YVF\nJFBZm5LGgbXR47XA97sPMLM5ZnZ69PhM4ELgcXd34H7gyl7nV86gTUQaty0igcoaGDYBl5rZXuDS\n6Dlm1jKzW6Jj3gBMmNmjtAPBJnd/PNr3WeDTZraPdp/D1zKWZ/QGbSLSuG0RCZS1b9yrpdVq+cTE\nxKiLkWzzG1OaiBbBNY+VXx4RkYiZPeTurX7HaeZz3tREJCIVp8CQNzURiUjFaaGeImhqv4hUmGoM\nIiLSQYFBREQ6KDCIiEgHBQaRJlJmX+lBnc8iTTNo2hZpLNUYRJpGmX2lDwUGkabJmtlXzVC1p8Ag\n0jRpGXwHyeyrBaYaQYFBpGmypG3JqxlKtY6gqfNZpGkmO5j7LSaVJI8FptT5HTwFBpEmGjZtSx4L\nTPWqdSgwBEFNSSIyuDyyB2tZ2+ApMIjUVRHt+HlkD87S+Q3qnyiBmpJE6qjIdvys2YMvvr6zbDB4\nrUP9E6VQjUGkjkKexJal1pH2c33vY6o55Eg1BpE6Cr0df9haR1r5/aRqDjnKVGMws7lmdp+Z7Y2+\nz0k45l1m9kjs69dmtirad5uZPRHbtyxLeUQkkrUdP1S9yh9KjagGsjYlbQC2uftSYFv0vIO73+/u\ny9x9GXAR8Dzwt7FD1k/ud/dHMpZH6kCdi9nVde3xpJ8rLpQaUcVlDQwrgdujx7cDq/ocfyXwI3d/\nPuP7Sl0p5UI+6rr2+OTPZTOS91e9RhQIc/fhTzY76u5jsefPuvuU5qTY/u3An7v7/4ye3wb8HnCc\nqMbh7sdTzl0HrANYvHjxW5988smhyy0B2/zGlAlUi+Cax8ovj4Spe3QStGsSdQh+BTKzh9y91e+4\nvjUGM/uxmT2W8LVymgWaD7wJ2BrbfC3weuDfAnOBz6ad7+5b3L3l7q158+ZN562lSkLvNJUw1LVG\nFIi+o5Lc/ZK0fWb2SzOb7+6How/+p3u81Grge+5+Ivbah6OHx83s68BnBiy31FUeKRekGbLOp5BU\nWfsYxoG10eO1wPd7HHsV8O34hiiYYGZGu39CbQVNV9dOU5EKyRoYNgGXmtle4NLoOWbWMrNbJg8y\ns7OARcD/6jr/r8xsF7ALOBP4s4zlkapTE4HIyGXqfB6VVqvlExMToy6GiEil5Nb5LCIizaLAICIi\nHRQYRESkgwKDiIh0UGAQEZEOCgwiItKhksNVzewI8CTtuQ//MuLi9KLyZRd6GUMvH4RfRpUvu0HL\n+Dp375tTqJKBYZKZTQwyJndUVL7sQi9j6OWD8Muo8mWXdxnVlCQiIh0UGEREpEPVA8OWURegD5Uv\nu9DLGHr5IPwyqnzZ5VrGSvcxiIhI/qpeYxARkZwFHRjM7P1mttvMXjKz1B53M7vCzPaY2T4z2xDb\nvsTM/sHM9prZnWZ2WgFlnGtm90XvcZ+ZTVna1MzeZWaPxL5+bWaron23mdkTsX3Lyi5fdNzJWBnG\nY9tDuYbLzOwn0d/DTjP7QGxfIdcw7e8qtv/06Jrsi67RWbF910bb95jZ5XmUZ4jyfdrMHo+u1zYz\ne11sX+LvewRl/IiZHYmV5aOxfWujv4m9Zra2+9ySyrc5VrafmdnR2L7Cr6GZ3WpmT5tZ4lo11nZz\nVP6dZvaW2L7hr5+7B/sFvAE4B3gAaKUcMwP4OXA2cBrwKHButO8uYE30+CvAxwso45dor1UNsAH4\nYp/j5wLPAP8men4bcGWB13Cg8gH/N2V7ENcQ+F1gafT4tcBhYKyoa9jr7yp2zH8CvhI9XgPcGT0+\nNzr+dGBJ9DozRlC+d8X+zj4+Wb5ev+8RlPEjwH9LOHcusD/6Pid6PKfs8nUd/8fArSVfw98H3gI8\nlrL/PcCPAAPeDvxDHtcv6BqDu//U3ff0OewCYJ+773f3F4A7gJVmZsBFwN3RcbfTXiUubyuj1x70\nPa4EfuTuzxdQliTTLd/LQrqG7v4zd98bPT5EexnZIhf/Tvy76jomXu67gYuja7YSuMPdj7v7E8C+\n6PVKLZ+73x/7O9sBlL0+6iDXMM3lwH3u/oy7PwvcB1wx4vJNWYWyaO7+d7RvJNOsBL7hbTuAMWuv\njJnp+gUdGAa0AIgvEnwg2vZq4Ki7v9i1PW+/5dHa1dH33+xz/Bqm/nF9PqoGbjaz00dUvjPMbMLM\ndkw2cxHoNTSzC2jf4f08tjnva5j2d5V4THSNnqN9zQY5t4zyxV1N+85yUtLvO2+DlvEPo9/d3Wa2\naJrnllE+oma4JcD22OYyrmH2SYJdAAADCklEQVQ/aT9Dput3ai5Fy8DMfgy8JmHXde7eaw3pl18i\nYZv32D5tvco4zdeZD7wJ2BrbfC3wz7Q/6LYAnwVuHEH5Frv7ITM7G9hu7SVX/zXhuBCu4TeBte7+\nUrQ58zVMequEbd0/e+F/ez0M/B5m9iGgBbwjtnnK79vdf550fsFlvBf4trsfN7OP0a6BXTTguWWU\nb9Ia4G53PxnbVsY17KeQv8GRBwZ3vyTjSxygvZ70pIXAIdp5Q8bM7NTobm5ye65lNLNfmtl8dz8c\nfWg93eOlVgPfc/cTsdc+HD08bmZfBz4zivJFzTO4+34zewBYDvwNAV1DM/sN4AfAn0TV5snXznwN\nE6T9XSUdc8DMTgVm0672D3JuGeXDzC6hHXzf4e7HJ7en/L7z/lDrW0Z3/1Xs6V8CX4yd+86ucx8o\nu3wxa4BPxDeUdA37SfsZMl2/OjQlPQgstfbomdNo/wLHvd0Dcz/tNn2AtcAgNZDpGo9ee5D3mNJG\nGX0QTrbnrwISRx8UWT4zmzPZ/GJmZwIXAo+HdA2j3+33aLenfqdrXxHXMPHvqke5rwS2R9dsHFhj\n7VFLS4ClwD/mUKZplc/MlgNfBVa4+9Ox7Ym/75zLN2gZ58eergB+Gj3eClwWlXUOcBmdNe1SyheV\n8RzaHbg/iW0r6xr2Mw58OBqd9HbguehGKdv1K7pXPcsX8O9pR77jwC+BrdH21wI/jB33HuBntKP1\ndbHtZ9P+h9wHfAc4vYAyvhrYBuyNvs+NtreAW2LHnQUcBE7pOn87sIv2h9m3gFeVXT7g30VleDT6\nfnVo1xD4EHACeCT2tazIa5j0d0W7iWpF9PiM6Jrsi67R2bFzr4vO2wO8u6D/j37l+3H0fzN5vcb7\n/b5HUMYvALujstwPvD527n+Mru0+4I9GUb7o+UZgU9d5pVxD2jeSh6O//QO0+4o+Bnws2m/Al6Py\n7yI2ejPL9dPMZxER6VCHpiQREcmRAoOIiHRQYBARkQ4KDCIi0kGBQUREOigwiIhIBwUGERHpoMAg\nIiId/j9FhtGN27HKVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) #负样本散点图\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF.txt')                        #加载训练集\n",
    "    showDataSet(dataArr, labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，数据明显是线性不可分的。下面我们根据公式，编写核函数，并增加初始化参数kTup用于存储核函数有关的信息，同时我们只要将之前的内积运算变成核函数的运算即可。最后编写testRbf()函数，用于测试。创建svmMLiA.py文件，编写代码如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全样本遍历:第0次迭代 样本:0, alpha优化次数:1\n",
      "全样本遍历:第0次迭代 样本:1, alpha优化次数:1\n",
      "全样本遍历:第0次迭代 样本:2, alpha优化次数:2\n",
      "全样本遍历:第0次迭代 样本:3, alpha优化次数:3\n",
      "全样本遍历:第0次迭代 样本:4, alpha优化次数:3\n",
      "全样本遍历:第0次迭代 样本:5, alpha优化次数:4\n",
      "全样本遍历:第0次迭代 样本:6, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:7, alpha优化次数:5\n",
      "全样本遍历:第0次迭代 样本:8, alpha优化次数:6\n",
      "全样本遍历:第0次迭代 样本:9, alpha优化次数:6\n",
      "全样本遍历:第0次迭代 样本:10, alpha优化次数:7\n",
      "全样本遍历:第0次迭代 样本:11, alpha优化次数:8\n",
      "全样本遍历:第0次迭代 样本:12, alpha优化次数:8\n",
      "全样本遍历:第0次迭代 样本:13, alpha优化次数:9\n",
      "全样本遍历:第0次迭代 样本:14, alpha优化次数:10\n",
      "全样本遍历:第0次迭代 样本:15, alpha优化次数:11\n",
      "全样本遍历:第0次迭代 样本:16, alpha优化次数:12\n",
      "全样本遍历:第0次迭代 样本:17, alpha优化次数:12\n",
      "全样本遍历:第0次迭代 样本:18, alpha优化次数:13\n",
      "全样本遍历:第0次迭代 样本:19, alpha优化次数:14\n",
      "全样本遍历:第0次迭代 样本:20, alpha优化次数:14\n",
      "全样本遍历:第0次迭代 样本:21, alpha优化次数:15\n",
      "全样本遍历:第0次迭代 样本:22, alpha优化次数:15\n",
      "全样本遍历:第0次迭代 样本:23, alpha优化次数:15\n",
      "全样本遍历:第0次迭代 样本:24, alpha优化次数:16\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:25, alpha优化次数:16\n",
      "全样本遍历:第0次迭代 样本:26, alpha优化次数:16\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:27, alpha优化次数:16\n",
      "全样本遍历:第0次迭代 样本:28, alpha优化次数:16\n",
      "全样本遍历:第0次迭代 样本:29, alpha优化次数:16\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:30, alpha优化次数:16\n",
      "全样本遍历:第0次迭代 样本:31, alpha优化次数:17\n",
      "全样本遍历:第0次迭代 样本:32, alpha优化次数:17\n",
      "全样本遍历:第0次迭代 样本:33, alpha优化次数:18\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:34, alpha优化次数:18\n",
      "全样本遍历:第0次迭代 样本:35, alpha优化次数:18\n",
      "全样本遍历:第0次迭代 样本:36, alpha优化次数:19\n",
      "全样本遍历:第0次迭代 样本:37, alpha优化次数:19\n",
      "全样本遍历:第0次迭代 样本:38, alpha优化次数:19\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:39, alpha优化次数:19\n",
      "全样本遍历:第0次迭代 样本:40, alpha优化次数:20\n",
      "全样本遍历:第0次迭代 样本:41, alpha优化次数:21\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:42, alpha优化次数:21\n",
      "全样本遍历:第0次迭代 样本:43, alpha优化次数:21\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:44, alpha优化次数:21\n",
      "全样本遍历:第0次迭代 样本:45, alpha优化次数:22\n",
      "全样本遍历:第0次迭代 样本:46, alpha优化次数:22\n",
      "全样本遍历:第0次迭代 样本:47, alpha优化次数:22\n",
      "全样本遍历:第0次迭代 样本:48, alpha优化次数:23\n",
      "全样本遍历:第0次迭代 样本:49, alpha优化次数:23\n",
      "全样本遍历:第0次迭代 样本:50, alpha优化次数:24\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:51, alpha优化次数:24\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:52, alpha优化次数:24\n",
      "全样本遍历:第0次迭代 样本:53, alpha优化次数:24\n",
      "全样本遍历:第0次迭代 样本:54, alpha优化次数:25\n",
      "全样本遍历:第0次迭代 样本:55, alpha优化次数:25\n",
      "全样本遍历:第0次迭代 样本:56, alpha优化次数:26\n",
      "全样本遍历:第0次迭代 样本:57, alpha优化次数:26\n",
      "全样本遍历:第0次迭代 样本:58, alpha优化次数:26\n",
      "全样本遍历:第0次迭代 样本:59, alpha优化次数:26\n",
      "全样本遍历:第0次迭代 样本:60, alpha优化次数:26\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:61, alpha优化次数:26\n",
      "全样本遍历:第0次迭代 样本:62, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:63, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:64, alpha优化次数:27\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:65, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:66, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:67, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:68, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:69, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:70, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:71, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:72, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:73, alpha优化次数:27\n",
      "全样本遍历:第0次迭代 样本:74, alpha优化次数:28\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:75, alpha优化次数:28\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:76, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:77, alpha优化次数:28\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:78, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:79, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:80, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:81, alpha优化次数:28\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:82, alpha优化次数:28\n",
      "alpha_j变化太小\n",
      "全样本遍历:第0次迭代 样本:83, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:84, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:85, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:86, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:87, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:88, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:89, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:90, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:91, alpha优化次数:28\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:92, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:93, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:94, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:95, alpha优化次数:28\n",
      "L==H\n",
      "全样本遍历:第0次迭代 样本:96, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:97, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:98, alpha优化次数:28\n",
      "全样本遍历:第0次迭代 样本:99, alpha优化次数:28\n",
      "迭代次数: 1\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:0, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:2, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:3, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:6, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:8, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:11, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:13, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:14, alpha优化次数:0\n",
      "非边界遍历:第1次迭代 样本:15, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:16, alpha优化次数:1\n",
      "非边界遍历:第1次迭代 样本:18, alpha优化次数:2\n",
      "非边界遍历:第1次迭代 样本:19, alpha优化次数:3\n",
      "非边界遍历:第1次迭代 样本:21, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:27, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:30, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:31, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:33, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:36, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:40, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:41, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:42, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:45, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:48, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:50, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:54, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:56, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:62, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:74, alpha优化次数:3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第1次迭代 样本:83, alpha优化次数:3\n",
      "迭代次数: 2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:0, alpha优化次数:0\n",
      "非边界遍历:第2次迭代 样本:2, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:3, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:6, alpha优化次数:1\n",
      "非边界遍历:第2次迭代 样本:8, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:11, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:13, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:14, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:15, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:16, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:18, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:19, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:21, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:27, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:30, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:31, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:33, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:36, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:40, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:41, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:42, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:45, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:48, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:50, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:54, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:56, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:62, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:74, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "非边界遍历:第2次迭代 样本:83, alpha优化次数:2\n",
      "迭代次数: 3\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:0, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:3, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:6, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:11, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:13, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:14, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:15, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:16, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:18, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:19, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:21, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:27, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:30, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:31, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:33, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:36, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:40, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:41, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:42, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:45, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:48, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:50, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:54, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:56, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:62, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:74, alpha优化次数:0\n",
      "非边界遍历:第3次迭代 样本:76, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第3次迭代 样本:83, alpha优化次数:0\n",
      "迭代次数: 4\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:0, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:1, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:2, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:3, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:4, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:5, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:6, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:7, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:8, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:9, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:10, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:11, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:12, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:13, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:14, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:15, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:16, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:17, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:18, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:19, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:20, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:21, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:22, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:23, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:24, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:25, alpha优化次数:0\n",
      "全样本遍历:第4次迭代 样本:26, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:27, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:28, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:29, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:30, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:31, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:32, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:33, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:34, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:35, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:36, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:37, alpha优化次数:1\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:38, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:39, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:40, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:41, alpha优化次数:1\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:42, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:43, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:44, alpha优化次数:1\n",
      "全样本遍历:第4次迭代 样本:45, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:46, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:47, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:48, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:49, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:50, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:51, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:52, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:53, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:54, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:55, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:56, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:57, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:58, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:59, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:60, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:61, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:62, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:63, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:64, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:65, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:66, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:67, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:68, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:69, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:70, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:71, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:72, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:73, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:74, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:75, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:76, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:77, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:78, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:79, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:80, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:81, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:82, alpha优化次数:2\n",
      "alpha_j变化太小\n",
      "全样本遍历:第4次迭代 样本:83, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:84, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:85, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:86, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:87, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:88, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:89, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:90, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:91, alpha优化次数:2\n",
      "L==H\n",
      "全样本遍历:第4次迭代 样本:92, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:93, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:94, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:95, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:96, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:97, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:98, alpha优化次数:2\n",
      "全样本遍历:第4次迭代 样本:99, alpha优化次数:2\n",
      "迭代次数: 5\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:0, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:3, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:6, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:11, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:13, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:14, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:15, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:16, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:18, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:19, alpha优化次数:0\n",
      "非边界遍历:第5次迭代 样本:21, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:26, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:27, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:30, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:31, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:33, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:36, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:40, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:41, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:42, alpha优化次数:0\n",
      "非边界遍历:第5次迭代 样本:45, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:48, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:50, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:54, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:56, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:62, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:74, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:76, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "非边界遍历:第5次迭代 样本:83, alpha优化次数:0\n",
      "迭代次数: 6\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:0, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:1, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:2, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:3, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:4, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:5, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:6, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:7, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:8, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:9, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:10, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:11, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:12, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:13, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:14, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:15, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:16, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:17, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:18, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:19, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:20, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:21, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:22, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:23, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:24, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:25, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:26, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:27, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:28, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:29, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:30, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:31, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:32, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:33, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:34, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:35, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:36, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:37, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:38, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:39, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:40, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:41, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:42, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:43, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:44, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:45, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:46, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:47, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:48, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:49, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:50, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:51, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:52, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:53, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:54, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:55, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:56, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:57, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:58, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:59, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:60, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:61, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:62, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:63, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:64, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:65, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:66, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:67, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:68, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:69, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:70, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:71, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:72, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:73, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:74, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:75, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:76, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:77, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:78, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:79, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:80, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:81, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:82, alpha优化次数:0\n",
      "alpha_j变化太小\n",
      "全样本遍历:第6次迭代 样本:83, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:84, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:85, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:86, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:87, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:88, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:89, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:90, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:91, alpha优化次数:0\n",
      "L==H\n",
      "全样本遍历:第6次迭代 样本:92, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:93, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:94, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:95, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:96, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:97, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:98, alpha优化次数:0\n",
      "全样本遍历:第6次迭代 样本:99, alpha优化次数:0\n",
      "迭代次数: 7\n",
      "支持向量个数:29\n",
      "训练集错误率: 7.00%\n",
      "测试集错误率: 8.00%\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Author:\n",
    "    Jack Cui\n",
    "Blog:\n",
    "    http://blog.csdn.net/c406495762\n",
    "Zhihu:\n",
    "    https://www.zhihu.com/people/Jack--Cui/\n",
    "Modify:\n",
    "    2017-10-03\n",
    "\"\"\"\n",
    "\n",
    "class optStruct:\n",
    "    \"\"\"\n",
    "    数据结构，维护所有需要操作的值\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        kTup - 包含核函数信息的元组,第一个参数存放核函数类别，第二个参数存放必要的核函数需要用到的参数\n",
    "    \"\"\"\n",
    "    def __init__(self, dataMatIn, classLabels, C, toler, kTup):\n",
    "        self.X = dataMatIn                                #数据矩阵\n",
    "        self.labelMat = classLabels                        #数据标签\n",
    "        self.C = C                                         #松弛变量\n",
    "        self.tol = toler                                 #容错率\n",
    "        self.m = np.shape(dataMatIn)[0]                 #数据矩阵行数\n",
    "        self.alphas = np.mat(np.zeros((self.m,1)))         #根据矩阵行数初始化alpha参数为0   \n",
    "        self.b = 0                                         #初始化b参数为0\n",
    "        self.eCache = np.mat(np.zeros((self.m,2)))         #根据矩阵行数初始化虎误差缓存，第一列为是否有效的标志位，第二列为实际的误差E的值。\n",
    "        self.K = np.mat(np.zeros((self.m,self.m)))        #初始化核K\n",
    "        for i in range(self.m):                            #计算所有数据的核K\n",
    "            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n",
    "\n",
    "def kernelTrans(X, A, kTup):\n",
    "    \"\"\"\n",
    "    通过核函数将数据转换更高维的空间\n",
    "    Parameters：\n",
    "        X - 数据矩阵\n",
    "        A - 单个数据的向量\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        K - 计算的核K\n",
    "    \"\"\"\n",
    "    m,n = np.shape(X)\n",
    "    K = np.mat(np.zeros((m,1)))\n",
    "    if kTup[0] == 'lin': K = X * A.T                       #线性核函数,只进行内积。\n",
    "    elif kTup[0] == 'rbf':                                 #高斯核函数,根据高斯核函数公式进行计算\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j,:] - A\n",
    "            K[j] = deltaRow*deltaRow.T\n",
    "        K = np.exp(K/(-1*kTup[1]**2))                     #计算高斯核K\n",
    "    else: raise NameError('核函数无法识别')\n",
    "    return K                                             #返回计算的核K\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "    Parameters:\n",
    "        fileName - 文件名\n",
    "    Returns:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    \"\"\"\n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():                                     #逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])      #添加数据\n",
    "        labelMat.append(float(lineArr[2]))                          #添加标签\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def calcEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算误差\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据\n",
    "    Returns:\n",
    "        Ek - 标号为k的数据误差\n",
    "    \"\"\"\n",
    "    fXk = float(np.multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n",
    "    Ek = fXk - float(oS.labelMat[k])\n",
    "    return Ek\n",
    "\n",
    "def selectJrand(i, m):\n",
    "    \"\"\"\n",
    "    函数说明:随机选择alpha_j的索引值\n",
    "\n",
    "    Parameters:\n",
    "        i - alpha_i的索引值\n",
    "        m - alpha参数个数\n",
    "    Returns:\n",
    "        j - alpha_j的索引值\n",
    "    \"\"\"\n",
    "    j = i                                 #选择一个不等于i的j\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    "\n",
    "def selectJ(i, oS, Ei):\n",
    "    \"\"\"\n",
    "    内循环启发方式2\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "        Ei - 标号为i的数据误差\n",
    "    Returns:\n",
    "        j, maxK - 标号为j或maxK的数据的索引值\n",
    "        Ej - 标号为j的数据误差\n",
    "    \"\"\"\n",
    "    maxK = -1; maxDeltaE = 0; Ej = 0                         #初始化\n",
    "    oS.eCache[i] = [1,Ei]                                      #根据Ei更新误差缓存\n",
    "    validEcacheList = np.nonzero(oS.eCache[:,0].A)[0]        #返回误差不为0的数据的索引值\n",
    "    if (len(validEcacheList)) > 1:                            #有不为0的误差\n",
    "        for k in validEcacheList:                           #遍历,找到最大的Ek\n",
    "            if k == i: continue                             #不计算i,浪费时间\n",
    "            Ek = calcEk(oS, k)                                #计算Ek\n",
    "            deltaE = abs(Ei - Ek)                            #计算|Ei-Ek|\n",
    "            if (deltaE > maxDeltaE):                        #找到maxDeltaE\n",
    "                maxK = k; maxDeltaE = deltaE; Ej = Ek\n",
    "        return maxK, Ej                                        #返回maxK,Ej\n",
    "    else:                                                   #没有不为0的误差\n",
    "        j = selectJrand(i, oS.m)                            #随机选择alpha_j的索引值\n",
    "        Ej = calcEk(oS, j)                                    #计算Ej\n",
    "    return j, Ej                                             #j,Ej\n",
    "\n",
    "def updateEk(oS, k):\n",
    "    \"\"\"\n",
    "    计算Ek,并更新误差缓存\n",
    "    Parameters：\n",
    "        oS - 数据结构\n",
    "        k - 标号为k的数据的索引值\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    Ek = calcEk(oS, k)                                        #计算Ek\n",
    "    oS.eCache[k] = [1,Ek]                                    #更新误差缓存\n",
    "\n",
    "def clipAlpha(aj,H,L):\n",
    "    \"\"\"\n",
    "    修剪alpha_j\n",
    "    Parameters:\n",
    "        aj - alpha_j的值\n",
    "        H - alpha上限\n",
    "        L - alpha下限\n",
    "    Returns:\n",
    "        aj - 修剪后的alpah_j的值\n",
    "    \"\"\"\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    "\n",
    "def innerL(i, oS):\n",
    "    \"\"\"\n",
    "    优化的SMO算法\n",
    "    Parameters：\n",
    "        i - 标号为i的数据的索引值\n",
    "        oS - 数据结构\n",
    "    Returns:\n",
    "        1 - 有任意一对alpha值发生变化\n",
    "        0 - 没有任意一对alpha值发生变化或变化太小\n",
    "    \"\"\"\n",
    "    #步骤1：计算误差Ei\n",
    "    Ei = calcEk(oS, i)\n",
    "    #优化alpha,设定一定的容错率。\n",
    "    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n",
    "        #使用内循环启发方式2选择alpha_j,并计算Ej\n",
    "        j,Ej = selectJ(i, oS, Ei)\n",
    "        #保存更新前的aplpha值，使用深拷贝\n",
    "        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n",
    "        #步骤2：计算上下界L和H\n",
    "        if (oS.labelMat[i] != oS.labelMat[j]):\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L == H:\n",
    "            print(\"L==H\")\n",
    "            return 0\n",
    "        #步骤3：计算eta\n",
    "        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j]\n",
    "        if eta >= 0:\n",
    "            print(\"eta>=0\")\n",
    "            return 0\n",
    "        #步骤4：更新alpha_j\n",
    "        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej)/eta\n",
    "        #步骤5：修剪alpha_j\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n",
    "        #更新Ej至误差缓存\n",
    "        updateEk(oS, j)\n",
    "        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n",
    "            print(\"alpha_j变化太小\")\n",
    "            return 0\n",
    "        #步骤6：更新alpha_i\n",
    "        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])\n",
    "        #更新Ei至误差缓存\n",
    "        updateEk(oS, i)\n",
    "        #步骤7：更新b_1和b_2\n",
    "        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n",
    "        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n",
    "        #步骤8：根据b_1和b_2更新b\n",
    "        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]): oS.b = b1\n",
    "        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]): oS.b = b2\n",
    "        else: oS.b = (b1 + b2)/2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup = ('lin',0)):\n",
    "    \"\"\"\n",
    "    完整的线性SMO算法\n",
    "    Parameters：\n",
    "        dataMatIn - 数据矩阵\n",
    "        classLabels - 数据标签\n",
    "        C - 松弛变量\n",
    "        toler - 容错率\n",
    "        maxIter - 最大迭代次数\n",
    "        kTup - 包含核函数信息的元组\n",
    "    Returns:\n",
    "        oS.b - SMO算法计算的b\n",
    "        oS.alphas - SMO算法计算的alphas\n",
    "    \"\"\"\n",
    "    oS = optStruct(np.mat(dataMatIn), np.mat(classLabels).transpose(), C, toler, kTup)                #初始化数据结构\n",
    "    iter = 0                                                                                         #初始化当前迭代次数\n",
    "    entireSet = True; alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):                            #遍历整个数据集都alpha也没有更新或者超过最大迭代次数,则退出循环\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:                                                                                #遍历整个数据集                           \n",
    "            for i in range(oS.m):       \n",
    "                alphaPairsChanged += innerL(i,oS)                                                    #使用优化的SMO算法\n",
    "                print(\"全样本遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:                                                                                         #遍历非边界值\n",
    "            nonBoundIs = np.nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]                        #遍历不在边界0和C的alpha\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i,oS)\n",
    "                print(\"非边界遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter,i,alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet:                                                                                #遍历一次后改为非边界遍历\n",
    "            entireSet = False\n",
    "        elif (alphaPairsChanged == 0):                                                                #如果alpha没有更新,计算全样本遍历\n",
    "            entireSet = True \n",
    "        print(\"迭代次数: %d\" % iter)\n",
    "    return oS.b,oS.alphas                                                                             #返回SMO算法计算的b和alphas\n",
    "\n",
    "def testRbf(k1 = 1.3):\n",
    "    \"\"\"\n",
    "    测试函数\n",
    "    Parameters:\n",
    "        k1 - 使用高斯核函数的时候表示到达率\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF.txt')                        #加载训练集\n",
    "    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 100, ('rbf', k1))        #根据训练集计算b和alphas\n",
    "    datMat = np.mat(dataArr); labelMat = np.mat(labelArr).transpose()\n",
    "    svInd = np.nonzero(alphas.A > 0)[0]                                        #获得支持向量\n",
    "    sVs = datMat[svInd]                                                     \n",
    "    labelSV = labelMat[svInd];\n",
    "    print(\"支持向量个数:%d\" % np.shape(sVs)[0])\n",
    "    m,n = np.shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))                #计算各个点的核\n",
    "        predict = kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b     #根据支持向量的点，计算超平面，返回预测结果\n",
    "        if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1        #返回数组中各元素的正负符号，用1和-1表示，并统计错误个数\n",
    "    print(\"训练集错误率: %.2f%%\" % ((float(errorCount)/m)*100))             #打印错误率\n",
    "    dataArr,labelArr = loadDataSet('testSetRBF2.txt')                         #加载测试集\n",
    "    errorCount = 0\n",
    "    datMat = np.mat(dataArr); labelMat = np.mat(labelArr).transpose()         \n",
    "    m,n = np.shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))                 #计算各个点的核           \n",
    "        predict=kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b         #根据支持向量的点，计算超平面，返回预测结果\n",
    "        if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1        #返回数组中各元素的正负符号，用1和-1表示，并统计错误个数\n",
    "    print(\"测试集错误率: %.2f%%\" % ((float(errorCount)/m)*100))             #打印错误率\n",
    "\n",
    "def showDataSet(dataMat, labelMat):\n",
    "    \"\"\"\n",
    "    数据可视化\n",
    "    Parameters:\n",
    "        dataMat - 数据矩阵\n",
    "        labelMat - 数据标签\n",
    "    Returns:\n",
    "        无\n",
    "    \"\"\"\n",
    "    data_plus = []                                  #正样本\n",
    "    data_minus = []                                 #负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)              #转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)            #转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1])   #正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) #负样本散点图\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    testRbf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
