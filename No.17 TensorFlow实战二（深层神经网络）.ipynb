{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - 深度学习与深层神经网络\n",
    "\n",
    "深度学习的**精确定义为：“一类通过多层非线性变换对高复杂性数据建模算法的集合”**\n",
    "\n",
    "因此，多层神经网络有着2个非常重要的特性\n",
    "\n",
    "* 多层\n",
    "\n",
    "* 非线性\n",
    "\n",
    "# 1.1 - 线性模型的局限性\n",
    "\n",
    "在线性模型中，模型的输出为输入的加权和，假设一个模型的输出y和输入$x_i$满足以下关系，那么这么模型就是一个线性模型\n",
    "$$y=\\sum_{i}w_ix_i+b$$\n",
    "其中$w_i，b\\in R$为模型的参数，而一个线性模型的最大特点是任意线性模型的组合仍然还是线性模型\n",
    "\n",
    "前向传播的计算公式为：$$a^{(1)}=xW^{(i)}$$,y=a^{(1)}W^{(2)}\n",
    "其中x为输入，W为参数。整理一下上面的公式可以得到整个模型的输出为：\n",
    "$$y=(xW^{(1)})W^{(2)}$$\n",
    "根据矩阵乘法的结合律有：\n",
    "$$y=x(W^{(1)}W^{(2)})=XW^{'}$$\n",
    "而$W^{(1)}W^{(2)}$\n",
    "根据矩阵乘法的结合律有：\n",
    "$$y=x(W^{(1)}W^{(2)})=XW^{'}$$\n",
    "而$W^{(1)}W^{(2)}$其实可以被表示为一个新的参数$W^{'}$:\n",
    "$$W^{'}=x(W^{(1)}W^{(2)})=\\begin{bmatrix}\n",
    " W_{1,1}^{(1)}&  W_{1,2}^{(1)}& W_{1,3}^{(1)}\\\\ \n",
    "W_{2,1}^{(1)} & W_{2,2}^{(1)} & W_{2,3}^{(1)}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    " W_{1,1}^{(2)} \\\\ \n",
    " W_{2,1}^{(2)} \\\\\n",
    " W_{3,1}^{(2)}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    " W_{1,1}^{(1)}W_{1,1}^{(2)}+ W_{1,2}^{(1)}W_{2,1}^{(2)}+ W_{1,3}^{(1)}W_{3,1}^{(2)} \\\\ \n",
    "  W_{2,1}^{(1)}W_{1,1}^{(2)}+ W_{2,2}^{(1)}W_{2,1}^{(2)}+ W_{2,3}^{(1)}W_{3,1}^{(2)}  \n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    " W_1^{'} \\\\ \n",
    "  W_2^{'}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "其中$W^{'}$是新的参数。整个前向传播的算法完全符合线性模型的定义。因此虽然这个神经网络有两层，但是它和单层的神经网络并没有什么区别。因此如果都是线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别。这就是线性模型最大的局限性，也是为什么深度学习要强调非线性。\n",
    "\n",
    "虽然在解决线性分类问题时，线性模型可以很好的解决问题。\n",
    "\n",
    "(TensorFlow游乐场http://playground.tensorflow.org 是一个通过网页浏览器就可以训练的简单神经网络并实现了可视化训练过程的工具。)\n",
    "\n",
    "如图，在TensorFlow游乐场中，经过506次迭代之后线性模型很好的画出了二分问题的分界线，并且损失函数为0\n",
    "\n",
    "![2018-11-05_143956.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-1.png?raw=true)\n",
    "\n",
    "但是对于非线性数据，线性模型就不能很好的把数据分类，经过507次迭代后仍然不能画出分界线，并且损失函数的值很高\n",
    "\n",
    "![2018-11-05_144135.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-2.png?raw=true)\n",
    "\n",
    "当我们将线性的激活函数换成非线性的ReLu函数时，经过507次迭代后，画出的分界线明显好于线性模型，并且损失函数的值只有0.005。\n",
    "\n",
    "![2018-11-05_144422.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-3.png?raw=true)\n",
    "\n",
    "\n",
    "# 1.2 - 激活函数实现去线性化\n",
    "\n",
    "如果将每一个神经元（也就是神经网络中的节点）的输出通过一个非线性函数，那么整个神经网络也就不再是线性了。这个非线性函数就是激活函数![QQ%E5%9B%BE%E7%89%8720181105141902.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png?raw=true)\n",
    "使用激活函数主要有两个改变\n",
    "* 新的公式中添加了偏置项（bias）偏置项是神经网络中非常常用的一种结构\n",
    "\n",
    "* 每个节点的输出在加权和的基础上海做了一个非线性变换\n",
    "\n",
    "下图是几种常用的非线性激活函数的函数图像：\n",
    "![QQ%E5%9B%BE%E7%89%8720181105142347.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B02.png?raw=true)\n",
    "\n",
    "目前TensorFlow提供了7种不同的线性激活函数\n",
    "* tf.nn.relu\n",
    "* tf.sigmoid\n",
    "* tf.tanh\n",
    "是其中比较常用的几个。当然TensorFlow也支持自己定义的激活函数\n",
    "\n",
    "使用激活函数的前向传播算法代码如下：\n",
    "```\n",
    "a = tf.nn.relu(tf.matmul(x,w1)+biases1)\n",
    "y = tf.nn.relu(tf.matmul(a,w2)+biases2)\n",
    "```\n",
    "# 2 - 损失函数\n",
    "\n",
    "## 2.1 - 分类问题的经典损失函数\n",
    "\n",
    "分类问题和回归问题是监督学习的两大种类。分类问题是将不同的样本分到实现定义好的类别中。如判断一个零件是否合格的问题是一个二分类问题，而手写体数字识别问题可以被归纳为一个十分类问题。\n",
    "\n",
    "之前在解决判断零件是否合格的二分类问题时，设立了一个0.5的阈值，凡是输出大于0.5的样本都认为是合格的，小于0.5的则是不合格的。然而这种做法在实际问题解决的过程中一般不会这么处理\n",
    "\n",
    "通过神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。对于每一个样例，神经网络可以得到的一个n维数组作为输出结果。数组中的每一个维度（也就是每一个输出节点）对应一个类别。以识别数字1位例，神经网络模型的输出结果越接近$[1,0,0,0,0,0,0,0,0,0]$越好。那么如何判断一个输出向量和期望的向量有多接近呢？\n",
    "\n",
    "**交叉熵（cross entropy）**是常用的评判方法之一，交叉熵刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数\n",
    "给定两个概率分布p和q，通过q来表示p的交叉熵为：\n",
    "$$H(p,q)=-\\sum_{x}p(x)logq(x)$$\n",
    "注意交叉熵刻画的是两个概率分布之间的距离，然而神经网络的输出却不一定是一个概率分布，概率分布刻画了不同事件发生的概率。当时间总数是有限的情况下，概率分布函数p(X=x)满足：\n",
    "$$\\forall x p(X=x)\\in [0,1]且\\sum_{x}p(X=x)=1$$\n",
    "如何将神经网络前向传播得到的结果也变成概率分布呢？Softmax回归就是一个非常常用的方法。\n",
    "\n",
    "Softmax回归本身可以作为一个学习算法来优化分类结果，但在TensorFlow中，Softmax回归的参数被去掉了，它只是一层额外的处理层，将神经网络的输出编程了一个概率分布。如图展示了加上Softmax回归的神经网络结构图\n",
    "![QQ%E5%9B%BE%E7%89%8720181105151956.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-softmax.png?raw=true)\n",
    "\n",
    "假设原始的神经网络输出为$y_1,y_2,\\dots,y_n$，那么经过Softmax回归处理之后的输出为：\n",
    "\n",
    "$$softmax(y)_i=y_i^{'}=\\frac{e^{y_i}}{\\sum_{j=1}^ne^{y_j}}$$\n",
    "\n",
    "假设有一个三分类问题，某个样例的正确答案是（1,0,0）。某模型经过Softmax回归之后的预测答案是（0.5,0.4,0.1）那么这个预测和正确答案之间的交叉熵为：\n",
    "$$H((1,0,0),(0.5,0.4,0.1))=-(1*log0.5+0*log0.4+0*log0.1)\\approx 0.3$$\n",
    "\n",
    "如果另外一个模型的预测是（0.8,0.1,0.1），那么这个预测值和真是值之间的交叉熵是：\n",
    "$$H((1,0,0),(0.8,0.1,0.1))=-(1*log0.8+0*log0.1+0*log0.1)\\approx 0.1$$\n",
    "\n",
    "从直观上可以很容易的指导第二个预测答案要优于第一个，通过交叉熵计算得到的结果也是一致的（第二个交叉熵的值更小）\n",
    "\n",
    "其代码实现如下：\n",
    "```\n",
    "cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "```\n",
    "其中y_代表正确结果，y代表预测结果。通过tf.clip_by_value函数可以将一个张量中的数值限制在一个范围之内，这样可以避免一些运算错误（比如log0是无效的）下面给出了使用tf.clip_by_value的简单样例。\n",
    "```\n",
    "v = tf.constant([1.0,2.0,3.0],[4.0,5.0,6.0])\n",
    "print(tf.clip_by_value(v,2.5,4.5).eval()\n",
    "#输出[[2.5,2.5,3],[4,4.5,4.5]]\n",
    "```\n",
    "因为交叉熵一般会与softmax回归一起使用，所以TensorFlow对这个功能进行了统一封装，并提供了tf.nn.softmax_cross_entropy_with_logits函数。比如可以直接通过下面代码来实现softmax回归之后的交叉熵损失函数：\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y,y_)\n",
    "\n",
    "其中y代表了原始神经网络的输出结果，而y_给出了标准答案。这样通过一个命令就可以得到使用了Softmax回归之后的交叉熵。\n",
    "\n",
    "## 2.2 - 回归问题的损失函数\n",
    "\n",
    "与分类问题不同，回归问题解决的是对具体数值的预测。比如房价预测、销量预测等都是回归问题。这些问题需要预测的不是一个事先定义好的类别，而是一个任意实数。解决回归问题的神经网络一般只有一个输出节点，这个界定啊的输出值就是预测值。对于回归问题，最长用的损失函数是均方差误差（MSE,mean squared error）。它的定义如下：\n",
    "\n",
    "$$MSE(y,y^{'})=\\frac{\\sum_{i=1}^n(y_i-y_i^{'})^2}{n}$$\n",
    "\n",
    "其中$y_i$为一个batch中第i个数据的正确答案，而$y_i^{'}$为神经网络给出的预测值。以下代码展示了如何通过TensorFlow实现均方差损失函数：\n",
    "$$mse=tf.reduce_mean(tf.square(y_-y))$$\n",
    "其中y代表了神经网络的输出答案，y_代表了标准答案。\n",
    "\n",
    "## 2.3 - 自定义损失函数\n",
    "\n",
    "TensorFlow不仅支持经典的损失函数，还也优化任意的自定义损失函数。\n",
    "如以预测商品销量问题为例：\n",
    " \n",
    " 在预测商品销量时，因为利润和成本代价不同，少预测一个的代价为10元，多预测一个的代价为1元，因此均方差代价函数无法最大化预期的利润。\n",
    " \n",
    " 所以可以自定义损失函数：\n",
    " $$Loss(y,y^{'})=\\sum_{i=1}^nf(y_i,y_i^{'}),f(x,y)=\\begin{cases}\n",
    " 10(x-y)& \\text{ if } x>y \\\\ \n",
    " (y-x)& \\text{ if } x\\leq y \n",
    "\\end{cases}$$\n",
    "\n",
    "在TensorFlow中，可以通过以下代码来实现这个损失函数。\n",
    "```\n",
    "loss=tf.reduce_sum(tf.where(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))\n",
    "```\n",
    "* tf.greater：输入两个张量，比较大小，并返回True or false \n",
    "\n",
    "* tf.where:有三个参数，第一个为选择条件根据，当选择条件为True时，选择第二个参数的值，为False时，返回的是第三个参数的值\n",
    "\n",
    "以下是这两个函数的使用例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True]\n",
      "[4 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "v1 = tf.constant([1,2,3,4])\n",
    "\n",
    "v2 = tf.constant([4,3,2,1])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(tf.greater(v1,v2).eval())\n",
    "\n",
    "print(tf.where(tf.greater(v1,v2),v1,v2).eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义了损失函数之后，下面将通过一个简单的神经网络程序来讲解损失函数对模型训练结果的影响，下面市县一个拥有两个输入节点，一个输出节点，没有隐藏层的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1= [[1.019347 ]\n",
      " [1.0428089]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "batch_size = 8\n",
    "#两个输入节点\n",
    "x = tf.placeholder(tf.float32, shape=(None,2), name='x-input')\n",
    "#回归问题一般只有一个输出节点\n",
    "y_ = tf.placeholder(tf.float32, shape=(None,1), name='y-input')\n",
    "\n",
    "#定义了一个单层的神经网络前向传播的过程，这里就是简单加权和\n",
    "w1 = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y = tf.matmul(x,w1)\n",
    "\n",
    "#定义预测多了和预测少了的成本\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*loss_more,(y_-y)*loss_less))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#通过随机数生成了一个模拟数据集\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size,2)\n",
    "#设置回归的正确值为两个输入的和加入一个随机量，之所以要加上一个随机量是为了\n",
    "#加入不可预测的噪音，这里的噪音设置为-0.05~0.05的随机数\n",
    "\n",
    "Y = [[x1 + x2 + rdm.rand()/10.0-0.05] for (x1,x2) in X]\n",
    "\n",
    "#训练神经网络\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size)% dataset_size\n",
    "        end = min(start+batch_size,dataset_size)\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "    print(\"w1=\",sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - 神经网络优化算法\n",
    "\n",
    "**反向传播算法（backpropagation）**和**梯度下降算法(gradient decent)**是训练神经网络的核心算法。\n",
    "\n",
    "（有关梯度下降和反向传播的具体原理推导详见学习笔记1、3）\n",
    "\n",
    "\n",
    "* 只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解\n",
    "\n",
    "* 批量梯度下降计算时间太长。为了加速训练过程，可以使用随机梯度下降算法（stochastic gradient descnet）\n",
    "\n",
    "但是随机梯度下降算法每次优化的只是某一条数据上的损失函数，所以它的问题也非常明显：在某一条数据上损失函数更小并不代表在全部数据上损失函数更小，有可能连局部最优都无法达到\n",
    "\n",
    "因此，在实际应用中一般采用这两个算法的折中——**每次计算计算一小部分训练数据的损失函数。**这一小部分数据被称之为一个batch。通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多。\n",
    "\n",
    "另一方面，每次使用一个batch可以大大减少收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。\n",
    "\n",
    "因此在本文样例使用中，神经网络的训练大致遵循以下过程：\n",
    "```\n",
    "batch_size = n\n",
    "\n",
    "#每次读取一小部分数据作为当前的训练数据来执行反向传播算法。\n",
    "x = tf.placeholder(tf.float32,shape=(batch_size,2),name='x-input')\n",
    "y_ = tf.placeholder(tf.float32,shape=(batch_size,1),name='y-input')\n",
    "\n",
    "#定义神经网络结构和优化算法\n",
    "loss = ...\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#训练神经网络\n",
    "with tf.Session() as sess:\n",
    "    #参数初始化\n",
    "    ...\n",
    "    #迭代的更新参数\n",
    "    for i in range (STEPS):\n",
    "        #准备batch_size个训练数据。一般将所有训练数据打乱之后再选取可以得到更好的优化效果\n",
    "        current_X,current_Y= ...\n",
    "        sess_run(train_step,feed_dict={x:current_X,y:current_Y})\n",
    "```\n",
    "# 4 - 神经网络进一步优化\n",
    "\n",
    "## 4.1 - 学习速率的设置\n",
    "\n",
    "当学习速率过大时，参数将来回波动不会收敛到极小值，相反当学习速率过小时，虽然能保证收敛性，但是这回大大降低优化速率。\n",
    "\n",
    "为了解决学习率的问题，TensorFlow提供了一种更加灵活的学习率设置方式——指数衰减法。**tf.train.exponential_decay**函数实现了指数衰减学习率。通过这个函数，可以先使用较大的学习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后期更加稳定。\n",
    "\n",
    "其示范代码：\n",
    "```\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "#通过exponential_decay函数生成学习速率。\n",
    "learning_rate = tf.train.exponential_decay(0.1,global_step,100,0.96,staircase=True)\n",
    "\n",
    "#如果staircase=True下降曲线为阶梯函数\n",
    "#Flase则为连续衰减函数\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf;  \n",
    "import numpy as np;  \n",
    "import matplotlib.pyplot as plt;  \n",
    "  \n",
    "learning_rate = 0.1  \n",
    "decay_rate = 0.96  \n",
    "global_steps = 1000  \n",
    "decay_steps = 100  \n",
    "  \n",
    "global_ = tf.Variable(tf.constant(0))  \n",
    "c = tf.train.exponential_decay(learning_rate, global_, decay_steps, decay_rate, staircase=True)  \n",
    "d = tf.train.exponential_decay(learning_rate, global_, decay_steps, decay_rate, staircase=False)  \n",
    "  \n",
    "T_C = []  \n",
    "F_D = []  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "    for i in range(global_steps):  \n",
    "        T_c = sess.run(c,feed_dict={global_: i})  \n",
    "        T_C.append(T_c)  \n",
    "        F_d = sess.run(d,feed_dict={global_: i})  \n",
    "        F_D.append(F_d)  \n",
    "  \n",
    "  \n",
    "plt.figure(1)  \n",
    "plt.plot(range(global_steps), F_D, 'r-')  \n",
    "plt.plot(range(global_steps), T_C, 'b-')  \n",
    "      \n",
    "plt.show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - 过拟合问题\n",
    "\n",
    "过拟合问题是指模型对数据过度拟合噪音而不能很好的预测数据如下图所示\n",
    "![2018-11-07_161556.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/17-%E8%BF%87%E6%8B%9F%E5%90%88.png?raw=true)\n",
    "\n",
    "为了避免过拟合问题，一个非常常用的方法是正则化（regularization）。正则化的思想就是在损失函数中加入刻画模型复杂程度的指标。\n",
    "\n",
    "假设损失函数为$j(\\theta)$,那么正则化优化的是$J(\\theta)+\\lambda R(w)$。其中$R(W)$刻画的是模型的复杂程度，而$\\lambda$表示模型复杂损失在总损失中的比例\n",
    "\n",
    "常用的刻画模型复杂度的函数R(w)有两种:\n",
    "\n",
    "** L1正则化：$R(w)=\\sum_{i}|w_i|$**\n",
    "\n",
    "** L2正则化：$R(w)=\\sum_{i}|w_i^2|$**\n",
    "\n",
    "两种正则化的基本思想都是通过限制权重的大小。但是也有很大的区别，首先L1会让参数变得更稀疏，可以达到类似特征选取的功能。其次，L1正则化的计算公式不可导，因为在优化时需要计算损失函数的偏导数，所以对含有L2正则化损失函数的优化要更简介。优化带L1正则化的损失函数要更加复杂。\n",
    "\n",
    "在实践中，也可以将L1和L2正则化同时使用：\n",
    "** $$R(w)=\\sum_i\\alpha|w_i|+(1-\\alpha)w_i^2$$ **\n",
    "\n",
    "以下代码给出了一个简单的带L2正则化的损失函数定义：\n",
    "```\n",
    "w = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y = tf.matmul(x,w)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y_-y))+tf.contrib.layers.l2_regularizer(lambda)(w)\n",
    "```\n",
    "\n",
    "在上面的程序中，loss为定义的损失函数，它由两个部分组成。第一个部分是**tf.reduce_mean**均方差损失函数，它刻画了模型在训练数据上的表现。第二个部分就是正则化，它模型过度拟合数据中的随机噪音。lambda参数表示了正则化的权重，也就是公式$J(\\theta)+\\lambda R(w)$中的$\\lambda$。$w$为需要计算正则化损失的参数。TensorFlow提供了**tf.contrib.layers.l2_regularizer函数**，这个函数可以计算一个给定参数的L2正则化项的值。\n",
    "\n",
    "类似的，**tf.contib.layers.l1_regularizer**可以计算L1正则化的值\n",
    "\n",
    "但当神经网络的参数增多之后，这样的方式可能导致损失函数loss的定义很长，可读性差且容易出错。但更主要的是，当网络结构复杂之后网络结构的部分和计算损失函数的部分可能不在同一个函数中，这样通过变量这种方式计算损失函数就不方便了。为了解决这个问题，可以使用TensorFlow中提供的**集合（collection）。**\n",
    "\n",
    "它可以在计算图（tf.Graph）中保存一组实体（比如张量）。一下代码给出了通过集合计算一个5层神经网络带L2正则化的损失函数的计算方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 获得一层神经网络边上的数量，并将这个权重L2正则化损失加入名称为‘losses’的集合中\n",
    "def get_weight(shape,Lambda):\n",
    "    #生成一个变量\n",
    "    var = tf.Variable(tf.random_normal(shape),dtype=tf.float32)\n",
    "    # add_to_collection函数将这个新生成变量的L2正咋花损失项加入集合\n",
    "    #这个函数的第一个参数'losses'是这个集合的名字，第二个参数是要加入这个集合的内容\n",
    "    tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(Lambda)(var))\n",
    "    #返回生成的变量。\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1))\n",
    "batch_size = 8\n",
    "#定义了每一层网络中节点的个数。\n",
    "layer_dimension = [2,10,10,10,1]\n",
    "# 神经网络的层数\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "#这个变量维护前向传播时最深层的节点，开始的时候就是输入层。\n",
    "cur_layer = x\n",
    "#当前层的节点个数\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "#通过一个循环来生成5层全连接的神经网络结构。\n",
    "for i in range(1,n_layers):\n",
    "    # layer_dimension[i]为下一层的节点个数\n",
    "    out_dimension = layer_dimension[i]\n",
    "    # 生成当前层中权重的变量，并将这个变量的L2正则化损失加入计算图上的集合。\n",
    "    weight = get_weight([in_dimension,out_dimension],0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1,shape=[out_dimension]))\n",
    "    #使用ReLu激活函数\n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer,weight)+bias)\n",
    "    #进入下一层之前将下一层的节点个数更新为当前层节点个数\n",
    "    in_dimension = layer_dimension[i]\n",
    "\n",
    "#在定义神经网络前向传播的同时已经将所有的L2正则化损失加入了图上的集合，\n",
    "#这里只需要计算刻画模型在训练数据上表现的损失函数。\n",
    "mse_loss = tf.reduce_mean(tf.square(y_-cur_layer))\n",
    "\n",
    "#将均方误差损失函数加入损失集合。\n",
    "tf.add_to_collection('losses',mse_loss)\n",
    "\n",
    "#get_collection返回一个列表，这个列表是所有这个集合中的元素，这个样例中\n",
    "#这些元素就是损失函数的不同部分，将它们加起来就可以得到最终的损失函数\n",
    "loss = tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - 滑动平均模型\n",
    "\n",
    "在采用随机梯度下降算法训练神经网络时，使用滑动平均模型在很多应用中都可以在一定程度提高最终模型在测试数据上的表现\n",
    "\n",
    "在TensorFlow中提供了tf.train.ExponentialMovingAverage来实现滑动平均模型。在初始化ExponentialMovingAverage时，需要提供一个衰减率（decay）。这个衰减率将用于控制模型的更新速度。ExponentialMovingAverage对每一个变量会维护一个影子变量（shadowvariable），这个影子变量的初始值就是相应变量的初始值，而每次运行变量更新时，影子变量的值会更新为：\n",
    "\n",
    "**shadow_variable = decay * shadow_variable+(1-decay)*variable**\n",
    "\n",
    "从公式中可以看到，decay决定了模型更新的速度，decay越大模型越趋于稳定。在实际应用中，decay一般会设成非常接近1的数（比如0.999或0.9999）。\n",
    "\n",
    "为了使得模型在训练前期可以更新得更快，ExponentialMovingAverage还提供了num_updates参数来动态设置decay的大小。如果在ExponentialMovingAverage初始化时提供了num_updates参数，那么每次使用的衰减率将是：\n",
    "$$min\\left\\{decay,\\frac{1+num＿updates}{10+num＿updates}\\right\\}$$\n",
    "\n",
    "下面给出一段ExponentialMovingAverage的示范代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[5.0, 4.5]\n",
      "[10.0, 4.555]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "#定义一个变量用于计算滑动平均，这个变量的初始值为0，注意这里手动指定了变量的类型为\n",
    "#tf.float32，因为所有需要计算滑动平均的变量必须是实数型。\n",
    "v1 = tf.Variable(0,dtype=tf.float32)\n",
    "#这里step变量模拟神经网络中迭代的轮数，可以用于动态控制衰减率\n",
    "step = tf.Variable(0,trainable=False)\n",
    "\n",
    "#定义一个滑动平均的类（class)。初始化时给定了衰减率（0.99）和控制衰减率的变量step。\n",
    "ema = tf.train.ExponentialMovingAverage(0.99,step)\n",
    "# 定义一个更新变量滑动平均的操作，这里需要给定一个列表，每次执行这个操作时\n",
    "# 这个列表中的变量都会被更新。\n",
    "maintain_averager_op = ema.apply({v1})\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    #通过ema_average(v1)获取滑动平均值后变量的取值。在初始化值后变量v1和v1的\n",
    "    #滑动平均都为0\n",
    "    print(sess.run([v1,ema.average(v1)]))\n",
    "\n",
    "    #更新变量v1的值到5\n",
    "    sess.run(tf.assign(v1,5))\n",
    "    #更新v1的滑动平均值。衰减率为min{0.99,(1+step)/(10+step)=0,1}=0.1\n",
    "    #所以v1的滑动平均值会被更新为0.1*0+0.9*5 = 4.5\n",
    "    sess.run(maintain_averager_op)\n",
    "    print(sess.run([v1,ema.average(v1)]))\n",
    "\n",
    "    #更新step的值为10000\n",
    "    sess.run(tf.assign(step,10000))\n",
    "    #更新v1的值为10\n",
    "    sess.run(tf.assign(v1,10))\n",
    "    #更新v1的滑动平均值。衰减率为min{0.99,(1+step)/(10+step)=0.999}=0.99\n",
    "    #所以v1的滑动平均值会被更新为0.99*4.5+0.01*10=4.555\n",
    "    sess.run(maintain_averager_op)\n",
    "    print(sess.run([v1,ema.average(v1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "滑动平均模型让最后得到的模型在未知数据上更加健壮。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
