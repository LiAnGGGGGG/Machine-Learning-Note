{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 一、概述\n",
    "\n",
    "## 1.1 简介\n",
    "\n",
    "朴素贝叶斯（Naive Bayesian）是基于贝叶斯定理和特征条件独立假设的分类方法，它通过特征计算分类的概率，选取概率大的情况进行分类，因此它是基于概率论的一种机器学习分类方法。因为分类的目标是确定的，所以也是属于监督学习。\n",
    "\n",
    "### Q1：什么是基于概率论的方法？\n",
    "\n",
    "通过概率来衡量事件发生的可能性。概率论和统计学恰好是两个相反的概念，统计学是抽取部分样本进行统计来估算总体的情况，而概率论是通过总体情况来估计单个事件或者部分事情的发生情况。因此，概率论需要已知的数据去预测未知的事件。\n",
    "\n",
    "例如，我们看到天气乌云密布，电闪雷鸣并阵阵狂风，在这样的天气特征(F)下，我们推断下雨的概率比不下雨的概率大，也就是p(下雨)>p(不下雨)p(下雨)>p(不下雨),所以认为待会儿会下雨。这个从经验上看对概率进行判断。 \n",
    "\n",
    "而气象局通过多年长期积累的数据，经过计算，今天下雨的概率p(下雨)=85%,p(不下雨)=15%p(下雨)=85%,p(不下雨)=15%,同样的，p(下雨）>p(不下雨)p(下雨）>p(不下雨)，因此今天的天气预报肯定预报下雨。这是通过一定的方法计算概率从而对下雨事件进行判断。\n",
    "\n",
    "### Q2:朴素贝叶斯，朴素在什么地方？ \n",
    "\n",
    "之所以叫朴素贝叶斯，因为它简单、易于操作，基于特征独立性假设，假设各个特征不会相互影响，这样就大大减小了计算概率的难度。\n",
    "\n",
    "## 1.2 条件概率与贝叶斯定理\n",
    "\n",
    "### （1）概率论中几个基本概念\n",
    "\n",
    "### 事件交和并： \n",
    "\n",
    "A和B两个事件的交，指的是事件A和B同时出现，记为A∩B; \n",
    "\n",
    "A和B两个事件的并，指的是事件A和事件B至少出现一次的情况，记为A∪B。\n",
    "\n",
    "### 互补事件：\n",
    "\n",
    "事件A的补集，也就是事件A不发生的时候的事件，记为$A^c$。这个时候，要么A发生，要么$A^c$发生，$P(A)+P(A^c)=1$。\n",
    "\n",
    "### 条件概率(conditional probability)： \n",
    "\n",
    "某个事件发生时另外一个事件发生的概率，如事件B发生条件下事件A发生的概率：\n",
    "\n",
    "$$P(A|B)=\\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "### 概率的乘法法则（multiplication rule of probability）： \n",
    "\n",
    "$$P(A\\cap B)=P(A)P(A|B)orP(A\\cap B)=P(B)P(A|B)$$\n",
    "\n",
    "### 独立事件交的概率： \n",
    "\n",
    "两个相互独立的事件，其交的概率为： \n",
    "\n",
    "$$P(A\\cap B)=P(A)P(B)$$\n",
    "\n",
    "### （2）贝叶斯定理（Bayes’s Rule）： \n",
    "\n",
    "如果有k个互斥且有穷个事件 \n",
    "\n",
    "$B_1,B_2,\\dots,B_k$,并且，$P(B_1)+P(B_2)+\\dots+P(B_k)=1$和一个可以观测到的事件A，那么有：\n",
    "\n",
    "$$P(B_i|A)=\\frac{P(B_i\\cap A)}{P(A)}=\\frac{P(B_i)P(A|B_i)}{P(B_i)P(A|B_i)+P(B_2)P(A|B_2)+\\dots+P(B_k)P(A|B_k)}$$\n",
    "\n",
    "$$p(A):事件A发生的概率$$\n",
    "$$p(A\\cap B):事件A和事件B同时发生的概率$$\n",
    "$$p(A|B):表示事件A在事件B发生的条件下发生的概率$$\n",
    "\n",
    "### 1.3 朴素贝叶斯分类的原理\n",
    "\n",
    "朴素贝叶斯基于条件概率、贝叶斯定理和独立性假设原则 \n",
    "\n",
    "### (1)首先，我们来看条件概率原理：\n",
    "\n",
    "基于概率论的方法告诉我们，当只有两种分类时：\n",
    "\n",
    "如果$p1(x,y)>p2(x,y)$，那么分入类别1 \n",
    "如果$p1(x,y)<p2(x,y)$，那么分入类别2\n",
    "\n",
    "### （2）其次，贝叶斯定理\n",
    "\n",
    "同样的道理，引入贝叶斯定理，有： \n",
    "\n",
    "$$p(c_i|x,y)=\\frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$\n",
    "\n",
    "其中，$x,y$表示特征变量，$c_i$表示分类，$p(c_i|x,y)$即表示在特征为$x,y$的情况下分入类别$c_i$的概率，因此，结合条件概率和贝叶斯定理，有：\n",
    "\n",
    "* 如果$p(c_1|x,y)>p(c_2|x,y)$，那么分类应当属于$c_1$；\n",
    "\n",
    "* 如果$p(c_1|x,y)<p(c_2|x,y)$，那么分类应当属于$c_2$；\n",
    "\n",
    "* 贝叶斯定理最大的好处是可以用已知的三个概率去计算未知的概率，而如果仅仅是为了比较$p(c_i|x,y)和p(c_j|x,y)$的大小，只需要已知两个概率即可，分母相同，比较$p(x,y|c_i)p(c_i)$和$p(x,y|c_j)p(c_j)$即可。\n",
    "\n",
    "### （3）特征条件独立假设原则\n",
    "\n",
    "朴素贝叶斯最常见的分类应用是对文档进行分类，因此，最常见的特征条件是文档中，出现词汇的情况，通常将词汇出现的特征条件用词向量 $ω$表示，由多个数值组成，数值的个数和训练样本集中的词汇表个数相同。 \n",
    "因此，上述的贝叶斯条件概率公式可表示为： \n",
    "\n",
    "$$p(c_i|w)=\\frac{p(\\omega|c_i)p(c_i)}{p(\\omega)}$$\n",
    "\n",
    "前面提到朴素贝叶斯还有一个假设，就是基于特征条件独立的假设，也就是我们姑且认为词汇表中各个单词独立出现，不会相互影响，因此,$p(\\omega|c_i)$可以将$\\omega$展开成独立事件概率相乘的形式，因此：\n",
    "\n",
    "$$p(\\omega|c_i)=p(\\omega_0|c_i)p(\\omega_1|c_i)p(\\omega_2|c_i)\\dots p(\\omega_N|c_i)$$\n",
    "\n",
    "### 1.4 朴素贝叶斯分类的流程和优缺点\n",
    "\n",
    "### （1）分类流程\n",
    "\n",
    " 1. 数据准备：收集数据，并将数据预处理为数值型或者布尔型，如对文本分类，需要将文本解析为词向量 \n",
    " 2. 训练数据：根据训练样本集计算词项出现的概率，训练数据后得到各类下词汇出现概率的向量 \n",
    " 3.  测试数据：用测试样本集去测试分类的准确性\n",
    "\n",
    "### （2） 优缺点 \n",
    "\n",
    "1. 监督学习，需要确定分类的目标 \n",
    "2. 对缺失数据不敏感，在数据较少的情况下依然可以使用该方法 \n",
    "3. 可以处理多个类别 的分类问题 \n",
    "4. 适用于标称型数据 \n",
    "5. 对输入数据的形势比较敏感 \n",
    "6. 由于用先验数据去预测分类，因此存在误差\n",
    "\n",
    "# 二、Python算法实现\n",
    "\n",
    "以在线社区的留言板评论为例，运用朴素贝叶斯分类方法，对文本进行自动分类。\n",
    "\n",
    "构造一些实验样本，包括已经切分词条的文档集合，并且已经分类（带有侮辱性言论，和正常言论）。为了获取方便，先构造一个loadDataSet函数来生成实验样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec=[0,1,0,1,0,1] #1表示侮辱性言论，0表示正常言论\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 根据文档词汇表构建词向量\n",
    "\n",
    "(1)构建词汇表生成函数creatVocabList："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet=set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet=vocabSet|set(document) #取两个集合的并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)对输入的词汇表构建词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词集模型\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec=np.zeros(len(vocabList)) #生成零向量的array\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1 #有单词，则该位置填充0\n",
    "        else: print('the word:%s is not in my Vocabulary!'% word)\n",
    "    return returnVec #返回全为0和1的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种构建词向量的方法，只记录了每个词是否出现，而没有记录词出现的次数，这样的模型叫做词集模型，如果在词向量中记录词出现的次数，没出现一次，则多记录一次，这样的词向量构建方法，被称为词袋模型，下面构建以一个词袋模型的词向量生成函数bagOfWord2VecMN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词袋模型\n",
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]+=1\n",
    "    return returnVec #返回非负整数的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garbage',\n",
       " 'love',\n",
       " 'posting',\n",
       " 'steak',\n",
       " 'stupid',\n",
       " 'please',\n",
       " 'not',\n",
       " 'my',\n",
       " 'dalmation',\n",
       " 'I',\n",
       " 'worthless',\n",
       " 'help',\n",
       " 'ate',\n",
       " 'licks',\n",
       " 'how',\n",
       " 'buying',\n",
       " 'food',\n",
       " 'to',\n",
       " 'has',\n",
       " 'problems',\n",
       " 'dog',\n",
       " 'mr',\n",
       " 'flea',\n",
       " 'take',\n",
       " 'stop',\n",
       " 'quit',\n",
       " 'him',\n",
       " 'is',\n",
       " 'so',\n",
       " 'maybe',\n",
       " 'cute',\n",
       " 'park']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "listPosts,listClasses=loadDataSet()\n",
    "myVocabList=createVocabList(listPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listPosts[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 运用词向量计算概率\n",
    "\n",
    "再看前文提到的朴素贝叶斯的原理，要计算词向量$ω=(ω_0,ω_1,ω_2,...ω_N,)$，落入$c_i$类别下的概率: \n",
    "\n",
    "$$p(c_i|\\omega)=\\frac{p(\\omega|c_i)p(c_i)}{p(\\omega)}$$\n",
    "\n",
    "$p(c_i)$好求，用样本集中，$c_i$的数量/样本总数即可\n",
    "\n",
    "$p(\\omega|c_i)$由各个条件特征互相独立且地位相同，$p(\\omega|c_i)=p(\\omega_0|c_i)p(\\omega_1|c_i)\\dots p(\\omega_N|c_i)$,可以分别求$p(\\omega_o|c_i),p(\\omega_1|c_i),p(\\omega_2|c_i),\\dots,p(\\omega_N|c_i)$，从而得到$p(\\omega|c_i)$\n",
    "\n",
    "而求$p(\\omega_k|c_i)$也就编程了求在分类类别为$c_i$的文档词汇表集合中，单个词$w_k$出现的概率，也就是\n",
    "$$p(w_k|c_i)=\\frac{w_k在c_i中出现的次数}{c_i中词总数}$$\n",
    "\n",
    "因此计算出现概率大致有这么一些流程： \n",
    "\n",
    "![20170509211027818.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/12-1.png?raw=true)\n",
    "\n",
    "## 用Python代码实现，创建函数TrainNB："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix) #文档数目\n",
    "    numWord=len(trainMatrix[0]) #词汇表词数目\n",
    "    pAbusive=sum(trainCategory)/len(trainCategory) #p1,出现侮辱性评论的概率\n",
    "    p0Num=np.zeros(numWord);p1Num=np.zeros(numWord)\n",
    "    p0Demon=0;p1Demon=0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==0:\n",
    "            p0Num+=trainMatrix[i] #向量相加\n",
    "            p0Demon+=sum(trainMatrix[i]) #向量中1累加求和\n",
    "        else:\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Demon+=sum(trainMatrix[i])\n",
    "    p0Vec=p0Num/p0Demon\n",
    "    p1Vec=p1Num/p1Demon\n",
    "    return p0Vec,p1Vec,pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解释： \n",
    "1. pAbusive=sum(trainCategory)/len(trainCategory)，表示文档集中分类为1的文档数目，累加求和将词向量中所有1相加，len求长度函数则对所有0和1进行计数，最后得到分类为1的概率 \n",
    "2. p0Num+=trainMatrix[i];p0Demon+=sum(trainMatrix[i])，前者是向量相加，其结果还是向量，trainMatrix[i]中是1的位置全部加到p0Num中，后者是先求和（该词向量中词项的数目），其结果是数值，表示词项总数。 \n",
    "3. p0Vec=p0Num/p0Demon，向量除以数值，结果是向量，向量中每个元素都除以该数值。\n",
    "\n",
    "### 测试：对构建的朴素贝叶斯分类器训练函数进行测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMat=[]\n",
    "for postinDoc in listPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "p0v,p1v,pAb=trainNB0(trainMat,listClasses)\n",
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.04166667, 0.        , 0.04166667, 0.        ,\n",
       "       0.04166667, 0.        , 0.125     , 0.04166667, 0.04166667,\n",
       "       0.        , 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.        , 0.        , 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.        , 0.08333333, 0.04166667, 0.04166667, 0.        ,\n",
       "       0.04166667, 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05263158, 0.        , 0.05263158, 0.        , 0.15789474,\n",
       "       0.        , 0.05263158, 0.        , 0.        , 0.        ,\n",
       "       0.10526316, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05263158, 0.05263158, 0.05263158, 0.        , 0.        ,\n",
       "       0.10526316, 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "       0.05263158, 0.05263158, 0.        , 0.        , 0.05263158,\n",
       "       0.        , 0.05263158])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#myVocabList中第2个词汇是\"Love\",即myVocabList[1]='Love'\n",
    "p0v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#myVocabList中第5个词汇是\"stupid\",即myVocabList[4]='stupid'\n",
    "p0v[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15789473684210525"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果我们看到，侮辱性文档出现的概率是0.5，词项’love’在侮辱性文档中出现的概率是0，在正常言论中出现的概率是0.042；词项‘stupid’在正常言论中出现的概率是0，在侮辱性言论中出现的规律是0.158.\n",
    "\n",
    "* 算法漏洞： \n",
    "   1. 乘积为0 \n",
    "   \n",
    "   我们看到，当某分类下某词项出现频次为0时，其概率也是0，因此在计算$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)......p(w_N|c_i)$会因为其中某个的概率为0而全部是0。\n",
    "   \n",
    "   为了避免这样的情况发生，我们将所有词项出现的频次都初始化为1，某类所有词项数量初始化为2。\n",
    "   \n",
    "   2. 因子太小导致结果溢出问题 \n",
    "   \n",
    "    由于$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)......p(w_N|c_i)$中每个因子都很小，所有因子相乘，特别是因子数量多的时候，会导致结果溢出，从而得到错误的数据 \n",
    "    \n",
    "    避免溢出问题的发生，可以使用求自然对数的方法，自然对数和原本的数值同增同减，不会有任何损失，因此不会影响求得的概率结果。\n",
    "\n",
    "因此，将朴素贝叶斯分类器函数修改为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB1(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix)\n",
    "    numWord=len(trainMatrix[0])\n",
    "    pAbusive=sum(trainCategory)/len(trainCategory)\n",
    "    p0Num=np.ones(numWord);p1Num=np.ones(numWord)# 初始化为1\n",
    "    p0Demon=2;p1Demon=2 #初始化为2\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==0:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Demon+=sum(trainMatrix[i])\n",
    "        else:\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Demon+=sum(trainMatrix[i])\n",
    "    p0Vec=np.log(p0Num/p0Demon) #对结果求对数\n",
    "    p1Vec=np.log(p1Num/p1Demon) #对结果求自然对数\n",
    "    return p0Vec,p1Vec,pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 运用分类器函数对文档进行分类\n",
    "\n",
    "前文概率论讲到，计算文档在各类中的概率，取较大者作为该文档的分类，所以构建分类函数classifyNB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1=sum(vec2Classify*p1Vec)+np.log(pClass1)\n",
    "    p0=sum(vec2Classify*p0Vec)+np.log(1-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明： \n",
    "p1=sum(vec2Classify*p1Vec)+log(pClass1) 的数学原理是ln(a*b)=ln(a) +ln(b)\n",
    "\n",
    "接下来构造几个样本，来测试分类函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listPosts,listClasses=loadDataSet()\n",
    "    myVocabList=createVocabList(listPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb=trainNB1(trainMat,listClasses)\n",
    "    testEntry=['love','my','dalmation']\n",
    "    thisDoc=setOfWords2Vec(myVocabList,testEntry)\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry=['stupid','garbage']\n",
    "    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
