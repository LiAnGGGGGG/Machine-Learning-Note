{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.神经网络的底层搭建\n",
    "\n",
    "这里，我们要实现一个拥有卷积层（CONV）和池化层（POOL）的网络，它包含了前向和反向传播，我们来定义一下符号：\n",
    "\n",
    "* 上标$[l]$是指第$l$层\n",
    "\n",
    "    * 比如：$a^{[4]}$是指第4层的激活值。$W^{[5]}$和$b^{[5]}$是指第5层的参数。\n",
    "    \n",
    "\n",
    "* 上标$（i）$是指来自第$i$个样本\n",
    "    * 比如：$x^{(i)}$是指来自输入的第$i$个样本\n",
    "    \n",
    "    \n",
    "* 下标$i$是指向量的第$i$项\n",
    "    * 比如：$a_i^{[l]}$是指来自第$l$层的第$i$个激活值，假设这是一个完全连接层（FC）\n",
    "    \n",
    "    \n",
    "* $n_H、n_W$与$n_c$是指分别表示给顶层的图像的高度、宽度和通道数，如果你想特指某一层，那么可以这样写：$n_H^{[l]}、n_W^{[l]}$与$n_c^{[l]}$\n",
    "\n",
    "\n",
    "* $n_{H_{prev}}、n_{W_{prev}}、n_{C_{prev}}$分别表示前一层的图像的高度、宽度和通道数，如果你想制定特定层$l$,那么你也可以这样写：\n",
    "$n_H^{[l-1]}、n_W^{[l-1]}、n_C^{[l-1]}$\n",
    "\n",
    "## 1.1 - 导入库\n",
    "\n",
    "我们先要导入一些库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "#ipython很好用，但是如果在ipython里已经import过的模块修改后需要重新reload就需要这样\n",
    "#在执行用户代码前，重新装入软件的扩展和模块。\n",
    "%load_ext autoreload   \n",
    "#autoreload 2：装入所有 %aimport 不包含的模块。\n",
    "%autoreload 2          \n",
    "\n",
    "np.random.seed(1)      #指定随机种子"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - 大纲\n",
    "\n",
    "我们将实现一个卷积神经网络的一些模块，下面我们将列举我们要实现的模块的函数功能：\n",
    "\n",
    "* 卷积模块，包含以下函数：\n",
    "    * 使用0扩充边界\n",
    "    * 卷积窗口\n",
    "    * 前向卷积\n",
    "    * 反向卷积（可选）\n",
    "    \n",
    "* 池化模块，包含以下函数：\n",
    "    * 前向池化\n",
    "    * 创建掩码\n",
    "    * 值分配\n",
    "    * 反向池化（可选）\n",
    "    \n",
    "我们将在这里从底层搭建一个完整的模块，之后我们会用TensorFlow实现，模型结构如下：\n",
    "![2018042521470222.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E7%BB%93%E6%9E%841.png?raw=true)\n",
    "\n",
    "需要注意的是我们在前向传播的过程中，我们会存储一些值，以便在反向传播的过程中计算梯度值。\n",
    "\n",
    "## 1.3 - 卷积神经网络\n",
    "\n",
    "尽管编程框架使卷积容易使用，但它们仍然是深度学习中最难理解的概念之一。卷积层将输入转换成不同纬度的输出，如下所示：\n",
    "![2018042521470222.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E7%BB%93%E6%9E%842.png?raw=true)\n",
    "\n",
    "我们将一步步构建卷积层，我们将首先实现两个辅助函数：一个用于零填充，另一个用于计算卷积。\n",
    "\n",
    "### 1.3.1 - 边界填充\n",
    "\n",
    "边界填充将会在图像边界周围添加值为0的像素点，如下图所示：\n",
    "![20180425213940990.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E8%BE%B9%E7%95%8C1.png?raw=true)\n",
    "\n",
    "使用0填充边界有以下好处：\n",
    "* 卷积了上一层之后的CONV层，没有缩小高度和宽度，这对建立更深的网络非常重要，否则在更深层时，高度/宽度会缩小。一个重要的例子是“same”卷积，自重高度/宽度在卷积完一层之后会被完全保留。\n",
    "\n",
    "\n",
    "* 它可以帮助我们在图像边界保留更多信息，在没有填充的情况下，卷积过程中图像边缘的极少数值会受到过滤器的影响从而导致信息丢失。\n",
    "\n",
    "\n",
    "我们将实现一个边界填充函数，它会把所有的样本图像$X$都使用0进行填充，我们可以使用np.pad来快速填充，需要注意的是如果你想使用pad = 1填充数组a.shape = （5,5,5,5,5）的第二维，使用pad = 3填充第四维，使用pad = 0来填充剩下的部分，我们可以这么做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(X,pad):\n",
    "    \"\"\"\n",
    "    把数据集X的图像边界全部使用0来扩充Pad个宽度和高度\n",
    "    \n",
    "    参数：\n",
    "    \n",
    "        X - 图像数据集，维度为（样本数，图像高度，图像宽度，图像通道数）\n",
    "        \n",
    "        pad - 整数，每个图像在垂直和水平维度上的填充量\n",
    "    \n",
    "    返回：\n",
    "        \n",
    "        X_paded - 扩充后的图像数据集，维度为（样本数，图像高度+2*pad，图像宽度 + 2*pad，通道数）\n",
    "    \"\"\"\n",
    "    X_paded  = np.pad(X,(\n",
    "                        (0,0),       #样本数，不填充\n",
    "                        (pad,pad),   #图像高度,你可以视为上面填充x个，下面填充y个(x,y)\n",
    "                        (pad,pad),   #图像宽度,你可以视为左边填充x个，右边填充y个(x,y)\n",
    "                        (0,0)),      #通道数，不填充\n",
    "                        'constant', constant_values=0)      #连续一样的值填充\n",
    "\n",
    "    return X_paded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_paded.shape = (4, 7, 7, 2)\n",
      "x[1, 1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_paded[1, 1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xf2c764bd30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAACuCAYAAABUfpQYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVkKJ0gzsxROkGZmKZwgzcxS/D8W6IgOAYS6DgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf2cb8b50b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4,3,3,2)\n",
    "x_paded = zero_pad(x,2)\n",
    "#查看信息\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_paded.shape =\", x_paded.shape)\n",
    "print (\"x[1, 1] =\", x[1, 1])\n",
    "print (\"x_paded[1, 1] =\", x_paded[1, 1])\n",
    "\n",
    "#绘制图\n",
    "fig , axarr = plt.subplots(1,2)  #一行两列\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_paded')\n",
    "axarr[1].imshow(x_paded[0,:,:,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 - 单步卷积\n",
    "\n",
    "在这里，我们要实现第一步卷积，我们要使用一个过滤器来卷积输入的数据。先来看看下面这个gif :\n",
    "![20180425214517222.gif](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E5%8D%B7%E7%A7%AF1.gif?raw=true)\n",
    "\n",
    "在计算机视觉应用中，左侧矩阵中的每个值都对应一个像素值，我们通过将其值与原始矩阵元素相乘，然后对它们进行求和来将3x3滤波器与图像进行卷积，我们需要实现一个函数，可以将一个3x3滤波器与单独的切片快进行卷积并输出一个实数。现在我们开始实现conv_single_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev,W,b):\n",
    "    \"\"\"\n",
    "    在前一层的激活输出的一个片段上对应一个有参数W定义的过滤器。\n",
    "    这里切片大小和过滤器大小相同\n",
    "    \n",
    "    参数：\n",
    "        a_slice_prev - 输入数据的一个片段，维度为（过滤器大小，过滤器大小，上一通道数）\n",
    "        w - 权重参数，包含在一个矩阵中，维度为（过滤器大小，过滤器大小，上一通道数）\n",
    "        b - 偏置参数，包含在了一个矩阵中，维度为（1,1,1）\n",
    "    \n",
    "    返回：\n",
    "        Z - 在输入数据的片X上卷积滑动窗口（W，b）的结果。\n",
    "    \"\"\"\n",
    "    s = np.multiply(a_slice_prev,W) + b\n",
    "\n",
    "    Z = np.sum(s)\n",
    "\n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "#这里切片大小和过滤器大小相同\n",
    "a_slice_prev = np.random.randn(4,4,3)\n",
    "W = np.random.randn(4,4,3)\n",
    "b = np.random.randn(1,1,1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev,W,b)\n",
    "\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 - 卷积神经网络 - 前向传播\n",
    "\n",
    "在前向传播的过程中，我们将使用多种过滤器对输入的数据进行卷积操作，每个过滤器会产生一个2D的矩阵，我们可以把它们堆叠起来，于是这些2D的卷积矩阵就变成了高维的矩阵。\n",
    "\n",
    "我们需要实现一个函数以实现对激活值进行卷积，我们需要在激活值矩阵$A_{prev}$上使用过滤器$W$进行卷积，该函数的输入时前一层的激活输出$A_{prev}$,$F$个过滤器，其权重矩阵为$W$、偏置矩阵为$b$,每个过滤器只有一个偏置，最后，我们需要一个包含了步长$s$和填充$p$的字典的超参数。\n",
    "\n",
    "小提示：\n",
    "\n",
    "* 如果我要在矩阵A_prev(shape = (5,5,3))的左上角选择一个2x2的矩阵进行切片操作，那么可以这样做： \n",
    "a_slice_prev = a_prev[0:2,0:2,:] \n",
    "\n",
    "* 如果我想要自定义切片，我们可以这么做：先定义要切片的位置，vert_start、vert_end、horiz_start、horiz_end，它们的位置我们看一下下面的图就明白了。\n",
    "![20180425214940264.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E5%89%8D%E5%90%911.png?raw=true)\n",
    "\n",
    "我们还是说一下输出维度的计算公式吧：\n",
    "\n",
    "$$n_H = \\left \\lfloor \\frac{n_{Hprev}-f+2\\times pad}{stride} \\right \\rfloor+1$$\n",
    "\n",
    "$$n_W = \\left \\lfloor \\frac{n_{Wprev}-f+2\\times pad}{stride} \\right \\rfloor+1$$\n",
    "\n",
    "$$n_C=过滤器数量$$\n",
    "\n",
    "这里我们不会使用矢量化，知识用for循环来实现所有的东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    实现卷积函数的前向传播\n",
    "\n",
    "    参数：\n",
    "        A_prev - 上一层的激活输出矩阵，维度为(m, n_H_prev, n_W_prev, n_C_prev)，（样本数量，上一层图像的高度，上一层图像的宽度，上一层过滤器数量）\n",
    "        W - 权重矩阵，维度为(f, f, n_C_prev, n_C)，（过滤器大小，过滤器大小，上一层的过滤器数量，这一层的过滤器数量）\n",
    "        b - 偏置矩阵，维度为(1, 1, 1, n_C)，（1,1,1,这一层的过滤器数量）\n",
    "        hparameters - 包含了\"stride\"与 \"pad\"的超参数字典。\n",
    "\n",
    "    返回：\n",
    "        Z - 卷积输出，维度为(m, n_H, n_W, n_C)，（样本数，图像的高度，图像的宽度，过滤器数量）\n",
    "        cache - 缓存了一些反向传播函数conv_backward()需要的一些数据\n",
    "    \"\"\"\n",
    "\n",
    "    #获取来自上一层数据的基本信息\n",
    "    (m , n_H_prev , n_W_prev , n_C_prev) = A_prev.shape\n",
    "\n",
    "    #获取权重矩阵的基本信息\n",
    "    ( f , f ,n_C_prev , n_C ) = W.shape\n",
    "\n",
    "    #获取超参数hparameters的值\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "\n",
    "    #计算卷积后的图像的宽度高度，参考上面的公式，使用int()来进行板除\n",
    "    n_H = int(( n_H_prev - f + 2 * pad )/ stride) + 1\n",
    "    n_W = int(( n_W_prev - f + 2 * pad )/ stride) + 1\n",
    "\n",
    "    #使用0来初始化卷积输出Z\n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "\n",
    "    #通过A_prev创建填充过了的A_prev_pad\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "\n",
    "    for i in range(m):                              #遍历样本\n",
    "        a_prev_pad = A_prev_pad[i]                  #选择第i个样本的扩充后的激活矩阵\n",
    "        for h in range(n_H):                        #在输出的垂直轴上循环\n",
    "            for w in range(n_W):                    #在输出的水平轴上循环\n",
    "                for c in range(n_C):                #循环遍历输出的通道\n",
    "                    #定位当前的切片位置\n",
    "                    vert_start = h * stride         #竖向，开始的位置\n",
    "                    vert_end = vert_start + f       #竖向，结束的位置\n",
    "                    horiz_start = w * stride        #横向，开始的位置\n",
    "                    horiz_end = horiz_start + f     #横向，结束的位置\n",
    "                    #切片位置定位好了我们就把它取出来,需要注意的是我们是“穿透”取出来的，\n",
    "                    #自行脑补一下吸管插入一层层的橡皮泥就明白了\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    #执行单步卷积\n",
    "                    Z[i,h,w,c] = conv_single_step(a_slice_prev,W[: ,: ,: ,c],b[0,0,0,c])\n",
    "\n",
    "    #数据处理完毕，验证数据格式是否正确\n",
    "    assert(Z.shape == (m , n_H , n_W , n_C ))\n",
    "\n",
    "    #存储一些缓存值，以便于反向传播使用\n",
    "    cache = (A_prev,W,b,hparameters)\n",
    "\n",
    "    return (Z , cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mean(Z) =  0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "\n",
    "hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "\n",
    "print(\"np.mean(Z) = \", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - 池化层\n",
    "\n",
    "池化层会减少输入的宽度和高度，这样它会较少计算量的同时也使特征检测器对其输入中的位置更加稳定\n",
    "\n",
    "* 最大值池化层：在输入矩阵中滑动一个大小为fx的窗口，选取窗口里的值中的最大值，然后作为输出的一部分。\n",
    "\n",
    "\n",
    "* 均值池化层：在输入矩阵中滑动一个大小为fx的窗口，计算窗口里的值中的平均值，然后这个均值作为输出的一部分\n",
    "![20180425215218806.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E6%B1%A0%E5%8C%961.png?raw=true)\n",
    "\n",
    "池化层没有用于反向传播的参数，但是它们有像窗口的大小为f的超参数，它制定fxf窗口的高度和宽度，我们可以计算出最大值或平均值。\n",
    "\n",
    "### 1.4.1 - 池化层的前向传播\n",
    "\n",
    "现在我们要在同一个函数中实现最大值池化层和均值池化层，和之前计算输出维度一样，池化层的计算也是一样的。\n",
    "$$n_H = \\left \\lfloor \\frac{n_{Hprev}-f}{stride} \\right \\rfloor+1$$\n",
    "\n",
    "$$n_W = \\left \\lfloor \\frac{n_{Wprev}-f}{stride} \\right \\rfloor+1$$\n",
    "\n",
    "$$n_C=n_{Cprev}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev,hparameters,mode=\"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的前向传播\n",
    "\n",
    "    参数：\n",
    "        A_prev - 输入数据，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        hparameters - 包含了 \"f\" 和 \"stride\"的超参数字典\n",
    "        mode - 模式选择【\"max\" | \"average\"】\n",
    "\n",
    "    返回：\n",
    "        A - 池化层的输出，维度为 (m, n_H, n_W, n_C)\n",
    "        cache - 存储了一些反向传播需要用到的值，包含了输入和超参数的字典。\n",
    "    \"\"\"\n",
    "\n",
    "    #获取输入数据的基本信息\n",
    "    (m , n_H_prev , n_W_prev , n_C_prev) = A_prev.shape\n",
    "\n",
    "    #获取超参数的信息\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    #计算输出维度\n",
    "    n_H = int((n_H_prev - f) / stride ) + 1\n",
    "    n_W = int((n_W_prev - f) / stride ) + 1\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    #初始化输出矩阵\n",
    "    A = np.zeros((m , n_H , n_W , n_C))\n",
    "\n",
    "    for i in range(m):                              #遍历样本\n",
    "        for h in range(n_H):                        #在输出的垂直轴上循环\n",
    "            for w in range(n_W):                    #在输出的水平轴上循环\n",
    "                for c in range(n_C):                #循环遍历输出的通道\n",
    "                    #定位当前的切片位置\n",
    "                    vert_start = h * stride         #竖向，开始的位置\n",
    "                    vert_end = vert_start + f       #竖向，结束的位置\n",
    "                    horiz_start = w * stride        #横向，开始的位置\n",
    "                    horiz_end = horiz_start + f     #横向，结束的位置\n",
    "                    #定位完毕，开始切割\n",
    "                    a_slice_prev = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "\n",
    "                    #对切片进行池化操作\n",
    "                    if mode == \"max\":\n",
    "                        A[ i , h , w , c ] = np.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        A[ i , h , w , c ] = np.mean(a_slice_prev)\n",
    "\n",
    "    #池化完毕，校验数据格式\n",
    "    assert(A.shape == (m , n_H , n_W , n_C))\n",
    "\n",
    "    #校验完毕，开始存储用于反向传播的值\n",
    "    cache = (A_prev,hparameters)\n",
    "\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.51981682 2.18557541]]]]\n",
      "----------------------------\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2,4,4,3)\n",
    "hparameters = {\"f\":4 , \"stride\":1}\n",
    "\n",
    "A , cache = pool_forward(A_prev,hparameters,mode=\"max\")\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print(\"----------------------------\")\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - 卷积神经网络中的反向传播（选学）\n",
    "\n",
    "在现在的深度学习框架中，你只需要实现前向传播，框架负责后向传播，所以大多数深度学习工程师不需要费心处理后向传播的细节，卷积网络的后向传播是有点复杂的，但是如果你愿意，你可以选择性来学习本节\n",
    "\n",
    "在前面的课程中，我们已经实现了一个简单的（全连接）神经网络，我们使用反向传播来计算关于更新参数的成本梯度，类似的，在卷积神经网络中我们可以计算出关于成本的导数来更新参数，反向传播的方程并不简单，吴恩达老师并没有在课堂上推导它们，单我们可以在下面简要介绍。\n",
    "\n",
    "### 1.5.1 - 卷积层的反向传播\n",
    "\n",
    "我们来看一下如何实现卷积层的反向传播\n",
    "\n",
    "### 1.5.1.1 - 计算dA\n",
    "\n",
    "下面的公式是计算$dA$的：\n",
    "$$dA+=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}W_c\\times dZ_{hw}$$\n",
    "\n",
    "其中，$W_c$是过滤器，$Z_{hw}$是一个标量，$Z_hw$是卷积层第$h$行第$w$列的使用点乘计算后的输出$Z$的梯度。需要注意的是在每次更新$dA$的时候，都会用相同的过滤器$W_c$乘以不同的$dZ$,因为在前向传播的时候，每个过滤器都与a_slice进行了点乘相加，所以在计算$dA$的时候，我们需要把a_slice的梯度也加进来，我们可以在循坏中加一句代码：\n",
    "```\n",
    "da_perv_pad[vert_start:vert_end,horiz_start:horiz_end,:] += W[:,:,:,c] * dZ[i,h,w,c]\n",
    "```\n",
    "### 1.5.1.2 - 计算dW\n",
    "\n",
    "这是计算$dW_c$的公式（$dW_c$是一个过滤器的梯度）：\n",
    "$$dW_c+=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}a_{slice}\\times dZ_{hw}$$\n",
    "\n",
    "其中$a_{slice}$对应着$Z_{ij}$的激活值。由此，我们就可以推导$W$的梯度，因为我们使用了过滤器来对数据进行窗口滑动，在这里，我们实际上是切出了和过滤器一样大小的切片，切了多少次就产生了多少个梯度，所以我们需要把它们加起来得到这个数据集的整体$dW$.\n",
    "\n",
    "在代码上我们只需要使用一行代码实现：\n",
    "```\n",
    "dW[:,:,:, c] += a_slice * dZ[i , h , w , c]\n",
    "```\n",
    "\n",
    "### 1.5.1.3 - 计算db\n",
    "\n",
    "这个是计算$db$的公式：\n",
    "$$db=\\sum_h\\sum_w dZ_{hw}$$\n",
    "\n",
    "和以前的神经网络一样，$db$是由$dZ$累加计算的，在这里，我们只需要将conv的输出Z的所有梯度累加就好了。在代码上我们只需要使用一行代码实现：\n",
    "```\n",
    "db[:,:,:,c] += dZ[ i, h, w, c]\n",
    "```\n",
    "\n",
    "### 1.5.1.4 - 函数实现\n",
    "\n",
    "现在我们将实现反向传播函数conv_backward(),我们需要把所有的训练样本的过滤器、权值、高度、宽度都要加进来，然后使用公式1、2、3计算对应的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "    实现卷积层的反向传播\n",
    "\n",
    "    参数：\n",
    "        dZ - 卷积层的输出Z的 梯度，维度为(m, n_H, n_W, n_C)\n",
    "        cache - 反向传播所需要的参数，conv_forward()的输出之一\n",
    "\n",
    "    返回：\n",
    "        dA_prev - 卷积层的输入（A_prev）的梯度值，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW - 卷积层的权值的梯度，维度为(f,f,n_C_prev,n_C)\n",
    "        db - 卷积层的偏置的梯度，维度为（1,1,1,n_C）\n",
    "\n",
    "    \"\"\"\n",
    "    #获取cache的值\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "\n",
    "    #获取A_prev的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    #获取dZ的基本信息\n",
    "    (m,n_H,n_W,n_C) = dZ.shape\n",
    "\n",
    "    #获取权值的基本信息\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    #获取hparaeters的值\n",
    "    pad = hparameters[\"pad\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    #初始化各个梯度的结构\n",
    "    dA_prev = np.zeros((m,n_H_prev,n_W_prev,n_C_prev))\n",
    "    dW = np.zeros((f,f,n_C_prev,n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "\n",
    "    #前向传播中我们使用了pad，反向传播也需要使用，这是为了保证数据结构一致\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev,pad)\n",
    "\n",
    "    #现在处理数据\n",
    "    for i in range(m):\n",
    "        #选择第i个扩充了的数据的样本,降了一维。\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    #定位切片位置\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    #定位完毕，开始切片\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    #切片完毕，使用上面的公式计算梯度\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i,h,w,c]\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c]\n",
    "        #设置第i个样本最终的dA_prev,即把非填充的数据取出来。\n",
    "        dA_prev[i,:,:,:] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "    #数据处理完毕，验证数据格式是否正确\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    return (dA_prev,dW,db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547566\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#初始化参数\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "#前向传播\n",
    "Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "#反向传播\n",
    "dA , dW , db = conv_backward(Z,cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 - 池化层的反向传播\n",
    "\n",
    "接下来，我们从最大值池化层开始实现池化层的反向传播，即使池化层没有反向传播过程中要更新的参数，我们仍然要通过池化层反向传播梯度，以便在池化层之前的层（比如卷积层）计算梯度\n",
    "\n",
    "### 1.5.2.1 - 最大值池化层的反向传播\n",
    "\n",
    "在开始池化层的反向传播之前，我们需要创建一个create_mask_from_window()的函数，我们来看一下它是干什么的：\n",
    "\n",
    "$$X=\\begin{bmatrix}\n",
    "1 &3 \\\\ \n",
    "4 & 2\n",
    "\\end{bmatrix}\\rightarrow M=\\begin{bmatrix}\n",
    "0&0 \\\\ \n",
    "1 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "正如你所看到的，这个函数创建了一个掩码矩阵，以保存最大值的位置，当为1的时候表示最大值的位置，其他的为0，这个是最大值池化层，均值池化层的向后传播也和这个差不多，但是使用的是不同的掩码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    从输入矩阵中创建掩码，以保存最大值的矩阵的位置。\n",
    "\n",
    "    参数：\n",
    "        x - 一个维度为(f,f)的矩阵\n",
    "\n",
    "    返回：\n",
    "        mask - 包含x的最大值的位置的矩阵\n",
    "    \"\"\"\n",
    "    mask = x == np.max(x)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask = [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x = np.random.randn(2,3)\n",
    "\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "print(\"x = \" + str(x)) \n",
    "print(\"mask = \" + str(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们要创建这一个掩码矩阵呢？想一下我们的正向传播首先是经过卷积层，然后滑动地取卷积层最大值构成了池化层，如果我们不记录最大值的位置，那么我们怎样才能反向传播到卷积层呢？\n",
    "\n",
    "\n",
    "\n",
    "### 1.5.2.2 均值池化层的反向传播\n",
    "\n",
    "  在最大值池化层中，对于每个输入窗口，输出的所有值都来自输入中的最大值，但是在均值池化层中，因为是计算均值，所以输入窗口的每个元素对输出有一样的影响，我们来看看如何反向传播吧\n",
    "  \n",
    "$$dZ = 1\\rightarrow dZ = \\begin{bmatrix}\n",
    "\\frac{1}{4}  &\\frac{1}{4} \\\\ \n",
    " \\frac{1}{4}&\\frac{1}{4} \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz,shape):\n",
    "    \"\"\"\n",
    "    给定一个值，为按矩阵大小平均分配到每一个矩阵位置中。\n",
    "\n",
    "    参数：\n",
    "        dz - 输入的实数\n",
    "        shape - 元组，两个值，分别为n_H , n_W\n",
    "\n",
    "    返回：\n",
    "        a - 已经分配好了值的矩阵，里面的值全部一样。\n",
    "\n",
    "    \"\"\"\n",
    "    #获取矩阵的大小\n",
    "    (n_H , n_W) = shape\n",
    "\n",
    "    #计算平均值\n",
    "    average = dz / (n_H * n_W)\n",
    "\n",
    "    #填充入矩阵\n",
    "    a = np.ones(shape) * average\n",
    "\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "dz = 2\n",
    "shape = (2,2)\n",
    "\n",
    "a = distribute_value(dz,shape)\n",
    "print(\"a = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2.3 - 池化层的反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA,cache,mode = \"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的反向传播\n",
    "\n",
    "    参数:\n",
    "        dA - 池化层的输出的梯度，和池化层的输出的维度一样\n",
    "        cache - 池化层前向传播时所存储的参数。\n",
    "        mode - 模式选择，【\"max\" | \"average\"】\n",
    "\n",
    "    返回：\n",
    "        dA_prev - 池化层的输入的梯度，和A_prev的维度相同\n",
    "\n",
    "    \"\"\"\n",
    "    #获取cache中的值\n",
    "    (A_prev , hparaeters) = cache\n",
    "\n",
    "    #获取hparaeters的值\n",
    "    f = hparaeters[\"f\"]\n",
    "    stride = hparaeters[\"stride\"]\n",
    "\n",
    "    #获取A_prev和dA的基本信息\n",
    "    (m , n_H_prev , n_W_prev , n_C_prev) = A_prev.shape\n",
    "    (m , n_H , n_W , n_C) = dA.shape\n",
    "\n",
    "    #初始化输出的结构\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "\n",
    "    #开始处理数据\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]      \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    #定位切片位置\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    #选择反向传播的计算方式\n",
    "                    if mode == \"max\":\n",
    "                        #开始切片\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        #创建掩码\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        #计算dA_prev\n",
    "                        dA_prev[i,vert_start:vert_end,horiz_start:horiz_end,c] += np.multiply(mask,dA[i,h,w,c])\n",
    "\n",
    "                    elif mode == \"average\":\n",
    "                        #获取dA的值\n",
    "                        da = dA[i,h,w,c]\n",
    "                        #定义过滤器大小\n",
    "                        shape = (f,f)\n",
    "                        #平均分配\n",
    "                        dA_prev[i,vert_start:vert_end, horiz_start:horiz_end ,c] += distribute_value(da,shape)\n",
    "    #数据处理完毕，开始验证格式\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 神经网络的应用\n",
    "\n",
    "我们应急使用了原生代码实现了卷积神经网络，现在我们要使用TensorFlow来实现，然后应用到手势识别中，在这里我们要实现4个函数，一起来看看吧\n",
    "\n",
    "## 2.1.0 TensorFlow 模型\n",
    "\n",
    "我们先来导入库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import tf_utils\n",
    "import time\n",
    "import math\n",
    "import cnn_utils\n",
    "#%matplotlib inline #如果你使用的是jupyter notebook取消注释\n",
    "np.random.seed(1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用的是【中文】【吴恩达课后编程作业】Course 2 - 改善深层神经网络 - 第三周作业 - TensorFlow入门的数据集 \n",
    "![20180425220341697.png](https://github.com/LiAnGGGGGG/Machine-Learning-Note/blob/master/%E5%9B%BE%E7%89%87/8-%E9%97%AE%E9%A2%981.png?raw=true)\n",
    "\n",
    "我们再来看一下里面有什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf2cbb307b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = 0\n"
     ]
    }
   ],
   "source": [
    "X_train_orig , Y_train_orig , X_test_orig , Y_test_orig , classes = tf_utils.load_dataset()\n",
    "index =20\n",
    "\n",
    "plt.imshow(X_train_orig[index])\n",
    "plt.show()\n",
    "print(\"Y = \" + str(np.squeeze(Y_train_orig[:,index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对TensorFlow的练习中，我们已经建立了一个神经网络，也是使用了这个数据集，我们再来看一下数据的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = cnn_utils.convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = cnn_utils.convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1 -  创建placeholders\n",
    "\n",
    "TensorFlow要求为运行会话时将输入到模型中的输入数据创建占位符，现在我们要实现创建占位符的函数，因为我们使用的是小批量数据块，输入的样本数量可能不固定，所以我们在数量那里我们要使用None作为可变变量，输入X的维度为【None,n_H0,n_W0,n_C0】,对应的Y是【None,n_y】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    为session创建占位符\n",
    "\n",
    "    参数：\n",
    "        n_H0 - 实数，输入图像的高度\n",
    "        n_W0 - 实数，输入图像的宽度\n",
    "        n_C0 - 实数，输入的通道数\n",
    "        n_y  - 实数，分类数\n",
    "\n",
    "    输出：\n",
    "        X - 输入数据的占位符，维度为[None, n_H0, n_W0, n_C0]，类型为\"float\"\n",
    "        Y - 输入数据的标签的占位符，维度为[None, n_y]，维度为\"float\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32,[None, n_H0, n_W0, n_C0])\n",
    "    Y = tf.placeholder(tf.float32,[None, n_y])\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X , Y = create_placeholders(64,64,3,6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2 -  初始化参数\n",
    "\n",
    "现在我们将使用tf.contrib.layers.xavier_initializer(seed = 0)来初始化权值/过滤器$W1、W2$。在这里，我们不需要考虑偏置，因为TensorFlow会考虑到的，需要注意的是我们只需要初始化为2D卷积函数，全连接层TensorFlow会自动初始化的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    初始化权值矩阵，这里我们把权值矩阵硬编码：\n",
    "    W1 : [4, 4, 3, 8]\n",
    "    W2 : [2, 2, 8, 16]\n",
    "\n",
    "    返回：\n",
    "        包含了tensor类型的W1、W2的字典\n",
    "    \"\"\"\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\",[4,4,3,8],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\",[2,2,8,16],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [ 0.00131723  0.1417614  -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))\n",
    "\n",
    "    sess_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2 - 前向传播\n",
    "\n",
    "在TensorFlow里面有一些可以直接拿来用的函数：\n",
    "\n",
    "* tf.nn.conv2d(X,W1,strides=[1,s,s,1],padding='SAME'):给定输入$X$和一组过滤器$W1$,这个函数将会自动使用$W1$来对$X$进行卷积，第三个输入参数是[1,s,s,1]是指对于输入 (m, n_H_prev, n_W_prev, n_C_prev)而言，每次滑动的步伐。\n",
    "\n",
    "\n",
    "* tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')：给定输入$X$,该函数将会使用大小为（f,f）以及步伐为(s,s)的窗口对其进行滑动取最大值\n",
    "\n",
    "* tf.nn.relu(Z1)：计算Z1的ReLU激活\n",
    "\n",
    "\n",
    "* tf.contrib.layers.flatten(P)：给定一个输入P，此函数将会把每个样本转化成一维的向量，然后返回一个tensor变量，其维度为（batch_size,k）.\n",
    "\n",
    "\n",
    "* tf.contrib.layers.fully_connected(F, num_outputs)：给定一个已经一维化了的输入F，此函数将会返回一个由全连接层计算过后的输出。\n",
    "\n",
    "\n",
    "使用tf.contrib.layers.fully_connected(F, num_outputs)的时候，全连接层会自动初始化权值且在你训练模型的时候它也会一直参与，所以当我们初始化参数的时候我们不需要专门去初始化它的权值。\n",
    "\n",
    "我们实现前向传播的时候，我们需要定义一下我们模型的大概样子：\n",
    "\n",
    "$$CONV2D\\rightarrow RELU\\rightarrow MAXPOOL\\rightarrow CONV2D\\rightarrow RELU\\rightarrow MAXPOOL\\rightarrow FULLCONNECTED$$\n",
    "\n",
    "具体我们事先的时候，我们需要使用一下步骤和参数：\n",
    "\n",
    "* Conv2d : 步伐：1，填充方式：“SAME”\n",
    "\n",
    "\n",
    "* ReLU \n",
    "\n",
    "\n",
    "* Max pool : 过滤器大小：8x8，步伐：8x8，填充方式：“SAME”\n",
    "\n",
    "\n",
    "* Conv2d : 步伐：1，填充方式：“SAME”\n",
    "\n",
    "\n",
    "* ReLU \n",
    "\n",
    "\n",
    "* Max pool : 过滤器大小：4x4，步伐：4x4，填充方式：“SAME”\n",
    "\n",
    "\n",
    "* 一维化上一层的输出\n",
    "\n",
    "\n",
    "* 全连接层（FC）：使用没有非线性激活函数的全连接层。这里不要调用SoftMax， 这将导致输出层中有6个神经元，然后再传递到softmax。 在TensorFlow中，softmax和cost函数被集中到一个函数中，在计算成本时您将调用不同的函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    \"\"\"\n",
    "    实现前向传播\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "    参数：\n",
    "        X - 输入数据的placeholder，维度为(输入节点数量，样本数量)\n",
    "        parameters - 包含了“W1”和“W2”的python字典。\n",
    "\n",
    "    返回：\n",
    "        Z3 - 最后一个LINEAR节点的输出\n",
    "\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "\n",
    "    #Conv2d : 步伐：1，填充方式：“SAME”\n",
    "    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    #ReLU ：\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    #Max pool : 窗口大小：8x8，步伐：8x8，填充方式：“SAME”\n",
    "    P1 = tf.nn.max_pool(A1,ksize=[1,8,8,1],strides=[1,8,8,1],padding=\"SAME\")\n",
    "\n",
    "    #Conv2d : 步伐：1，填充方式：“SAME”\n",
    "    Z2 = tf.nn.conv2d(P1,W2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    #ReLU ：\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    #Max pool : 过滤器大小：4x4，步伐：4x4，填充方式：“SAME”\n",
    "    P2 = tf.nn.max_pool(A2,ksize=[1,4,4,1],strides=[1,4,4,1],padding=\"SAME\")\n",
    "\n",
    "    #一维化上一层的输出\n",
    "    P = tf.contrib.layers.flatten(P2)\n",
    "\n",
    "    #全连接层（FC）：使用没有非线性激活函数的全连接层\n",
    "    Z3 = tf.contrib.layers.fully_connected(P,6,activation_fn=None)\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = [[ 1.4416984  -0.24909666  5.450499   -0.2618962  -0.20669907  1.3654671 ]\n",
      " [ 1.4070846  -0.02573211  5.08928    -0.48669922 -0.40940708  1.2624859 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(1)\n",
    "\n",
    "with tf.Session() as sess_test:\n",
    "    X,Y = create_placeholders(64,64,3,6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "\n",
    "    a = sess_test.run(Z3,{X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "    print(\"Z3 = \" + str(a))\n",
    "\n",
    "    sess_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.3 计算成本\n",
    "\n",
    "我们要在这里实现计算成本的函数，下面的两个函数是我们要用到的：\n",
    "\n",
    "\n",
    "* tf.nn.softmax_cross_entropy_with_logits(logits = Z3 , lables = Y)：计算softmax的损失函数。这个函数既计算softmax的激活，也计算其损失，你可以阅读手册\n",
    "\n",
    "\n",
    "* tf.reduce_mean：计算的是平均值，使用它来计算所有样本的损失来得到总成本。你可以阅读手册\n",
    "\n",
    "\n",
    "现在，我们就来实现计算成本的函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3,Y):\n",
    "    \"\"\"\n",
    "    计算成本\n",
    "    参数：\n",
    "        Z3 - 正向传播最后一个LINEAR节点的输出，维度为（6，样本数）。\n",
    "        Y - 标签向量的placeholder，和Z3的维度相同\n",
    "\n",
    "    返回：\n",
    "        cost - 计算后的成本\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 4.6648693\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess_test:\n",
    "    np.random.seed(1)\n",
    "    X,Y = create_placeholders(64,64,3,6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    a = sess_test.run(cost,{X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))\n",
    "\n",
    "    sess_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.4 构建模型\n",
    "\n",
    "最后，我们已经实现了我们所有的函数，我们现在就可以实现我们的模型了。\n",
    "\n",
    "我们之前在课程2就实现过random_mini_batches()这个函数，它返回的是一个mini-batches的列表。\n",
    "\n",
    "在实现这个模型的时候我们要经历以下步骤：\n",
    "\n",
    "\n",
    "* 创建占位符\n",
    "\n",
    "\n",
    "* 初始化参数\n",
    "\n",
    "\n",
    "* 前向传播\n",
    "\n",
    "\n",
    "* 计算成本\n",
    "\n",
    "\n",
    "* 反向传播\n",
    "\n",
    "\n",
    "* 创建优化器\n",
    "\n",
    "\n",
    "最后，我们将创建一个session来运行模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.009, \n",
    "         num_epochs=100,minibatch_size=64,print_cost=True,isPlot=True):\n",
    "    \"\"\"\n",
    "    使用TensorFlow实现三层的卷积神经网络\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "    参数：\n",
    "        X_train - 训练数据，维度为(None, 64, 64, 3)\n",
    "        Y_train - 训练数据对应的标签，维度为(None, n_y = 6)\n",
    "        X_test - 测试数据，维度为(None, 64, 64, 3)\n",
    "        Y_test - 训练数据对应的标签，维度为(None, n_y = 6)\n",
    "        learning_rate - 学习率\n",
    "        num_epochs - 遍历整个数据集的次数\n",
    "        minibatch_size - 每个小批量数据块的大小\n",
    "        print_cost - 是否打印成本值，每遍历100次整个数据集打印一次\n",
    "        isPlot - 是否绘制图谱\n",
    "\n",
    "    返回：\n",
    "        train_accuracy - 实数，训练集的准确度\n",
    "        test_accuracy - 实数，测试集的准确度\n",
    "        parameters - 学习后的参数\n",
    "    \"\"\"\n",
    "    ops.reset_default_graph()  #能够重新运行模型而不覆盖tf变量\n",
    "    tf.set_random_seed(1)    #确保你的数据和我一样\n",
    "    seed = 3                 #指定numpy的随机种子\n",
    "    (m , n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []\n",
    "\n",
    "    #为当前维度创建占位符\n",
    "    X , Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    #初始化参数\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    #前向传播\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "\n",
    "    #计算成本\n",
    "    cost = compute_cost(Z3,Y)\n",
    "\n",
    "    #反向传播，由于框架已经实现了反向传播，我们只需要选择一个优化器就行了\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    #全局初始化所有变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #开始运行\n",
    "    with tf.Session() as sess:\n",
    "        #初始化参数\n",
    "        sess.run(init)\n",
    "        #开始遍历数据集\n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(m / minibatch_size) #获取数据块的数量\n",
    "            seed = seed + 1\n",
    "            minibatches = cnn_utils.random_mini_batches(X_train,Y_train,minibatch_size,seed) \n",
    "\n",
    "            #对每个数据块进行处理\n",
    "            for minibatch in minibatches:\n",
    "                #选择一个数据块\n",
    "                (minibatch_X,minibatch_Y) = minibatch\n",
    "                #最小化这个数据块的成本\n",
    "                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "\n",
    "                #累加数据块的成本值\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "\n",
    "            #是否打印成本\n",
    "            if print_cost:\n",
    "                #每5代打印一次\n",
    "                if epoch % 5 == 0:\n",
    "                    print(\"当前是第 \" + str(epoch) + \" 代，成本值为：\" + str(minibatch_cost))\n",
    "\n",
    "            #记录成本\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "\n",
    "        #数据处理完毕，绘制成本曲线\n",
    "        if isPlot:\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "            plt.show()\n",
    "\n",
    "        #开始预测数据\n",
    "        ## 计算当前的预测情况\n",
    "        predict_op = tf.arg_max(Z3,1)\n",
    "        corrent_prediction = tf.equal(predict_op , tf.arg_max(Y,1))\n",
    "\n",
    "        ##计算准确度\n",
    "        accuracy = tf.reduce_mean(tf.cast(corrent_prediction,\"float\"))\n",
    "        print(\"corrent_prediction accuracy= \" + str(accuracy))\n",
    "\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuary = accuracy.eval({X: X_test, Y: Y_test})\n",
    "\n",
    "        print(\"训练集准确度：\" + str(train_accuracy))\n",
    "        print(\"测试集准确度：\" + str(test_accuary))\n",
    "\n",
    "        return (train_accuracy,test_accuary,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前是第 0 代，成本值为：1.9213323593139648\n",
      "当前是第 5 代，成本值为：1.9041557535529137\n",
      "当前是第 10 代，成本值为：1.904308833181858\n",
      "当前是第 15 代，成本值为：1.904477171599865\n",
      "当前是第 20 代，成本值为：1.901875652372837\n",
      "当前是第 25 代，成本值为：1.7840776294469833\n",
      "当前是第 30 代，成本值为：1.6810505613684654\n",
      "当前是第 35 代，成本值为：1.618206426501274\n",
      "当前是第 40 代，成本值为：1.5979710295796394\n",
      "当前是第 45 代，成本值为：1.5667064636945724\n",
      "当前是第 50 代，成本值为：1.5544866845011711\n",
      "当前是第 55 代，成本值为：1.5021873638033867\n",
      "当前是第 60 代，成本值为：1.4610355347394943\n",
      "当前是第 65 代，成本值为：1.3044895753264427\n",
      "当前是第 70 代，成本值为：1.2017599679529667\n",
      "当前是第 75 代，成本值为：1.1632419638335705\n",
      "当前是第 80 代，成本值为：1.102885439991951\n",
      "当前是第 85 代，成本值为：1.0871052145957947\n",
      "当前是第 90 代，成本值为：1.0519114024937153\n",
      "当前是第 95 代，成本值为：1.018553502857685\n",
      "当前是第 100 代，成本值为：1.0050987228751183\n",
      "当前是第 105 代，成本值为：0.9571819938719273\n",
      "当前是第 110 代，成本值为：0.9607120379805565\n",
      "当前是第 115 代，成本值为：0.9355748631060123\n",
      "当前是第 120 代，成本值为：0.9296386763453484\n",
      "当前是第 125 代，成本值为：0.9039849825203419\n",
      "当前是第 130 代，成本值为：0.9066716618835926\n",
      "当前是第 135 代，成本值为：0.8606524728238583\n",
      "当前是第 140 代，成本值为：0.8482984974980354\n",
      "当前是第 145 代，成本值为：0.8185466676950455\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf2d8b47748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-71-a145477c1e6a>:89: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "corrent_prediction accuracy= Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "训练集准确度：0.71666664\n",
      "测试集准确度：0.59166664\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test,num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
